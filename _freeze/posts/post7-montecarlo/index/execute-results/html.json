{
  "hash": "2b421dd34074931eefcada01b024f728",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Monte Carlo, Life paths, and AI\"\nauthor: \"Henrik Vitus Bering Laursen\"\ndate: \"2025-07-10\" \ncategories: \n  - forecasting\n  - monte carlo simulation\n  - life path\n  - randomness\n  - Nassim Taleb\n  - AI\n  - Mentor mentee\nformat:\n  html:\n    toc: true\n    number-sections: true\n    fig-width: 8\n    fig-height: 6\nfreeze: TRUE\ndraft: false\nimage: thumbnail.jpg\n---\n\n::: {.callout-warning}\nThis blogpost was concieved as a short look at monte carlo simulation, but turned into both musings on some health economic decision principles, and discussion of the advantages and drawbacks of using AI. \n:::\n\n## Purpose\n\n### A little intro text\n\nInspired by [Fooled by Randomness]() by [Nassim Taleb](wiki site), I decided to try doing some [Monte Carlo](wiki site) simulations of different potential life paths ahead of me.\n\nMonte Carlo simulations are not foreign to me, as they are frequently used in health economic models to take uncertainty into account when presenting a comparison between different health interventions or strategies. \n\nThe way that is usually done is by having point estimates and confidence intervals - or [credibility intervals](wiki) if you are bayesian, which those types of analyses mostly are - for certain outcomes, both in terms of their costs and effects, associated with each option being considered.\n\nThese estimates and intervals are then sampled randomly in simulations around at least 1000 times, to provide estimates of which option is better than the other, in **probability clouds**. \n\nThese **probability clouds** represent the spread of the different possible outcomes, given the uncertainty associated with the confidence intervals.\n\nImagine two dots on a plot, each representing the cost and effects of the interventions. One seems to be in better place than the other. *Is the intervention with seemingly the best cost/effect ratio compared to the other always the better choice?* **Not necessarily**.\n\nWith the monte carlo simulation, you can see thousands of dots of the two different colours. The clouds may overlap, or the cloud representing the seemingly worse intervention might have more estimates that seem to be in more advantagous spots. \n\nAdvantageous how? Well, more of the estimates may be in the \"much higher effect, at a sligthly higher cost\" than the single point estimate mentioned earlier. Visualising how the possible estimates spread out, in many different \"worlds\" or scenarios, can help decision-makers determine what is likely to be the best option, at least much better than a single point estimate, and a range of uncertainty with two numbers.\n\nSo. That would be nice to try to transfer to different life choices, As right now, I am looking for a job, and hey, who knows what kind of job I can get.\n\n\n### Distilled purpose\n\nWe will start simple with a comparison of 3 different life paths from on here:\n\n- Public sector job\n- Private sector job\n- Start own business / \"Startup\"\n\nNot to conclude what is best, but to see what the possible consequences can be.\n\nThis is of course ridiculously simplified, as I am likely to switch jobs more than once after the initial choice. That could be the theme of a different post - a decision tree or markov model where spending time in different states is possible.\n\n### Analysis plan\n\nIn brief:\n\n- 3 different options\n- 20 year into the future\n- 1000 simulations of \"life paths\"\n\nBecause the tool is available and I would be insane not to use it, I use GPT to help me generate the template, and then learn how to change it from there to fit my needs.\n\nAlso, I learn how it works, preferably by both searching for different ways of doing it online.\n\n## Process \n\nNo need to load and clean data, as I will generate it myself.\n\n### Setup\n\nJust need the `tidyverse` package for this. Might try adding health economic packages like `heemod`, `dampak`, or `hesim` in the future. Maybe there is a `tidyverse` like package collection for health economic packages?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n#### Parameters\n\nYears, simulations, options as a tibble.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# basic parameters\nn_years <- 20\nn_sim <- 1000\n\n# decisions/choices\ndecisions <- tibble(\n  decision = c(\"Private\", \"Public\", \"Startup\"), \n  startup_prob_success = c(NA, NA, 0.30), \n  # all moneys in 1000 DKK/year \n  startup_income_success = c(NA, NA, 6000), \n  base_income = c(600, 400, 50), \n  growth_rate_income = c(0.02, 0.02, 0.03),\n  std_dev_income = c(10, 5, 300), \n  base_content = c(3.2, 3.5, 3.0),\n  content_sd = c(0.4, 0.3, 0.9),\n  burnout_prob = c(0.10, 0.05, 0.25),\n  health_shock_prob = c(0.05, 0.05, 0.15)\n)\n```\n:::\n\n\n#### Simulation function\n\nFirst the simulation function for one simulation, for one decision.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_one_life <- function(decision_row, sim_id) {\n  # if dec=startup, calculate prob of success, if not, NA\n  has_success <- ifelse(decision_row$decision == \"startup\",\n                        rbinom(1, 1, decision_row$startup_prob_success), NA)\n\n  # Income, for startup it depends on success\n  years <- 1:n_years\n  income_base <- decision_row$base_income * (1 + decision_row$growth_rate_income)^(years - 1)\n\n  # for the gamma distribution thats necesary for startups (no 0)\n\n  income <- case_when(\n    decision_row$decision == \"Startup\" & !is.na(has_success) & has_success == 1 ~\n      rep(decision_row$startup_prob_success, n_years),\n    decision_row$decision == \"Startup\" ~ {\n      # I assume gamma because income even for startup is probably from 0 and above\n      shape <- (500 / 100)^2 #mean/sd\n      rate <- 500 / 100^2 #mean/sd\n      rgamma(n_years, shape = shape, rate = rate)\n    },\n    TRUE ~\n  # For the rest, it follows their respective distribution parameters\n      rnorm(n_years, mean = income_base, sd = decision_row$std_dev_income)\n  )\n\n  # Yearly chance events\n  health_shock <- rbinom(n_years, 1, decision_row$health_shock_prob)\n  burnout <- rbinom(n_years, 1, decision_row$burnout_prob)\n  \n  # Parenting - which affects the contentness\n    # First child: already present (baseline) - dont consider \n    # Second child: guaranteed to arrive within first 5 years\n    kid2_year <- sample(2:6, 1)\n    \n    # Parenting flag for the second\n    parenting2 <- ifelse((1:n_years) >= kid2_year, 1, 0)\n  \n  # Contentness\n  content <- rnorm(n_years, decision_row$base_content, decision_row$content_sd) -\n    0.4 * burnout - 0.6 * health_shock + 0.3 * parenting2\n  \n  # Scale must be between 1 (very not content), to 5 (very content)\n  content <- pmin(5, pmax(1, content))\n  \n  # Savings of 10% each year, minus healthshock prob * its cost\n  savings <- cumsum(income * 0.1 - 5 * health_shock) # health schock cost in 1000's too\n  \n  # Standardising a total hapiness score. each one contributes 20-40% to it\n  # happiness <- 0.4 * scale(income)[, 1] +\n  #              0.4 * scale(content)[, 1] +\n  #              0.2 * scale(savings)[, 1]\n  \n  # make tibble for output\n  tibble(\n    year = 1:n_years,\n    sim = sim_id,\n    decision = decision_row$decision,\n    income,\n    content,\n    burnout,\n    health_shock,\n    parenting2,\n    savings\n  )\n}\n```\n:::\n\n\n### Output\n#### Run and summarize\n\nFor each decision, we simulate 1000 iterations of what would happen over 20 years.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- map(1:nrow(decisions), function(i) {\n  decision_row <- decisions[i, ]\n\n  map(1:n_sim, ~simulate_one_life(decision_row, .x)) |> \n    list_rbind()\n}) |> list_rbind()\n```\n:::\n\n\nIt literally only takes seconds to simulate three diferent life paths (20 years), 1000 times. Holy moley. But, I know from experience that including more variables or more simulations increases the time non-linearly.\n\nNow, to get some point estimates for each decision, we can summarise the results by grouping the simulations results by simulation number, and decision and calculating some averages, and the last of the savings.\n\nThen, we group by each decision, and then just take the mean of each value, to get a point estimate for each decision. \n\nThe mean of all simulations for each decision. Mean of mean of the possible values, if you will.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary <- results |> \n  group_by(sim, decision) |> \n  summarise(\n    avg_income = mean(income),\n    avg_content = mean(content),\n    burnout_rate = mean(burnout),\n    health_event_rate = mean(health_shock),\n    end_savings = last(savings),\n    .groups = \"drop\"\n  ) |> \n  select(-sim) |>  # <- remove sim so it's not re-summarized\n  group_by(decision) |> \n  summarise(across(everything(), list(mean = mean, sd = sd)), .groups = \"drop\")\n\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 11\n  decision avg_income_mean avg_income_sd avg_content_mean avg_content_sd\n  <chr>              <dbl>         <dbl>            <dbl>          <dbl>\n1 Private             729.          2.25             3.38         0.101 \n2 Public              486.          1.11             3.70         0.0795\n3 Startup             500.         22.3              3.06         0.205 \n# ℹ 6 more variables: burnout_rate_mean <dbl>, burnout_rate_sd <dbl>,\n#   health_event_rate_mean <dbl>, health_event_rate_sd <dbl>,\n#   end_savings_mean <dbl>, end_savings_sd <dbl>\n```\n\n\n:::\n:::\n\n\n#### Visualize\n\nNow for perhaps the most helpful part of this: the raw visualisation of the possible future lifepath outcomes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults |> \n  ggplot(aes(x = income, fill = decision)) +\n  geom_density(alpha = 0.4) + \n  labs(\n    title = \"Life Income Distribution by Decision Path\", \n    x = \"Total Income in 1000 DKK over 30 Years\", y = \"Density\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Density plot-1.png){width=768}\n:::\n\n```{.r .cell-code}\nresults |> \n  ggplot(aes(x = content, fill = decision)) +\n  geom_density(alpha = 0.4) + \n  labs(\n    title = \"Life Income Distribution by Decision Path\", \n    x = \"Total Income in 1000 DKK over 30 Years\", y = \"Density\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Density plot-2.png){width=768}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults |> \n  ggplot(aes(x = income, y = content, color = decision)) +\n  geom_point(alpha = 0.2) + \n  geom_point(\n    data = summary,\n    aes(x = avg_income_mean, y = avg_content_mean, group = decision),\n    color = \"black\",\n    shape = 18) +\n  geom_text(\n    data = summary,\n    aes(x = avg_income_mean, y = avg_content_mean, label = decision),\n    vjust = -1,\n    inherit.aes = FALSE\n  ) +\n  labs(\n    title = \"Life Income Distribution by Decision Path - Point Estimates Added\", \n    x = \"Total Income in 1000 DKK over 30 Years\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/cloud with points-1.png){width=768}\n:::\n\n```{.r .cell-code}\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saved as thumbnail\n```\n:::\n\n\nAs the pure scatter plots are a bit wild on the eyes, I added the point estimates from the previous summary, for each decision.\n\nSo there. A truckload of simulations of arbitrary lifepaths with relatively arbitrary data feeding it. The following can be seen at a glance:\n\n- The public sector job decision sits with relatively high contentness at lower income\n- The private sector job has much higher income, at slightly more dispersed contentness\n- The startup path is mired in such great uncertainty, that its cloud takes up a lot of space. It has both the highest and lowest income and contentness, with many simulations at the highest and lowest contentment levels.\n\nI will skip visualising the savings for now.\n\nWhat is more important to visualise is the effects besides the income and contentness, that is:\n\n- How much burnout and health schock are associated with each decision?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults |> \n  group_by(decision, year) |> \n  summarise(burnout_rate = mean(burnout), .groups = \"drop\") |> \n  ggplot(aes(x = year, y = burnout_rate, color = decision)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Burnout Rate by Decision Over Time\",\n    y = \"Proportion Burned Out\",\n    x = \"Year\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/other effects - burnout, health shocks-1.png){width=768}\n:::\n\n```{.r .cell-code}\nresults |> \n  group_by(decision, year) |> \n  summarise(shock_rate = mean(health_shock), .groups = \"drop\") |> \n  ggplot(aes(x = year, y = shock_rate, color = decision)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Annual Health Shock Rate by Decision\",\n    y = \"Proportion with Health Shock\",\n    x = \"Year\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/other effects - burnout, health shocks-2.png){width=768}\n:::\n:::\n\n\nAround twice the burnout rate and three times the health shock rate for the Startup decision relative to the others.\n\nSo the simulations accurately represents (because the inputs accurately represents), the high risk high reward style of starting a company.\n\n## Summary / cleaned up / refactored \n\nSo, to summarise. \n\nI created three imaginary life paths that I could take and assumed that I would stick to the path chosen for the simulation period.\n\nThen I defined some different base values and uncertanties in the form of standard deviations and probabilities, for the life paths, in a little data frame.\n\nThese were then put into a simulation function that defined what would occur for each simulation, with differences for each path.\n\nThen the simulation function was applied to the life path dataframe, for each decision, for 20 years, 1000 times. \n\nAnd the results were visualised with density plots, scatter plots for content. With these arbitrary inputs, not much can be gleaned from such a simulation, other than the fact that uncertainty plays a huge role and one can have a lot of different outcomes. Perhaps based on this one could decide whether one felt \n\n### A Note on AI\n\nAI (ChatGPT) played a role in this blog in the following ways:\n\n1. Carrying my idea into practice (writing the main code)\n2. Changing the code\n\nThe amount of time I saved from thinking my way through this is enormous. Which leads to the critical question asked often these days:  \n\n> What have I lost in terms of learning, with the short term gain in efficiency from using AI? \n\nThis applies to both point 1 and 2 above.\n\nFor both points, it is obvious that I have lost ***something***. It has been established by many researchers that [slow](https://cpb-us-w2.wpmucdn.com/sites.udel.edu/dist/6/132/files/2010/11/Psychological-Science-2014-Mueller-0956797614524581-1u0h0yu.pdf) and [deliberate](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0032184) practice fosters better brain development and learning than just absorbing summaries or cramming a bunch of information in your head in a short amount of time.\n\nFrom my memory, the following has been shown to be very beneficial for learning:\n\n- Pen and paper (in most scenarios)\n- Slow, deliberate learning with clear goals for which progress can be tracked\n- Spaced repitition (learn about a thing.. let it sit a bit.. bring it to your memory again.. wait longer.. bring it to your memory, rinse, repeat)\n- Explaining it in your own terms, and to others (hitching on to the famous \"Feynman Technique\")\n- Doing it in a location and environment that specifically fits you, but probably mostly with distractions removed\n- Short intense bursts, and then breaks with NOTHING to do but relaxing (not on phone or SOME)\n\n**However**, is there perhaps also an obvious benefit to using AI? For example, how different is AI from , at least parts of, the [mentor-mentee relationship](https://www.ncbi.nlm.nih.gov/books/NBK552775/)?\n\nWhile a real mentor provides the following:\n\n- Personalised feedback and guidance on:\n  - Progress\n  - Strengths and weaknesses\n  - Career advice\n- Field specific expert level knowledge transfer\n- Tacit knowledge transfer\n- Motivation\n- Public approval and recommendations to others\n- Trust and support\n\nAn AI mentor can in large part provide the same, where, I believe, the following is significantly different, in terms of what the AI can do:\n\n- Much quicker access to knowledge\n- Poor deep brainstorming and not precise progress tracking\n  - A real expert will know exactly how much a person knows, and when to prompt different levels or stages of knowledge\n- Tracking of progress is fine until AI hallucinations occur\n- Much weaker transfer of tacit knowledge\n- Weaker motivation \n  - The text version of \"You've got it!\" does not hit nearly the same way as the real life, honest, and enthusiastic version\n- No recommendation and trust\n- AI can't abuse you (bad mentor)\n\nOr, as ChatGPT puts it:\n\n> What it can’t provide are the richer social-emotional cues, contextual wisdom and professional sponsorship that come from a flesh-and-blood mentor embedded in your world. Treat the model as a tireless practice partner and sounding board, not a full substitute, and you’ll capture the best of both worlds.\n\nIn summary, I believe AI will raise the floor of learning, making it resemble plot B more than plot A. See below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- data.frame(\n  a = as.numeric(rnorm(1000, 100, 15)),\n  b = as.numeric(rnorm(1000, 100, 25))\n)\n\nggplot(data = df1) + \n  geom_density(aes(x=a), color = \"red\") +\n  geom_density(aes(x=b), color = \"blue\") + \n  labs(x = NULL,y = NULL) + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/AI effect on learning-1.png){width=768}\n:::\n:::\n\nI set the mean around 100 to make it resemble IQ.\n\nThat is, I believe:\n\n1. It will be a good tool to gain access to knowledge for people who were either too lazy to access it or were prevented in some way before\n2. There are some who will overly rely on it and suffer the consequences of losing some of their ability to think for themselves\n3. There are some who will recognise its potential and downsides, and use it when it is appropriate, soring about others \n\nThe last group is likely the ones who are least likely, at least in the white-collar jobs, to be replaced by AI.\n\nUp until AI is more [\"Agentic\"](https://en.wikipedia.org/wiki/Agentic_AI), ofcourse. By **then** I have no idea what will happen. Some smart people are [thinking about this, though](https://ai-2027.com/).\n\n### Refactored code\n\n**Below you can find the entire code, refactored a bit**.\n\nI used the following prompt, which worked fine although it actually made some errors that made me have to remake some of it.\n\n    First, please take out all chunks, and refactor them in the following way:\n    \n    - make code more concise, with functions for repeating lines or plots\n    - remove comments\n    - remove unecessary code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nn_years <- 20\nn_sim   <- 1000\n\ndecisions <- tibble(\n\tdecision = c(\"Private\", \"Public\", \"Startup\"), \n\tstartup_prob_success = c(NA, NA, 0.30), \n\t# all moneys in 1000 DKK/year \n\tstartup_income_success = c(NA, NA, 6000), \n\tbase_income = c(600, 400, 50), \n\tgrowth_rate_income = c(0.02, 0.02, 0.03),\n\tstd_dev_income = c(10, 5, 300), \n  base_content = c(3.2, 3.5, 3.0),\n  content_sd = c(0.4, 0.3, 0.9),\n\tburnout_prob = c(0.10, 0.05, 0.25),\n\thealth_shock_prob = c(0.05, 0.05, 0.15)\n)\n\nsimulate_one_life <- function(decision_row, sim_id) {\n  has_success <- ifelse(decision_row$decision == \"startup\",\n                        rbinom(1, 1, decision_row$startup_prob_success), NA)\n  years <- 1:n_years\n  income_base <- decision_row$base_income * (1 + decision_row$growth_rate_income)^(years - 1)\n\n  income <- case_when(\n    decision_row$decision == \"Startup\" & !is.na(has_success) & has_success == 1 ~\n      rep(decision_row$startup_prob_success, n_years),\n    decision_row$decision == \"Startup\" ~ {\n      shape <- (500 / 100)^2 #mean/sd\n      rate <- 500 / 100^2 #mean/sd\n      rgamma(n_years, shape = shape, rate = rate)\n    },\n    TRUE ~ rnorm(n_years, mean = income_base, sd = decision_row$std_dev_income)\n  )\n\n  health_shock <- rbinom(n_years, 1, decision_row$health_shock_prob)\n  burnout <- rbinom(n_years, 1, decision_row$burnout_prob)\n  kid2_year <- sample(2:6, 1)\n  parenting2 <- ifelse((1:n_years) >= kid2_year, 1, 0)\n  \n  content <- rnorm(n_years, decision_row$base_content, decision_row$content_sd) -\n    0.4 * burnout - 0.6 * health_shock + 0.3 * parenting2\n  \n  content <- pmin(5, pmax(1, content))\n  savings <- cumsum(income * 0.1 - 5 * health_shock) \n  \n  tibble(\n    year = 1:n_years,\n    sim = sim_id,\n    decision = decision_row$decision,\n    income,\n    content,\n    burnout,\n    health_shock,\n    parenting2,\n    savings\n  )\n}\n\nrun_decision <- function(row, reps) {\n  map(seq_len(reps), ~simulate_one_life(row, .x)) |> list_rbind()\n}\n\nresults <- map(seq_len(nrow(decisions)),\n               ~run_decision(decisions[.x, ], n_sim)) |> list_rbind()\n\nsummary <- results |> \n    group_by(sim, decision) |> \n    summarise(\n      avg_income   = mean(income),\n      avg_content  = mean(content),\n      burnout_rate = mean(burnout),\n      health_rate  = mean(health_shock),\n      end_savings  = last(savings),\n      .groups = \"drop\"\n    ) |>\n    select(-sim) |>\n    group_by(decision) |>\n    summarise(across(everything(), list(mean = mean, sd = sd)), .groups = \"drop\")\n\nplot_density <- function(dat, var){\n  ggplot(dat, aes({{ var }}, fill = decision)) +\n    geom_density(alpha = 0.4) +\n    theme_minimal()\n  }\n\nplot_scatter <- function(dat, sumdat){\n  ggplot(dat, aes(income, content, colour = decision)) +\n    geom_point(alpha = 0.2) +\n    geom_point(data = sumdat,\n               aes(avg_income_mean, avg_content_mean),\n               colour = \"black\", shape = 18, inherit.aes = FALSE) +\n    geom_text(data = sumdat,\n              aes(avg_income_mean, avg_content_mean, label = decision),\n              vjust = -1, inherit.aes = FALSE) +\n    theme_minimal()\n}\n\nplot_rate <- function(dat, var, ttl){\n  ggplot(dat %>% group_by(decision, year) %>%\n           summarise(rate = mean({{ var }}), .groups = \"drop\"),\n         aes(year, rate, colour = decision)) +\n    geom_line(linewidth = 1) +\n    labs(title = ttl) +\n    theme_minimal()\n}\n\nplot_density(results, income)\n```\n\n```{.r .cell-code}\nplot_density(results, content)\n```\n\n```{.r .cell-code}\nplot_scatter(results, summary)\n```\n\n```{.r .cell-code}\nplot_rate(results, burnout,       \"Burnout Rate\")\n```\n\n```{.r .cell-code}\nplot_rate(results, health_shock,  \"Health-Shock Rate\")\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}