[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Vitus Bering exploring the Danish Data seas",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR shiny\n\n\n\nhospital usage\n\nr shiny dashboard\n\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo, Life paths, and AI\n\n\n\nforecasting\n\nmonte carlo simulation\n\nlife path\n\nrandomness\n\nNassim Taleb\n\nAI\n\nMentor mentee\n\n\n\n\n\n\n\n\n\nJul 10, 2025\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nDanish diabetes data\n\n\n\ndiabetes\n\ndata exploration\n\njoins\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\nJun 17, 2025\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nMoving on\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nGot the PhD\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nJan 19, 2025\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution in DDD use of Diabetes Drugs over time\n\n\n\ncode\n\nanalysis\n\nexploration\n\ncost\n\ndrugs\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R functions on medstat data\n\n\n\ncode\n\nanalysis\n\nexploration\n\nfunctions\n\ncost\n\ndrugs\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\n\n\n\n\n\n\n\nMaking sense of Medstat.dk data\n\n\n\ncode\n\nanalysis\n\nexploration\n\ncost\n\ndrugs\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nHenrik Vitus Bering Laursen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scratchpad.html",
    "href": "scratchpad.html",
    "title": "Vitus Bering exploring the Danish Data seas",
    "section": "",
    "text": "MjAxNl9hdGNfY29kZV9kYXRhLnR4dA MjAxN19hdGNfY29kZV9kYXRhLnR4dA MjAxN19hdGNfY29kZV9kYXRhLnR4dA"
  },
  {
    "objectID": "posts/2025-09-25-rshiny/index.html#purpose",
    "href": "posts/2025-09-25-rshiny/index.html#purpose",
    "title": "R shiny",
    "section": "2 Purpose",
    "text": "2 Purpose\nI wanted to walk through how to use R shiny and put in some publicly available Danish statistics as the basis for a dashboard.\nI want to do the following:\n\nEstablish how to get the data necessary\nGet the data\nStep-by-step how to use R shiny in the simplest possible way\n\none outcome, one group, over time\n\nStop it there and then explore more in another post\n\nSo here we go."
  },
  {
    "objectID": "posts/2025-09-25-rshiny/index.html#the-process",
    "href": "posts/2025-09-25-rshiny/index.html#the-process",
    "title": "R shiny",
    "section": "3 The Process",
    "text": "3 The Process\n\n3.1 Analysis plan\nFirst I sign in to Statistics Denmark website, after creating a user (necessary for bigger downloads).\nThen, I pick out a type of data I want to look at. There are so many. Because of my interest in healthcare I look at the table for hospital utilization, SBR04.\nFrom that table I select all variables. I will probably only use very few, but I do it just in case I use more of it in the future.\nI will then:\n\nExplore the data to find what I find interesting\nTry to structure the R shiny around what would be interesting to look at given the data\nBuild the R shiny app. Probably actually publish it online, while remembering to cite Statistics Denmark as the source\n\n\n\n3.2 Load\n\n3.2.1 Libraries\n\n\nShow the code\nsource(\"~/publicdataprojects/scripts/source.R\")\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n3.2.2 Data\nThe process of finding data from Statistics Denmark currently comes in these steps:\n\nFind overview of tables\nSelect a subtable\nSelect variables from that table, then click “vis tabel”\nClick on the selection of different export formats (Matrix [*.csv] is the type i’m used to)\nImport the downloaded file, remembering to account for Danish locale\n\n\n\nShow the code\nfile_path &lt;- c(\"C:/Users/henri/Downloads/2025917133644569067073SBR0449248987343.csv\")\n\ndf &lt;- read_csv(file_path)\n\n\nRows: 539 Columns: 1\n\n\nError in nchar(x, \"width\"): invalid multibyte string, element 1\n\n\nOdd. I didnt expect that error. Let’s see whats up.\n\n\nShow the code\nread_all &lt;- read_lines(file_path)\nhead(read_all)\n\n\n[1] \"\\\"K\\xf8n i alt\\\";\\\"Alder i alt\\\";\\\"Uanset sygehusv\\xe6sen\\\";\\\"Personer med ophold (antal)\\\";2788101.0;2876145.0;2900078.0;2793650.0;2838793.0;2919830.0;2962563.0;2957759.0\"                     \n[2] \"\\\"K\\xf8n i alt\\\";\\\"Alder i alt\\\";\\\"Uanset sygehusv\\xe6sen\\\";\\\"Personer med ophold (pct.)\\\";48.2;49.5;49.8;47.8;48.3;49.2;49.7;49.4\"                                                              \n[3] \"\\\"K\\xf8n i alt\\\";\\\"Alder i alt\\\";\\\"Uanset sygehusv\\xe6sen\\\";\\\"Ophold per person (antal)\\\";2.1;2.2;2.3;2.1;2.2;2.2;2.2;2.2\"                                                                       \n[4] \"\\\"K\\xf8n i alt\\\";\\\"Alder i alt\\\";\\\"Uanset sygehusv\\xe6sen\\\";\\\"Personer med ophold p\\xe5 under 12 timer (antal)\\\";2713543.0;2804125.0;2831454.0;2725266.0;2769264.0;2851193.0;2895701.0;2890614.0\"\n[5] \"\\\"K\\xf8n i alt\\\";\\\"Alder i alt\\\";\\\"Uanset sygehusv\\xe6sen\\\";\\\"Personer med ophold p\\xe5 under 12 timer (pct.)\\\";46.9;48.3;48.6;46.7;47.1;48.1;48.6;48.2\"                                         \n[6] \"\\\"K\\xf8n i alt\\\";\\\"Alder i alt\\\";\\\"Uanset sygehusv\\xe6sen\\\";\\\"Ophold p\\xe5 under 12 timer per person (antal)\\\";2.0;2.0;2.2;2.0;2.0;2.1;2.1;2.1\"                                                  \n\n\nOk this may need a different reading function.\n\n\nShow the code\ndf &lt;- read_delim(file_path,\n           delim = \";\",\n           locale = locale(encoding = \"ISO-8859-1\"),\n           col_names = FALSE\n           )\n\n\nRows: 540 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (4): X1, X2, X3, X4\ndbl (8): X5, X6, X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\nhead(df)\n\n\n# A tibble: 6 × 12\n  X1        X2      X3    X4        X5     X6     X7     X8     X9    X10    X11\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Køn i alt Alder … Uans… Pers… 2.79e6 2.88e6 2.90e6 2.79e6 2.84e6 2.92e6 2.96e6\n2 Køn i alt Alder … Uans… Pers… 4.82e1 4.95e1 4.98e1 4.78e1 4.83e1 4.92e1 4.97e1\n3 Køn i alt Alder … Uans… Opho… 2.1 e0 2.2 e0 2.3 e0 2.1 e0 2.2 e0 2.2 e0 2.2 e0\n4 Køn i alt Alder … Uans… Pers… 2.71e6 2.80e6 2.83e6 2.73e6 2.77e6 2.85e6 2.90e6\n5 Køn i alt Alder … Uans… Pers… 4.69e1 4.83e1 4.86e1 4.67e1 4.71e1 4.81e1 4.86e1\n6 Køn i alt Alder … Uans… Opho… 2   e0 2   e0 2.2 e0 2   e0 2   e0 2.1 e0 2.1 e0\n# ℹ 1 more variable: X12 &lt;dbl&gt;\n\n\nAlright. I expected comma, but it was semicolons. And some googling revealed “ISO-8859-1” as the danish encoding.\n\n\n\n3.3 Clean\nNow, the structure seems to need some cleaning, with several repeating values. The columns seem to be “supercolumns” with each column from the left to the right being “beneath” in the level of grouping of the previous.\nFirst, let’s replace those X column names with their corresponding actual names.\n\n\nShow the code\n# Rename descriptor columns\ndf2 &lt;- df |&gt; rename(sex = X1, age = X2, sector = X3, measure = X4)\n\n# Rename year columns\nyear_labels &lt;- c(\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\",\"2024\")\nnames(df2)[5:12] &lt;- year_labels\n\nhead(df2)\n\n\n# A tibble: 6 × 12\n  sex      age   sector measure `2017` `2018` `2019` `2020` `2021` `2022` `2023`\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Køn i a… Alde… Uanse… Person… 2.79e6 2.88e6 2.90e6 2.79e6 2.84e6 2.92e6 2.96e6\n2 Køn i a… Alde… Uanse… Person… 4.82e1 4.95e1 4.98e1 4.78e1 4.83e1 4.92e1 4.97e1\n3 Køn i a… Alde… Uanse… Ophold… 2.1 e0 2.2 e0 2.3 e0 2.1 e0 2.2 e0 2.2 e0 2.2 e0\n4 Køn i a… Alde… Uanse… Person… 2.71e6 2.80e6 2.83e6 2.73e6 2.77e6 2.85e6 2.90e6\n5 Køn i a… Alde… Uanse… Person… 4.69e1 4.83e1 4.86e1 4.67e1 4.71e1 4.81e1 4.86e1\n6 Køn i a… Alde… Uanse… Ophold… 2   e0 2   e0 2.2 e0 2   e0 2   e0 2.1 e0 2.1 e0\n# ℹ 1 more variable: `2024` &lt;dbl&gt;\n\n\n\n3.3.1 Filtering\nThe first column, Sex, is “Køn i alt” which is both sexes. I want to see the difference in sexes so I will remove, or filter out, those columns. Also, I want age ranges, and not total. Finally, I want it to be divided into somatic and psychiatric. All of these will be filtered out below.\n\n\nShow the code\ndf3 &lt;- df2 |&gt;\n  filter(sex != \"Køn i alt\",\n         age != \"Alder i alt\",\n         sector != \"Uanset sygehusvæsen\")\nhead(df3)\n\n\n# A tibble: 6 × 12\n  sex   age     sector  measure   `2017`  `2018`  `2019`  `2020`  `2021`  `2022`\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Mænd  0-17 år Somatik Persone… 2.17e+5 2.16e+5 2.19e+5 2.07e+5 2.06e+5 2.15e+5\n2 Mænd  0-17 år Somatik Persone… 3.64e+1 3.63e+1 3.69e+1 3.5 e+1 3.49e+1 3.62e+1\n3 Mænd  0-17 år Somatik Ophold … 9   e-1 9   e-1 9   e-1 9   e-1 9   e-1 9   e-1\n4 Mænd  0-17 år Somatik Persone… 2.06e+5 2.06e+5 2.09e+5 1.98e+5 1.97e+5 2.06e+5\n5 Mænd  0-17 år Somatik Persone… 3.45e+1 3.46e+1 3.53e+1 3.36e+1 3.34e+1 3.47e+1\n6 Mænd  0-17 år Somatik Ophold … 8   e-1 8   e-1 8   e-1 8   e-1 8   e-1 8   e-1\n# ℹ 2 more variables: `2023` &lt;dbl&gt;, `2024` &lt;dbl&gt;\n\n\nThis leaves a dataset divided into the following groupings:\n\nMen and women\nAge ranges\nSomatic contact\nPsychiatric contact\nBoth Somatic Psychiatric\n\nAnd then subvariables for each of the above for:\n\n\nShow the code\ndf3 |&gt; distinct(measure)\n\n\n# A tibble: 9 × 1\n  measure                                              \n  &lt;chr&gt;                                                \n1 Personer med ophold (antal)                          \n2 Personer med ophold (pct.)                           \n3 Ophold per person (antal)                            \n4 Personer med ophold på under 12 timer (antal)        \n5 Personer med ophold på under 12 timer (pct.)         \n6 Ophold på under 12 timer per person (antal)          \n7 Personer med ophold på 12 timer eller derover (antal)\n8 Personer med ophold på 12 timer eller derover (pct.) \n9 Ophold på 12 timer eller derover per person (antal)  \n\n\n\n\n3.3.2 Modifying language\nAnd, because it is there, and I do not miss out on any learning by translating the data to english via GPT, I’m gonna put in and example dataset and ask GPT to translate all values in Danish, to English. And ofcourse just do it by hand the few places where simple letters can be changed.\n\n\nShow the code\n# Hand translate the easy ones\n\ndf4 &lt;- df3 |&gt;\n  mutate(sex = str_replace_all(sex,\"Mænd\",\"Men\"),\n         sex = str_replace_all(sex,\"Kvinder\",\"Women\"),\n         age = str_replace_all(age,\"år\",\"years\"),\n         age = str_replace_all(age,\"og derover\",\"\"),\n         age = str_replace_all(age,\"60 years\",\"60+ years\"),\n         sector = str_replace_all(sector,\"Somatik\",\"Somatic\"),\n         sector = str_replace_all(sector,\"Psykiatri\",\"Psychiatry\"),\n         sector = str_replace_all(sector,\"Både somatik og psykiatri\",\"Both somatic and psychiatry\")\n         )\n\n# GPT aided translation of the measure\n  # clean text\ndf5 &lt;- df4 |&gt;\n  mutate(\n    measure = str_squish(measure)              # trim & collapse whitespace\n  )\n\n  # Define a named lookup (DA -&gt; EN)\nda_en &lt;- c(\n  \"Personer med ophold (antal)\"                          = \"Persons with stays (number)\",\n  \"Personer med ophold (pct.)\"                           = \"Persons with stays (percent)\",\n  \"Ophold per person (antal)\"                            = \"Stays per person (number)\",\n  \"Personer med ophold på under 12 timer (antal)\"        = \"Persons with stays under 12 hours (number)\",\n  \"Personer med ophold på under 12 timer (pct.)\"         = \"Persons with stays under 12 hours (percent)\",\n  \"Ophold på under 12 timer per person (antal)\"          = \"Stays under 12 hours per person (number)\",\n  \"Personer med ophold på 12 timer eller derover (antal)\"= \"Persons with stays of 12 hours or more (number)\",\n  \"Personer med ophold på 12 timer eller derover (pct.)\" = \"Persons with stays of 12 hours or more (percent)\",\n  \"Ophold på 12 timer eller derover per person (antal)\"  = \"Stays of 12 hours or more per person (number)\"\n)\n\n  # Translate (keep originals that don’t match)\ndf6 &lt;- df5  |&gt;\n  mutate(measure = recode(measure, !!!da_en, .default = measure))\n\n\nI had a hard time understanding the !!! thingy. Its reference page is helpful.\nIn plain terms, that I can understand, it injects, or splices, x into y, where x is a list and y is something else. Possibly a list, too. In the above example, the “lookup” object called da_en, is injected into the recode() function. So basically it tells recode() that for its chosen variable, X4, it can take the list as a series of operations it must go through.\nAnother way, which I am used to and learned before using AI, is just using case_when(). I like that one a lot since it is super easy to understand. When i is the case, set a chosen variable, j to k. It is perhaps better explained as a vectorized if-else statement.\n\n\n3.3.3 Pivoting\nAdditionally, I will need to reshape / pivot the data.\n\n\nShow the code\ndf7 &lt;- df6 |&gt;\n  pivot_longer(\n    cols = 5:12,\n    names_to = \"year\",\n    values_to = \"value\"\n    ) |&gt;\n  mutate(year = as.integer(year),\n         across(where(is.character), as.factor)\n  )\n\nhead(df7)\n\n\n# A tibble: 6 × 6\n  sex   age        sector  measure                      year  value\n  &lt;fct&gt; &lt;fct&gt;      &lt;fct&gt;   &lt;fct&gt;                       &lt;int&gt;  &lt;dbl&gt;\n1 Men   0-17 years Somatic Persons with stays (number)  2017 217228\n2 Men   0-17 years Somatic Persons with stays (number)  2018 216064\n3 Men   0-17 years Somatic Persons with stays (number)  2019 218968\n4 Men   0-17 years Somatic Persons with stays (number)  2020 207051\n5 Men   0-17 years Somatic Persons with stays (number)  2021 206277\n6 Men   0-17 years Somatic Persons with stays (number)  2022 214619\n\n\nSo - Now we have a dataset with the following:\n\nSupercategories\n\nSex\nAge intervals\nSector\n\nOutput measure categories and their values\nAll summarised within each year of registration\n\nThat should be clean enough to start putting into an R shiny app.\n\n\n\n3.4 Output\n\n3.4.1 Initial plots\nLet’s see what some of the data looks like before creating a dashboard. We can work with the bigger and simpler numbers first - The amount of people who have stayed in the hospital, by each year.\n\n\nShow the code\ndf7 |&gt; filter(measure == \"Persons with stays (number)\") |&gt;\n  mutate(value2 = value / 1000) |&gt;\n  ggplot(aes(year, value2, color = age)) +\n  geom_line() +\n  facet_grid(\n    rows = vars(sex),\n    cols = vars(sector)) +\n  labs(x = NULL, y = \"Count in 1000s\")\n\n\n\n\n\n\n\n\n\nIt doesn’t make much sense to compare the different sectors visually, so lets look at the sectors by themselves to start with, and focus on Sex.\n\n\nShow the code\ndf7 |&gt; filter(\n  measure == \"Persons with stays (number)\",\n  sector == \"Somatic\" ) |&gt;\n  mutate(value2 = value / 1000) |&gt;\n  ggplot(aes(year, value2, color = age)) +\n  geom_line() +\n  facet_grid(\n    cols = vars(sex)\n    ) +\n  labs(x = NULL, y = \"Count in 1000s\")\n\n\n\n\n\n\n\n\n\nShow the code\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saved as thumbnail\n\n\n\n\nShow the code\ndf7 |&gt; filter(\n  measure == \"Persons with stays (number)\",\n  sector == \"Psychiatry\" ) |&gt;\n  mutate(value2 = value / 1000) |&gt;\n  ggplot(aes(year, value2, color = age)) +\n  geom_line() +\n  facet_grid(\n    cols = vars(sex)\n    ) +\n  labs(x = NULL, y = \"Count in 1000s\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf7 |&gt; filter(\n  measure == \"Persons with stays (number)\",\n  sector == \"Both somatic and psychiatry\" ) |&gt;\n  mutate(value2 = value / 1000) |&gt;\n  ggplot(aes(year, value2, color = age)) +\n  geom_line() +\n  facet_wrap( ~ sex) +\n  labs(x = NULL, y = \"Count in 1000s\")\n\n\n\n\n\n\n\n\n\nThese graphs are very interesting for displaying differences between the sexes, in terms of stays in hospital.\nNotable results:\n\nMore of the female sex stay in hospitals, except for:\n\nMales aged 0-17 somatic\nMales aged 0-17 psychiatry\n\nRemarkably similar results for men and women aged 30 to 60+ with both somatic and psychiatric stays\n\nAnd this is only the counts. What about all the other measures?:\n\nPersons with stays (percent)\nPersons with stays of 12 hours or more (number)\nPersons with stays of 12 hours or more (percent)\nPersons with stays under 12 hours (number)\nPersons with stays under 12 hours (percent)\nStays of 12 hours or more per person (number)\nStays per person (number)\nStays under 12 hours per person (number)\n\nIn my estimation, a lot of info can be gleaned just from setting up a dashboard that lets you pick between these different measures, with the three plots I made above.\n\n\n3.4.2 R shiny basics\n\n\nShow the code\npackages &lt;- c(\"shiny\", \"bslib\")\nipak(packages)\n\n\nLoading required package: shiny\n\n\nLoading required package: bslib\n\n\n\nAttaching package: 'bslib'\n\n\nThe following object is masked from 'package:utils':\n\n    page\n\n\nshiny bslib \n TRUE  TRUE \n\n\nWith the R shiny packages loaded (bslib just lets you customize the R shiny dashboard more), it is time to start setting up the dashboard.\nIt seems like the example that is structured in the bslib github fits what I want to create as an initial dashboard:\n\nThree separate plots\nOne for each sector\nContaining a measure\nColoured by age groups\nFacet wrapped by sex\n\nSo let’s go. With R shiny you can make dashboards which update based on your input. Shiny code, producing a dashboard, basically consists of:\n\nA User interface (UI) object\n\ncontrols how it looks\n\nA Server function\n\ncontrols how the app is built\n\nThe ShinyApp() function call\n\ncreates shiny app objects from UI/server pair\n\n\nYou can host the dashboard locally or by connecting with it on Shiny apps website, which requires a user.\nI will try to make it here, and make it available on that website, or here in the blogpost if that is possible.\nFirst, I take code that fits, from the bslib github mentioned above that fits what I want:\n\n\nShow the code\ndata(penguins, package = \"palmerpenguins\")\n\nui &lt;- page_sidebar(\n  title = \"Penguins dashboard\",\n  sidebar = sidebar(\n    title = \"Histogram controls\",\n    varSelectInput(\n      \"var\", \"Select variable\",\n      dplyr::select_if(penguins, is.numeric)\n    ),\n    numericInput(\"bins\", \"Number of bins\", 30)\n  ),\n  card(\n    card_header(\"Histogram\"),\n    plotOutput(\"p\")\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$p &lt;- renderPlot({\n    ggplot(penguins) +\n      geom_histogram(aes(!!input$var), bins = input$bins) +\n      theme_bw(base_size = 20)\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nAlright. That actually just works! It seems to have opened in R studio, so must be hosted locally.\nNow, I have to adapt it to my needs:\n\nInput my own data, which should be cleaned for use\nMake the three cards, for each of the three plots\nMake my own dashboard with the ui+server+shinyapp() trifecta\n\nHere we go.\n\n\nShow the code\n# Cards\n\ncards &lt;- list(\n  card(\n    full_screen = TRUE,\n    card_header(\"Somatic sector\"),\n    plotOutput(\"soma\")\n  ),\n  card(\n    full_screen = TRUE,\n    card_header(\"Psychiatric sector\"),\n    plotOutput(\"psych\")\n  ),\n  card(\n    full_screen = TRUE,\n    card_header(\"Both sectors\"),\n    plotOutput(\"somapsych\")\n  )\n)\n\nmeasure &lt;- selectInput(\n  \"measure\", \"Measure\",\n  choices = sort(unique(df7$measure))\n)\n\n\nAnd then the dashboard.\n\n\nShow the code\nplot_sector &lt;- function(data, sector_label) {\n  data_sector &lt;- data |&gt; filter(sector == sector_label)\n  req(nrow(data_sector) &gt; 0)  # show nothing if that sector isn't present\n\n  ggplot(data_sector, aes(x = year, y = value, color = age)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~ sex, nrow = 2) +\n    theme(\n      plot.title = element_blank(),\n      legend.position = \"bottom\",\n    ) +\n    labs(\n      title = sector_label,\n      x = NULL, color = \"Age\",\n      y = NULL\n    ) +\n    theme_bw(base_size = 12)\n}\n\nmeasures &lt;- df7 |&gt;\n  distinct(measure) |&gt;\n  arrange(measure) |&gt;\n  pull(measure)\n\nui &lt;- page_sidebar(\n  title = \"Hospital stays — dashboard\",\n  sidebar = tagList(\n    selectInput(\"measure\", \"Measure\", choices = measures, selected = measures[1]),\n    helpText(\"All plots update to the selected measure.\"),\n    helpText(\"Lines = time\"),\n    helpText(\"Color = age\"),\n    helpText(\"Facets = sex\"),\n    helpText(\"Data has been supplied by Statistics Denmark (table SBR04), which contains aggregate data on hospital usage in Denmark between 2017-2024.\")\n  ),\n  # 3 cards side-by-side (wraps on narrow screens)\n  layout_columns(width = 1,\n     card(\n       full_screen = TRUE,\n       card_header(\"Somatic\"),\n       plotOutput(\"plot_somatic\", height = 300)\n     ),\n     card(\n       full_screen = TRUE,\n       card_header(\"Psychiatric\"),\n       plotOutput(\"plot_psychiatric\", height = 300)\n     ),\n     card(\n       full_screen = TRUE,\n       card_header(\"Both\"),\n       plotOutput(\"plot_both\", height = 300)\n     )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Filter once by measure; reuse for all sectors\n  dat_measure &lt;- reactive({\n    req(input$measure)\n    df7 |&gt; filter(measure == input$measure)\n  })\n\n  output$plot_somatic     &lt;- renderPlot(plot_sector(dat_measure(), sector_label = \"Somatic\"))\n  output$plot_psychiatric &lt;- renderPlot(plot_sector(dat_measure(), sector_label = \"Psychiatry\"))\n  output$plot_both        &lt;- renderPlot(plot_sector(dat_measure(), sector_label = \"Both somatic and psychiatry\"))\n\n}\n\nshinyApp(ui, server)\n\n\n\nListening on http://127.0.0.1:7617\n\n\n\n\n\n\n\n\n\nFantastic!!! It is now created. But how will it be viewed on the blog? The guide on shinyapps.io details that we need to install rsconnect, authorize account, and then deploy. See the Getting Started page of shinyapps.io when you login. Or if using Rstudio, it can just be deployed with the “publish” button.\nBut I am confused about how an app will be displayed on this blog.\nThe simplest solution seems to be a link within the blog post to the shiny dashboard, hosted on the shinyapps.io servers.\nBelow is the link to the resulting dashboard. Purpose complete!\n\n\n\n\n\n\nImportantLink to post\n\n\n\nClick here to get to my resulting R shiny Dashboard"
  },
  {
    "objectID": "posts/2025-09-25-rshiny/index.html#summary-cleaned-up-refactored",
    "href": "posts/2025-09-25-rshiny/index.html#summary-cleaned-up-refactored",
    "title": "R shiny",
    "section": "4 Summary / cleaned up / refactored",
    "text": "4 Summary / cleaned up / refactored\n#| label: Summary chunk\n#| echo: true\n#| output: false\n\n# Packages\nipak &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg))\n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\nipak_list &lt;- c(\"tidyverse\", \"here\", \"shiny\", \"bslib\")\nipak(ipak_list)\n\n# Load data\ndf &lt;- read_delim(\"insert file name\",\n           delim = \";\",\n           locale = locale(encoding = \"ISO-8859-1\"),\n           col_names = FALSE\n           ) |&gt;\n  rename(sex = X1, age = X2, sector = X3, measure = X4)\n\n# Prepping for long data cleaning chain\nyear_labels &lt;- c(\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\",\"2024\")\nnames(df)[5:12] &lt;- year_labels\nda_en &lt;- c(\n  \"Personer med ophold (antal)\"                          = \"Persons with stays (number)\",\n  \"Personer med ophold (pct.)\"                           = \"Persons with stays (percent)\",\n  \"Ophold per person (antal)\"                            = \"Stays per person (number)\",\n  \"Personer med ophold på under 12 timer (antal)\"        = \"Persons with stays under 12 hours (number)\",\n  \"Personer med ophold på under 12 timer (pct.)\"         = \"Persons with stays under 12 hours (percent)\",\n  \"Ophold på under 12 timer per person (antal)\"          = \"Stays under 12 hours per person (number)\",\n  \"Personer med ophold på 12 timer eller derover (antal)\"= \"Persons with stays of 12 hours or more (number)\",\n  \"Personer med ophold på 12 timer eller derover (pct.)\" = \"Persons with stays of 12 hours or more (percent)\",\n  \"Ophold på 12 timer eller derover per person (antal)\"  = \"Stays of 12 hours or more per person (number)\"\n)\n\n# Data cleaning chain\ndf &lt;- df |&gt;\n  filter(sex != \"Køn i alt\",\n         age != \"Alder i alt\",\n         sector != \"Uanset sygehusvæsen\") |&gt;\n  mutate(sex = str_replace_all(sex,\"Mænd\",\"Men\"),\n         sex = str_replace_all(sex,\"Kvinder\",\"Women\"),\n         age = str_replace_all(age,\"år\",\"years\"),\n         age = str_replace_all(age,\"og derover\",\"\"),\n         age = str_replace_all(age,\"60 years\",\"60+ years\"),\n         sector = str_replace_all(sector,\"Somatik\",\"Somatic\"),\n         sector = str_replace_all(sector,\"Psykiatri\",\"Psychiatry\"),\n         sector = str_replace_all(sector,\"Både somatik og psykiatri\",\"Both somatic and psychiatry\")\n         ) |&gt;\n  mutate(across(where(is.character), as.factor),\n         measure = str_squish(measure),\n         measure = recode(measure, !!!da_en, .default = measure),\n         value2 = value / 1000\n         )\n\n# Pilot Plot\ndf |&gt; filter(\n  measure == \"Persons with stays (number)\",\n  sector == \"Both somatic and psychiatry\" ) |&gt;\n  ggplot(aes(year, value2, color = age)) +\n  geom_line() +\n  facet_wrap( ~ sex) +\n  labs(x = NULL, y = \"Count in 1000s\")\n\n# Shiny app\n\ncards &lt;- list(\n  card(\n    full_screen = TRUE,\n    card_header(\"Somatic sector\"),\n    plotOutput(\"soma\")\n  ),\n  card(\n    full_screen = TRUE,\n    card_header(\"Psychiatric sector\"),\n    plotOutput(\"psych\")\n  ),\n  card(\n    full_screen = TRUE,\n    card_header(\"Both sectors\"),\n    plotOutput(\"somapsych\")\n  )\n)\n\nmeasure &lt;- varSelectInput(\n  \"measure\", \"Measure\",\n  distinct(df$measure),\n  selected = NULL\n)\n\nplot_sector &lt;- function(data, sector_label) {\n  data_sector &lt;- data |&gt; filter(sector == sector_label)\n  req(nrow(data_sector) &gt; 0)  # show nothing if that sector isn't present\n\n  ggplot(data_sector, aes(x = year, y = value, color = age)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~ sex, nrow = 2) +\n    theme(\n      plot.title = element_blank(),\n      legend.position = \"bottom\",\n    ) +\n    labs(\n      title = sector_label,\n      x = NULL, color = \"Age\",\n      y = NULL\n    ) +\n    theme_bw(base_size = 12)\n}\n\nmeasures &lt;- df |&gt;\n  distinct(measure) |&gt;\n  arrange(measure) |&gt;\n  pull(measure)\n\nui &lt;- page_sidebar(\n  title = \"Hospital stays — dashboard\",\n  sidebar = tagList(\n    selectInput(\"measure\", \"Measure\", choices = measures, selected = measures[1]),\n    helpText(\"All plots update to the selected measure.\"),\n    helpText(\"Lines = time\"),\n    helpText(\"Color = age\"),\n    helpText(\"Facets = sex\"),\n    helpText(\"Data has been supplied by Statistics Denmark (table SBR04), which contains aggregate data on hospital usage in Denmark between 2017-2024.\")\n  ),\n  # 3 cards side-by-side (wraps on narrow screens)\n  layout_columns(width = 1,\n     card(\n       full_screen = TRUE,\n       card_header(\"Somatic\"),\n       plotOutput(\"plot_somatic\", height = 300)\n     ),\n     card(\n       full_screen = TRUE,\n       card_header(\"Psychiatric\"),\n       plotOutput(\"plot_psychiatric\", height = 300)\n     ),\n     card(\n       full_screen = TRUE,\n       card_header(\"Both\"),\n       plotOutput(\"plot_both\", height = 300)\n     )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Filter once by measure; reuse for all sectors\n  dat_measure &lt;- reactive({\n    req(input$measure)\n    df |&gt; filter(measure == input$measure)\n  })\n\n  output$plot_somatic     &lt;- renderPlot(plot_sector(dat_measure(), sector_label = \"Somatic\"))\n  output$plot_psychiatric &lt;- renderPlot(plot_sector(dat_measure(), sector_label = \"Psychiatry\"))\n  output$plot_both        &lt;- renderPlot(plot_sector(dat_measure(), sector_label = \"Both somatic and psychiatry\"))\n\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/2025-06-17-steno2/index.html",
    "href": "posts/2025-06-17-steno2/index.html",
    "title": "Danish diabetes data",
    "section": "",
    "text": "I remember using the Epi::steno2 data-sets when taking a short course on multistate survival analysis with the creator of the package, Bendix Carstensen.\nThat was quite fun but also daunting to be sitting in a room full of mathematicians and people who actually knew the nitty gritty of survival analysis and the math behind it.\nI think I would just want to do something very basic here, though:\n\n\nPlaying around with joining the different data-sets\n\n\nExamine the data-sets and visualize the contents\n\n\nCreating a simple Cox Proportional Hazards model, comparing the risk of cardiovascular event between the two treatment groups in the data-set\n\n\nProbably also controlling for some important factors\n\n\nDeliver some sort of conclusion about these data\n\n\nSo lets start looking at it!"
  },
  {
    "objectID": "posts/2025-06-17-steno2/index.html#purpose",
    "href": "posts/2025-06-17-steno2/index.html#purpose",
    "title": "Danish diabetes data",
    "section": "",
    "text": "I remember using the Epi::steno2 data-sets when taking a short course on multistate survival analysis with the creator of the package, Bendix Carstensen.\nThat was quite fun but also daunting to be sitting in a room full of mathematicians and people who actually knew the nitty gritty of survival analysis and the math behind it.\nI think I would just want to do something very basic here, though:\n\n\nPlaying around with joining the different data-sets\n\n\nExamine the data-sets and visualize the contents\n\n\nCreating a simple Cox Proportional Hazards model, comparing the risk of cardiovascular event between the two treatment groups in the data-set\n\n\nProbably also controlling for some important factors\n\n\nDeliver some sort of conclusion about these data\n\n\nSo lets start looking at it!"
  },
  {
    "objectID": "posts/2025-06-17-steno2/index.html#load",
    "href": "posts/2025-06-17-steno2/index.html#load",
    "title": "Danish diabetes data",
    "section": "2 Load",
    "text": "2 Load\nThe data-sets can be loaded directly from the Epi library, through data()\n\ndata(\"steno2\")\ndata(\"st2clin\")\ndata(\"st2alb\")\n\nAlrighty now that they are loaded lets look at their structure. Ofcourse, I am completely ignoring the description in the documentation in the Epi() package, which describes how the data-sets are structured.\n\nstr(steno2)\n\n'data.frame':   160 obs. of  14 variables:\n $ id      : num  1 2 3 4 5 6 7 8 9 10 ...\n $ allo    : Factor w/ 2 levels \"Int\",\"Conv\": 1 1 2 2 2 2 2 1 1 1 ...\n $ sex     : Factor w/ 2 levels \"F\",\"M\": 2 2 2 2 2 2 1 2 2 2 ...\n $ baseCVD : num  0 0 0 0 0 1 0 0 0 0 ...\n $ deathCVD: num  0 0 0 0 1 0 0 0 1 0 ...\n $ doBth   : Date, format: \"1932-04-11\" \"1946-11-09\" ...\n $ doDM    : Date, format: \"1991-01-21\" \"1982-01-30\" ...\n $ doBase  : Date, format: \"1993-04-30\" \"1993-06-08\" ...\n $ doCVD1  : Date, format: \"2014-05-24\" \"2009-03-15\" ...\n $ doCVD2  : Date, format: NA \"2009-07-02\" ...\n $ doCVD3  : Date, format: NA \"2010-02-16\" ...\n $ doESRD  : Date, format: \"NaN\" \"NaN\" ...\n $ doEnd   : Date, format: \"2014-10-12\" \"2014-08-04\" ...\n $ doDth   : Date, format: NA NA ...\n\nstr(st2clin)\n\n'data.frame':   750 obs. of  5 variables:\n $ id  : num  1 2 3 4 5 6 7 8 9 10 ...\n $ doV : Date, format: \"1993-05-07\" \"1993-05-05\" ...\n $ a1c : num  87.3 66.5 73 61.2 102.7 ...\n $ chol: num  3.9 6.6 5.6 5.2 6 4.8 8.6 5.1 4.2 5.4 ...\n $ crea: num  83 83 68 97 149 55 56 78 123 79 ...\n\nstr(st2alb)\n\n'data.frame':   563 obs. of  3 variables:\n $ id   : num  1 1 1 1 1 2 2 2 2 2 ...\n $ doTr : Date, format: \"1993-06-12\" \"1995-05-13\" ...\n $ state: Factor w/ 3 levels \"Norm\",\"Mic\",\"Mac\": 2 1 2 1 2 1 2 3 2 2 ...\n\n\nIt looks like steno2 and st2clin are one row per subject, and st2alb is multiple, but that doesn’t make sense given the amount of rows. We can see how many id’s, the unique identifier for each person, are present, and if there is one row per subject it should be close to the amount of subjects.\n\n# create a 1 for each time there is an ID\nid_test1 &lt;- steno2 |&gt; count(id)\nhead(id_test1)\n\n  id n\n1  1 1\n2  2 1\n3  3 1\n4  4 1\n5  5 1\n6  6 1\n\n# different numbers on those counts\nid_test2 &lt;- id_test1 |&gt;  summarize(n=sum(n), idcount = max(id), sum = sum(id))\nid_test2\n\n    n idcount   sum\n1 160     176 13650\n\n\nAlright. So it is clear that only steno2 has one row per subject because the number of id’s that occur equal the number of unique rows.\nLets see about the others\n\nst2alb |&gt; count(id) |&gt; summarize(n=sum(n), idcount = max(id), sum = sum(id))\n\n    n idcount   sum\n1 563     176 13187\n\nst2clin |&gt; count(id) |&gt; summarize(n=sum(n), idcount = max(id), sum = sum(id))\n\n    n idcount   sum\n1 750     176 13650\n\n\nOk so the st2* datasets have more than one row per subject, but there is a strange lower sum in st2alb than the other datasets.\nCould it be that there are missing subjects in that dataset?\n\n2.1 A) Joining\nNow, first, what would the purpose be of joining these data-sets together?\nThere could be many reasons and I will touch upon some of them:\n\nadding demographic characteristics from one data-set to another to stratify analysis\nfind things that are in one data-set but not in another\n\nFor example, the st2clin and st2alb data-sets do not have the categories for each id from the steno2 data-set, such as sex and allocation.\nAlso, continuing from the previous section, were there any missing people in st2alb? There was a possible indication of it, which can be explored with a filtering join, more specifically, an anti_join, using the anti_join() function.\nanti_join() return all rows from x without a match in y\nSo let’s try to see if there are any missing.\n\nanti_join(steno2, st2alb, by = \"id\")\n\n   id allo sex baseCVD deathCVD      doBth       doDM     doBase     doCVD1\n1  83  Int   M       0        1 1930-12-23 1989-06-28 1992-12-22 1994-12-04\n2 102  Int   M       0        1 1935-04-09 1990-03-19 1993-07-17 1995-04-23\n3 128 Conv   M       0        0 1937-03-14 1989-02-16 1993-10-03 1995-11-13\n4 150 Conv   F       0        0 1946-02-18 1989-10-29 1993-12-24       &lt;NA&gt;\n      doCVD2     doCVD3     doESRD      doEnd      doDth\n1 1994-12-12       &lt;NA&gt;        NaN 1994-11-15 1994-12-19\n2       &lt;NA&gt;       &lt;NA&gt;        NaN 1995-03-30 1995-05-02\n3 2003-08-07 2004-05-31 1999-12-26 2005-05-19 2005-05-18\n4       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-27       &lt;NA&gt;\n\n\n\n\n [1] id       allo     sex      baseCVD  deathCVD doBth    doDM     doBase  \n [9] doCVD1   doCVD2   doCVD3   doESRD   doEnd    doDth   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nIt seems that id 83, 102, 128, and 150 are missing from st2alb. They are not missing from st2clin - I checked in a hidden code chunk. Since it’s stated in the documentation that it’s a data-set about transitions between states of albuminuria, perhaps one could assume that these individuals simply did not transition?"
  },
  {
    "objectID": "posts/2025-06-17-steno2/index.html#output",
    "href": "posts/2025-06-17-steno2/index.html#output",
    "title": "Danish diabetes data",
    "section": "3 Output",
    "text": "3 Output\n\n3.1 B) Table\nOf course, the good old summary() function might be worth to do to get a good overview of the three data-sets. First we do steno2 demographics.\n\nsummary(steno2)\n\n       id           allo    sex        baseCVD          deathCVD     \n Min.   :  1.00   Int :80   F: 41   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 40.75   Conv:80   M:119   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 82.50                     Median :0.0000   Median :0.0000  \n Mean   : 85.31                     Mean   :0.1375   Mean   :0.2375  \n 3rd Qu.:130.25                     3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :176.00                     Max.   :1.0000   Max.   :1.0000  \n                                                                     \n     doBth                 doDM                doBase          \n Min.   :1926-07-13   Min.   :1962-06-21   Min.   :1992-12-22  \n 1st Qu.:1932-07-18   1st Qu.:1982-10-01   1st Qu.:1993-03-23  \n Median :1937-03-12   Median :1987-05-02   Median :1993-06-11  \n Mean   :1938-06-03   Mean   :1986-01-15   Mean   :1993-07-22  \n 3rd Qu.:1944-11-28   3rd Qu.:1990-05-03   3rd Qu.:1993-11-15  \n Max.   :1956-04-07   Max.   :1994-02-07   Max.   :1994-06-14  \n                                                               \n     doCVD1               doCVD2               doCVD3          \n Min.   :1993-03-17   Min.   :1994-08-05   Min.   :1995-12-16  \n 1st Qu.:1997-02-16   1st Qu.:1999-04-18   1st Qu.:2001-11-23  \n Median :2001-04-03   Median :2002-12-20   Median :2004-12-13  \n Mean   :2001-12-18   Mean   :2003-10-04   Mean   :2005-01-11  \n 3rd Qu.:2006-07-20   3rd Qu.:2008-07-20   3rd Qu.:2009-06-03  \n Max.   :2014-05-30   Max.   :2014-03-26   Max.   :2012-07-21  \n NA's   :41           NA's   :84           NA's   :117         \n     doESRD               doEnd                doDth           \n Min.   :1998-02-18   Min.   :1994-11-16   Min.   :1994-12-19  \n 1st Qu.:2002-09-25   1st Qu.:2002-09-18   1st Qu.:2000-05-21  \n Median :2008-05-08   Median :2010-02-11   Median :2004-03-06  \n Mean   :2006-07-17   Mean   :2008-08-29   Mean   :2004-05-24  \n 3rd Qu.:2009-07-29   3rd Qu.:2014-09-25   3rd Qu.:2009-06-04  \n Max.   :2014-07-07   Max.   :2014-12-31   Max.   :2014-02-16  \n NA's   :145                               NA's   :67          \n\n\nBut I kinda want to mainly do graphs here.\n\n\n3.2 B) Graphs\n\n3.2.1 Some distributions\nNow, I love me some functions, and I feel like seeing the distribution of all the different date variables via histograms.\n\nplot_histo_spam &lt;- function(df,histovar) {\nout &lt;- ggplot(df) +\n  geom_histogram(mapping = aes( {{histovar}} ))\n\n  return(out)\n}\n\nplot_histo_spam(steno2,doBth)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nplot_histo_spam(steno2,doDM)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nplot_histo_spam(steno2,doBase)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBut this should be doable in a smarter way. A facet_wrap() over each of the date variables..\n\nplot_histospam_faceted &lt;- function(data, prefix = \"do\") {\n  data |&gt;\n    select(starts_with(prefix)) |&gt;\n    pivot_longer(everything()) |&gt; # everything() turns varnames into a column, and their values into a separate column\n    ggplot(aes(value)) +\n      geom_histogram(colour = \"white\") +\n      facet_wrap(~ name, scales = \"free_x\") +   # one panel per variable\n      labs(x = NULL)\n}\n\nplot_histospam_faceted(steno2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nplot_histospam_faceted(st2clin)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nplot_histospam_faceted(st2alb)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow those are some fine basic plots!\nFor steno2, it shows some slight right skewed doBase, and left-skewed doDM and doEnd. for the two st2* data-sets, it shows quite a right skew.\nBasically:\n\nmost of the signups to the trial occurred in the beginning\nmost of the CVD (cardiovascular) and death events occurred evenly-ish\nmost of the diabetes diagnoses occurred late\nnot much ESRD (end-stage renal disease) occurred\n\n\n\n3.2.2 Distributions by sex\nNow I want to figure out how to do it while grouping the histograms by sex.\n\nplot_histospam_faceted_sex &lt;- function(data, prefix = \"do\") {\n  data |&gt;\n    select(sex, starts_with(prefix)) |&gt;\n    pivot_longer(-sex,\n                 names_to = \"name\",\n                 values_to = \"value\") |&gt;\n    ggplot(aes(value, fill = sex)) +\n      geom_histogram(\n        position = \"dodge\",\n        colour = \"white\") +\n      facet_wrap(~ name, scales = \"free_x\") +   # one panel per variable\n      labs(x = NULL)\n}\n\nplot_histospam_faceted_sex(steno2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nOk. Not much of a difference at a glance.\n\n\n3.2.3 Distributions by treatment groups\nPerhaps it would be more relevant to see if there was some kind of difference between the two intervention groups?\n\nplot_histospam_faceted_allo &lt;- function(data, prefix = \"do\") {\n  data |&gt;\n    select(allo, starts_with(prefix)) |&gt;\n    pivot_longer(-allo,\n                 names_to = \"name\",\n                 values_to = \"value\") |&gt;\n    ggplot(aes(value, fill = allo)) +\n      geom_histogram(\n        position = \"identity\",\n        alpha    = 0.5,\n        colour = \"white\") +\n      facet_wrap(~ name, scales = \"free_x\") +   # one panel per variable\n      labs(x = NULL)\n}\n\nplot_histospam_faceted_allo(steno2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nOk so already here its possible to see some apparent differences between the two groups.\n\nthe Conv allocation group, which must be those allocated to conventional treatment, whatever that was, have more events of CVD, especially of the third CVD event. As well as earlier diabetes mellitus (DM) diagnoses. This is probably an indication of the future results of the survival analysis.\nInt group, likely the intervention group, seems to have less deaths and later DM diagnoses\n\nRemember, these are two groups of equal number, which can make it easier to conclude that there is a difference, at least initially and visually.\nNow. With the help of some AI, I made the function below which can produce different types of facet_* plots , but I choose the facet_grid, because its interesting to look at.\n\nsteno2 |&gt;\n    select(allo, starts_with(\"do\")) |&gt;\n    pivot_longer(-allo, names_to = \"variable\", values_to = \"value\") |&gt;\n    ggplot(aes(value, fill = allo)) +\n    geom_histogram(position = \"identity\", colour = \"white\") +\n        facet_grid(rows = vars(allo), cols = vars(variable)) +\n    #scale_x_continuous(breaks = floor(min(value)):ceiling(max(value)),\n    #               labels  = scales::number_format(accuracy = 1)) +\n    labs(x = NULL, fill = \"Allocation\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = .5, hjust = 1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThis might be information overload to such a degree that not much is gained from it.\n\n\n\n3.3 C) Model\nTo perform a basic Cox Proportional Hazards model, we can use the survival package.\nI want to do the following, just because that’s what I learned in the olden PhD studying days, mainly from the Basic Biostats courses from Aarhus University:\n\nSurvival set the data\nVisualise follow-up\nPlot Kaplan Meier survival function\nCox model\n\nEstimate\nObserved vs fitted survival curves\nTesting proportional hazards assumption\n\n\nLet’s just quickly and without much thinking (thats always good), decide what the start, stop, outcome, and other variables should be.\n\ndata: steno2\n\ncomparison: death doDth for intervention groups allo\nstart date: doBase\ncensor date: doEnd\noutcome event: doDth\nexit of study: earliest of doEnd and doDth\n\n\nSo, super basic, and does not take anything into account, such as competing events (doCVD* or doESRD) or baseline differences in risk of CVD (baseCVD).\n\n3.3.1 Survival set the data\nFirst we create the needed variables and check it.\n\nsteno2 &lt;- steno2 |&gt;\n  mutate(\n    doExit = coalesce(doDth, doEnd),\n    time_days = difftime(\n      coalesce(steno2$doDth, steno2$doEnd),\n      steno2$doBase,\n      units='days'),\n    time_yrs = as.numeric(time_days / 365.25),\n    event_num = if_else(!is.na(doDth),1,0),\n    event = factor(event_num, levels = c(0, 1), labels = c(\"Censored\", \"Dead\"))\n  )\n\nglimpse(steno2)\n\nRows: 160\nColumns: 19\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ allo      &lt;fct&gt; Int, Int, Conv, Conv, Conv, Conv, Conv, Int, Int, Int, Int, …\n$ sex       &lt;fct&gt; M, M, M, M, M, M, F, M, M, M, M, M, M, M, F, M, F, F, M, M, …\n$ baseCVD   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ deathCVD  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ doBth     &lt;date&gt; 1932-04-11, 1946-11-09, 1943-06-07, 1944-11-01, 1935-12-22,…\n$ doDM      &lt;date&gt; 1991-01-21, 1982-01-30, 1982-07-17, 1976-10-28, 1986-05-06,…\n$ doBase    &lt;date&gt; 1993-04-30, 1993-06-08, 1993-05-17, 1993-05-04, 1993-03-23,…\n$ doCVD1    &lt;date&gt; 2014-05-24, 2009-03-15, 2001-08-28, 1995-05-31, 1993-12-16,…\n$ doCVD2    &lt;date&gt; NA, 2009-07-02, NA, 1997-06-26, 1994-08-05, NA, 2006-02-27,…\n$ doCVD3    &lt;date&gt; NA, 2010-02-16, NA, 2003-06-17, 1997-12-25, NA, NA, NA, NA,…\n$ doESRD    &lt;date&gt; NaN, NaN, NaN, NaN, 1998-02-18, 2014-07-07, NaN, 2009-03-06…\n$ doEnd     &lt;date&gt; 2014-10-12, 2014-08-04, 2001-08-21, 2003-06-11, 1998-01-18,…\n$ doDth     &lt;date&gt; NA, NA, 2001-09-20, 2003-06-24, 1998-02-25, NA, 2006-04-24,…\n$ doExit    &lt;date&gt; 2014-10-12, 2014-08-04, 2001-09-20, 2003-06-24, 1998-02-25,…\n$ time_days &lt;drtn&gt; 7834.3828 days, 7726.4177 days, 3048.4091 days, 3703.4893 d…\n$ time_yrs  &lt;dbl&gt; 21.449371, 21.153779, 8.346089, 10.139601, 4.929256, 21.4398…\n$ event_num &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, …\n$ event     &lt;fct&gt; Censored, Censored, Dead, Dead, Dead, Censored, Dead, Dead, …\n\n\n\n\n3.3.2 Visualise follow-up\nThen, because I like it as an overview, a swimmer plot of the follow-up times.\nsimple first:\n\nswim_plot_1 &lt;- steno2 |&gt;\n  mutate(id = fct_reorder(as.factor(id), time_yrs)) |&gt;\n  ggplot(aes(x = doBase,\n             xend = doExit,\n             y = id)) +\n  geom_segment(color = \"grey\") +\n  geom_point(aes(x = doExit, shape = event)) +\n  geom_point(aes(x = doDth, shape = event))\nswim_plot_1\n\nWarning: Removed 67 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLooks good, but a tiny bit messy with the - although accurate - different starting times.\nLets do one with baseline time set to 0. This makes it easier to compare the follow-up times, which is more interesting for the analysis than calendar times.\nThe latter would probably be of more interest to an investigator trying to figure out when people signed up, or something.\n\nswim_plot_2 &lt;- steno2 |&gt;\n  mutate(\n    id = fct_reorder(as.factor(id), time_yrs),\n    t0 = 0) |&gt;\n  ggplot(aes(x = t0,\n             xend = time_yrs,\n             y = id)) +\n  geom_segment(color = \"grey\") +\n  geom_point(aes(x = time_yrs, shape = event, colour = event)) +\n  theme_minimal()\nswim_plot_2\n\n\n\n\n\n\n\n\nHere we have the x-axis in years since baseline.\nNow, I have a sense that doExit and doDth are the same, meaning that plots that use doExit end up having two shapes for subjects that died.\nHow can I make it so that there are different shapes for each event? I can just take the previous plot and add to it.\n\nswim_plot_3 &lt;- swim_plot_2 +\n  scale_shape_manual(values = c(Censored = 17, Dead = 4)) +\n  scale_colour_manual(values = c(Censored = \"black\", Dead = \"red\"))\nswim_plot_3\n\n\n\n\n\n\n\n\nOk that looks fine. Now I want to make the final plot for this and move on.\n\nswim_plot_4 &lt;- swim_plot_3 +\n  labs(\n    x = \"Follow-up time in years since baseline\",\n    y = \"Subject sorted by follow-up\",\n    title = \"Follow-up per subject, years since baseline\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank()\n    )\nswim_plot_4\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saved as thumbnail\n\nA nice little plot showing an overview of the length of follow-up for the subjects in the study.\nNot exactly sure how it is useful except for my own sake - I just like to know visually what my data looks like.\n\n\n3.3.3 Disclaimer\nI am not a statistician, so whatever is correct of below, is due to the wise words and lectures I have remembered from people who actually know what they are talking about. Many statistical results, especially when you wish to talk inference (X causes Y), require careful consideration of subjective priors, demographics, causal effects, and biases such as effect modifiers, confounders, selection bias, etc.\nNow - with that out of the way: LETS CLAIM SOME THINGS!\n\n\n3.3.4 Plot Kaplan Meier survival function\nNow, less fiddling around with plots, and more looking at survival analysis.\nThe Kaplan meier estimator on a plot is wonderful for portraying differences in survival times. First step we have is to fit the data.\n\n# Overall, by treatment allocation, subset into sex\nkm_os &lt;- survfit(Surv(time_yrs, event_num) ~ 1, data = steno2)\nkm_allo &lt;- survfit(Surv(time_yrs, event_num) ~ allo, data = steno2)\nkm_sex &lt;- survfit(Surv(time_yrs, event_num) ~ sex, data = steno2)\nkm_allo_sex &lt;- survfit(Surv(time_yrs, event_num) ~ allo + sex, data = steno2)\n\n# Summarising, showing just the first 10 years\nsummary(km_os, times=c(0:10))\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ 1, data = steno2)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    160       0    1.000 0.00000        1.000        1.000\n    1    160       0    1.000 0.00000        1.000        1.000\n    2    158       2    0.988 0.00878        0.970        1.000\n    3    155       3    0.969 0.01376        0.942        0.996\n    4    154       1    0.963 0.01502        0.934        0.992\n    5    148       6    0.925 0.02082        0.885        0.967\n    6    143       5    0.894 0.02436        0.847        0.943\n    7    137       6    0.856 0.02774        0.804        0.912\n    8    131       6    0.819 0.03045        0.761        0.881\n    9    125       6    0.781 0.03268        0.720        0.848\n   10    116       9    0.725 0.03530        0.659        0.798\n\nsummary(km_allo, times=c(0:10))\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ allo, data = steno2)\n\n                allo=Int \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     80       0    1.000  0.0000        1.000        1.000\n    1     80       0    1.000  0.0000        1.000        1.000\n    2     78       2    0.975  0.0175        0.941        1.000\n    3     77       1    0.963  0.0212        0.922        1.000\n    4     76       1    0.950  0.0244        0.903        0.999\n    5     75       1    0.938  0.0271        0.886        0.992\n    6     73       2    0.913  0.0316        0.853        0.977\n    7     70       3    0.875  0.0370        0.805        0.951\n    8     66       4    0.825  0.0425        0.746        0.913\n    9     64       2    0.800  0.0447        0.717        0.893\n   10     63       1    0.788  0.0457        0.703        0.882\n\n                allo=Conv \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     80       0    1.000  0.0000        1.000        1.000\n    1     80       0    1.000  0.0000        1.000        1.000\n    2     80       0    1.000  0.0000        1.000        1.000\n    3     78       2    0.975  0.0175        0.941        1.000\n    4     78       0    0.975  0.0175        0.941        1.000\n    5     73       5    0.913  0.0316        0.853        0.977\n    6     70       3    0.875  0.0370        0.805        0.951\n    7     67       3    0.838  0.0412        0.760        0.922\n    8     65       2    0.813  0.0436        0.731        0.903\n    9     61       4    0.763  0.0476        0.675        0.862\n   10     53       8    0.663  0.0529        0.567        0.775\n\nsummary(km_sex, times=c(0:10))\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ sex, data = steno2)\n\n                sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     41       0    1.000  0.0000        1.000        1.000\n    1     41       0    1.000  0.0000        1.000        1.000\n    2     41       0    1.000  0.0000        1.000        1.000\n    3     40       1    0.976  0.0241        0.930        1.000\n    4     40       0    0.976  0.0241        0.930        1.000\n    5     40       0    0.976  0.0241        0.930        1.000\n    6     39       1    0.951  0.0336        0.888        1.000\n    7     37       2    0.902  0.0463        0.816        0.998\n    8     34       3    0.829  0.0588        0.722        0.953\n    9     31       3    0.756  0.0671        0.635        0.900\n   10     29       2    0.707  0.0711        0.581        0.861\n\n                sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    119       0    1.000  0.0000        1.000        1.000\n    1    119       0    1.000  0.0000        1.000        1.000\n    2    117       2    0.983  0.0118        0.960        1.000\n    3    115       2    0.966  0.0165        0.935        0.999\n    4    114       1    0.958  0.0184        0.923        0.995\n    5    108       6    0.908  0.0266        0.857        0.961\n    6    104       4    0.874  0.0304        0.816        0.936\n    7    100       4    0.840  0.0336        0.777        0.909\n    8     97       3    0.815  0.0356        0.748        0.888\n    9     94       3    0.790  0.0373        0.720        0.867\n   10     87       7    0.731  0.0406        0.656        0.815\n\nsummary(km_allo_sex, times=c(0:10))\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ allo + sex, data = steno2)\n\n                allo=Int, sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     17       0    1.000  0.0000        1.000            1\n    1     17       0    1.000  0.0000        1.000            1\n    2     17       0    1.000  0.0000        1.000            1\n    3     17       0    1.000  0.0000        1.000            1\n    4     17       0    1.000  0.0000        1.000            1\n    5     17       0    1.000  0.0000        1.000            1\n    6     17       0    1.000  0.0000        1.000            1\n    7     17       0    1.000  0.0000        1.000            1\n    8     15       2    0.882  0.0781        0.742            1\n    9     14       1    0.824  0.0925        0.661            1\n   10     14       0    0.824  0.0925        0.661            1\n\n                allo=Int, sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     63       0    1.000  0.0000        1.000        1.000\n    1     63       0    1.000  0.0000        1.000        1.000\n    2     61       2    0.968  0.0221        0.926        1.000\n    3     60       1    0.952  0.0268        0.901        1.000\n    4     59       1    0.937  0.0307        0.878        0.999\n    5     58       1    0.921  0.0341        0.856        0.990\n    6     56       2    0.889  0.0396        0.815        0.970\n    7     53       3    0.841  0.0460        0.756        0.937\n    8     51       2    0.810  0.0495        0.718        0.913\n    9     50       1    0.794  0.0510        0.700        0.900\n   10     49       1    0.778  0.0524        0.682        0.888\n\n                allo=Conv, sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     24       0    1.000  0.0000        1.000        1.000\n    1     24       0    1.000  0.0000        1.000        1.000\n    2     24       0    1.000  0.0000        1.000        1.000\n    3     23       1    0.958  0.0408        0.882        1.000\n    4     23       0    0.958  0.0408        0.882        1.000\n    5     23       0    0.958  0.0408        0.882        1.000\n    6     22       1    0.917  0.0564        0.813        1.000\n    7     20       2    0.833  0.0761        0.697        0.997\n    8     19       1    0.792  0.0829        0.645        0.972\n    9     17       2    0.708  0.0928        0.548        0.916\n   10     15       2    0.625  0.0988        0.458        0.852\n\n                allo=Conv, sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     56       0    1.000  0.0000        1.000        1.000\n    1     56       0    1.000  0.0000        1.000        1.000\n    2     56       0    1.000  0.0000        1.000        1.000\n    3     55       1    0.982  0.0177        0.948        1.000\n    4     55       0    0.982  0.0177        0.948        1.000\n    5     50       5    0.893  0.0413        0.815        0.978\n    6     48       2    0.857  0.0468        0.770        0.954\n    7     47       1    0.839  0.0491        0.748        0.941\n    8     46       1    0.821  0.0512        0.727        0.928\n    9     44       2    0.786  0.0548        0.685        0.901\n   10     38       6    0.679  0.0624        0.567        0.813\n\n\nHere we can see that after 10 years, there is already a lower survival probability for the conventional treatment group. And for females, it seems. But that highly depends on the intervention group.\nBut it is much better to visualize it with a kaplan meier plot, to really see the differences, and when - in terms of time since baseline - does the survival differ.\nShown below are plots for each of the different fits mentioned above, showing the overall survival modelel, differences in survival between intervention groups for both sexes, and ditto, but subset for each sex.\n\n# Function to save some space\nsurvplot_function &lt;- function(fit,data){\n  out &lt;- ggsurvplot(\n    fit, data = data,\n    conf.int = TRUE,\n    risk.table = TRUE,\n    palette = \"dark2\",\n    xlab = NULL, ylab = \"Survival Probability\",\n    title = \"Kaplan Meier Estimator for survival probability, in years\"\n    )\n\n  return(out)\n}\n\n# Overall\nsurvplot_function(km_os,steno2)\n\n\n\n\n\n\n\n# allo\nsurvplot_function(km_allo,steno2)\n\n\n\n\n\n\n\n# sex\nsurvplot_function(km_sex,steno2)\n\n\n\n\n\n\n\n# allo sex\nsurvplot_function(km_allo_sex,steno2)\n\n\n\n\n\n\n\n\nThats a bit rough to look at, but provides plenty of information:\n\nintervention group has higher survival probability\nfemale sex seems to indicate having the highest survival probability, at least in intervention group\nfrequently overlapping confidence intervals, low probability of actual difference atleast in the first 10 years where most of the overlap occurs\n\nFor the second point, the lines cross twice, either because of the lower number of females or because of some unknown factor. When the lines cross its difficult to say whether there is any consistent survival difference with certainty.\nBut let’s try to make it pretty - just with the help of the survminer package help - accessed in Rstudio via ?ggsurvplot.\n\n# allo\nsurv_p_allo_pretty &lt;- ggsurvplot(\n  fit = km_allo, data = steno2,\n  surv.median.line = \"hv\", # Add medians survival\n  legend.title = \"Allocation group\",\n  legend.labs = c(\"Intervention\", \"Conventional\"),\n  pval = TRUE,\n  conf.int = TRUE,\n\n  risk.table = TRUE,\n  tables.height = 0.2,\n  tables.theme = theme(axis.title.y = element_blank()),\n\n  palette = \"dark2\",\n  ggtheme = theme_bw(),\n  xlab = NULL, ylab = \"Survival Probability\",\n  title = \"Kaplan Meier Estimator for survival probability, in years\"\n  )\n\nsurv_p_allo_sex_pretty &lt;- ggsurvplot(\n  fit = km_allo_sex, data = steno2,\n  surv.median.line = \"hv\", # Add medians survival\n  legend.title = \"Allocation group, Sex\",\n  legend.labs = c(\"Int, F\", \"Int, M\", \"Conv, F\", \"Conv, M\"),\n  conf.int = TRUE,\n\n  risk.table = TRUE,\n  tables.height = 0.2,\n  tables.theme = theme(axis.title.y = element_blank()),\n\n  palette = \"dark2\",\n  ggtheme = theme_bw(),\n  xlab = NULL, ylab = \"Survival Probability\",\n  title = \"Kaplan Meier Estimator for survival probability, in years\"\n  )\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\nsurv_p_allo_pretty\n\n\n\n\n\n\n\nsurv_p_allo_sex_pretty\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nNow slightly cleaned up, and with some median survival lines - the follow-up time at which 50 has experienced the event - added.\n\n3.3.4.1 Test of differences\nOne way of testing the differences between two survival curves is the log-rank test. Although, as far as I remember, its not given much weight - especially compared to visual inspection coupled with careful thinking and analysis of the different factors that play into a survival analysis.\n\nsurvdiff(Surv(time_yrs, event_num) ~ allo, data = steno2)\n\nCall:\nsurvdiff(formula = Surv(time_yrs, event_num) ~ allo, data = steno2)\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\nallo=Int  80       38     51.7      3.64      8.29\nallo=Conv 80       55     41.3      4.56      8.29\n\n Chisq= 8.3  on 1 degrees of freedom, p= 0.004 \n\nsurvdiff(Surv(time_yrs, event_num) ~ sex, data = steno2)\n\nCall:\nsurvdiff(formula = Surv(time_yrs, event_num) ~ sex, data = steno2)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F  41       20     25.5     1.182      1.63\nsex=M 119       73     67.5     0.446      1.63\n\n Chisq= 1.6  on 1 degrees of freedom, p= 0.2 \n\n\nSeems there is a statistically significant difference between allocation groups and not sex. Kinda what I expected from the reading of the plots.\nI have forgotten how to interpret the log-rank model for a fit with two different binary variables, so I will refrain from attempting that at the moment.\n\n\n\n3.3.5 Cox model\nNow we enter the part of survival analysis where we want to see what effect sizes the different predictors have on the outcome.\nAs mentioned in Section 3.3.3, I am not a statistician, and I do not think the target audience of 0 to 1/3 of a person should be a statistician, so I will not bombard You with all the equations and definitions of survival analysis, such as \\(S(t) = P(T&gt;t) = etc..etc\\).\nI will just try to write in plain terms.\nWhere the survival function derived from the Kaplan Meier Estimator up above could tell us how many people have survived at time \\(t\\), and the difference between groups, the cox proportional hazards model can describe the relative difference in effect between different predictors, in regards to the risk of the outcome.\nSpoken plainly: the cox proportional hazards model can tell us something about how much the risk of the outcome, doDth, increases or decreases, for those in one group compared to another group. And then if there are multiple variables in the analysis, its assumed all other variables stay the same.\nLet’s do the same models as for the survival function.\n\n# Overall, by treatment allocation, subset into sex\ncox_allo &lt;- coxph(Surv(time_yrs, event_num) ~ allo, data = steno2)\ncox_sex &lt;- coxph(Surv(time_yrs, event_num) ~ sex, data = steno2)\ncox_allo_sex &lt;- coxph(Surv(time_yrs, event_num) ~ allo + sex, data = steno2)\n\n\n3.3.5.1 Checking models out before interpreting estimates\nBefore we go ahead and interpret estimates, it is recommended that you see how the data fits with the assumptions that are necessary for the cox model to be interpreted correctly.\nIn general, I have been taught to test the cox proportional hazards assumption, which can be done in two ways:\n\nplot observed survival curves in the two groups with estimated survival curves from the model\nlog log plot of two survival curves\n\n\nplot(km_allo,\n     col=1:2,\n     lty=2,\n     main='Observed and fitted survival')\nlines(survfit(cox_allo,\n              newdata=data.frame(allo=c('Int','Conv'))),\n      col=1:2)\nlegend('bottomleft',\n       lty=rep(1:2,2),col=rep(1:2,each=2),\n       legend=c('Int, fit','Int, obs','Conv, fit','Conv, obs'))\n\n\n\n\n\n\n\nplot(km_allo,\n     col=1:2,\n     fun='cloglog',\n     main='Log-log plot')\nlegend('topleft',lty=rep(1,2),col=1:2,legend=levels(steno2$allo))\n\n\n\n\n\n\n\n\nHmm. I also wanted to use ggsurvplot but for some reason, I can only get the old code, without risk table, from my course long ago in basic biostats to work, so I will use that.\nBut more importantly:\nBasically what we can see from the two above plots is that the proportional hazards assumption doesn’t hold well, due to crossover of survival curves and perhaps a slight possibility of different baseline hazards.\nIf the hazards for the groups being compared are not relatively proportional over time, its estimates do not precisely answer the question of “how much more risk is one group in of the outcome versus the other”, since it varies over time.\nIt answers a general tendency, which might switch at some point in the data, as can be seen by earlier plots where observed survival curves overlap early.\n\n\n3.3.5.2 Estimates\nBut lets still look at the model estimates and interpret them a bit, but bearing in mind that the proportional hazards assumption doesn’t hold.\nFor a more simple output of the models, I use broom::tidy().\n\n# list\ncox_models &lt;- list(cox_allo,cox_sex,cox_allo_sex)\nnames(cox_models) &lt;- paste0(\"model_\", seq_along(cox_models))\n\n# tidy\ntidy_cox_models &lt;- map_dfr(cox_models, ~broom::tidy(.x, conf.int = TRUE))\ntidy_cox_models\n\n# A tibble: 4 × 7\n  term     estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 alloConv    0.602     0.212      2.84 0.00455    0.186     1.02 \n2 sexM        0.321     0.253      1.27 0.203     -0.174     0.816\n3 alloConv    0.628     0.213      2.95 0.00316    0.211     1.05 \n4 sexM        0.379     0.253      1.50 0.135     -0.118     0.875\n\ntidy_cox_models_exp &lt;- map_dfr(cox_models, ~broom::tidy(.x, conf.int = TRUE, exponentiate = TRUE))\ntidy_cox_models_exp\n\n# A tibble: 4 × 7\n  term     estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 alloConv     1.83     0.212      2.84 0.00455    1.20       2.77\n2 sexM         1.38     0.253      1.27 0.203      0.840      2.26\n3 alloConv     1.87     0.213      2.95 0.00316    1.23       2.84\n4 sexM         1.46     0.253      1.50 0.135      0.889      2.40\n\n\nWhere the first two rows are the first two models, and row three to four is the model with both variables, so you can see how much of an effect it has if you include them both.\nLet’s go ahead and do a simple interpretation on the model with both allocation and sex as predictors.\nUsing broom::augment, one can plop the estimates into a table as an overview of what the model estimates mean for each variable.\n\ncox_grid_1 &lt;- expand.grid(allo = c(\"Conv\", \"Int\"))\ncox_grid_2 &lt;- expand.grid(allo = c(\"Conv\", \"Int\"), sex = c(\"M\",\"F\"))\n(broom::augment(cox_allo,newdata = cox_grid_1, type.predict = \"risk\"))\n\n# A tibble: 2 × 3\n  allo  .fitted .se.fit\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Conv     1.83   0.287\n2 Int      1      0    \n\n(broom::augment(cox_allo_sex,newdata = cox_grid_2, type.predict = \"risk\"))\n\n# A tibble: 4 × 4\n  allo  sex   .fitted .se.fit\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Conv  M        2.74   0.567\n2 Int   M        1.46   0.306\n3 Conv  F        1.87   0.291\n4 Int   F        1      0    \n\n\nDue to the type.predict option, the result is exponentiated so it can be interpreted as risk increase.\nSo, the model output for cox_allo_sex can be interpreted in the following way:\nFemales on intervention treatment are the reference group, and females with conventional treatment have 87% higher risk of death, while males on intervention treatment have 46%, and males on conventional 174% higher risk."
  },
  {
    "objectID": "posts/2025-06-17-steno2/index.html#summary-cleaned-up-refactored",
    "href": "posts/2025-06-17-steno2/index.html#summary-cleaned-up-refactored",
    "title": "Danish diabetes data",
    "section": "4 Summary / cleaned up / refactored",
    "text": "4 Summary / cleaned up / refactored\n\n\nRows: 160\nColumns: 14\n$ id       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ allo     &lt;fct&gt; Int, Int, Conv, Conv, Conv, Conv, Conv, Int, Int, Int, Int, C…\n$ sex      &lt;fct&gt; M, M, M, M, M, M, F, M, M, M, M, M, M, M, F, M, F, F, M, M, F…\n$ baseCVD  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ deathCVD &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ doBth    &lt;date&gt; 1932-04-11, 1946-11-09, 1943-06-07, 1944-11-01, 1935-12-22, …\n$ doDM     &lt;date&gt; 1991-01-21, 1982-01-30, 1982-07-17, 1976-10-28, 1986-05-06, …\n$ doBase   &lt;date&gt; 1993-04-30, 1993-06-08, 1993-05-17, 1993-05-04, 1993-03-23, …\n$ doCVD1   &lt;date&gt; 2014-05-24, 2009-03-15, 2001-08-28, 1995-05-31, 1993-12-16, …\n$ doCVD2   &lt;date&gt; NA, 2009-07-02, NA, 1997-06-26, 1994-08-05, NA, 2006-02-27, …\n$ doCVD3   &lt;date&gt; NA, 2010-02-16, NA, 2003-06-17, 1997-12-25, NA, NA, NA, NA, …\n$ doESRD   &lt;date&gt; NaN, NaN, NaN, NaN, 1998-02-18, 2014-07-07, NaN, 2009-03-06,…\n$ doEnd    &lt;date&gt; 2014-10-12, 2014-08-04, 2001-08-21, 2003-06-11, 1998-01-18, …\n$ doDth    &lt;date&gt; NA, NA, 2001-09-20, 2003-06-24, 1998-02-25, NA, 2006-04-24, …\nRows: 750\nColumns: 5\n$ id   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ doV  &lt;date&gt; 1993-05-07, 1993-05-05, 1993-05-11, 1993-05-12, 1993-02-25, 1993…\n$ a1c  &lt;dbl&gt; 87.3, 66.5, 73.0, 61.2, 102.7, 93.4, 94.5, 103.9, 46.4, 84.4, 47.…\n$ chol &lt;dbl&gt; 3.9, 6.6, 5.6, 5.2, 6.0, 4.8, 8.6, 5.1, 4.2, 5.4, 8.9, 5.3, 6.0, …\n$ crea &lt;dbl&gt; 83, 83, 68, 97, 149, 55, 56, 78, 123, 79, 74, 84, 65, 52, 61, 61,…\nRows: 563\nColumns: 3\n$ id    &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 6…\n$ doTr  &lt;date&gt; 1993-06-12, 1995-05-13, 2000-01-26, 2001-12-26, 2007-04-27, 199…\n$ state &lt;fct&gt; Mic, Norm, Mic, Norm, Mic, Norm, Mic, Mac, Mic, Mic, Mic, Mic, M…\n\n\n[[1]]\n     id allo sex baseCVD deathCVD      doBth       doDM     doBase     doCVD1\n1     1  Int   M       0        0 1932-04-11 1991-01-21 1993-04-30 2014-05-24\n2     2  Int   M       0        0 1946-11-09 1982-01-30 1993-06-08 2009-03-15\n3     3 Conv   M       0        0 1943-06-07 1982-07-17 1993-05-17 2001-08-28\n4     4 Conv   M       0        0 1944-11-01 1976-10-28 1993-05-04 1995-05-31\n5     5 Conv   M       0        1 1935-12-22 1986-05-06 1993-03-23 1993-12-16\n6     6 Conv   M       1        0 1946-12-23 1986-06-03 1993-03-31 1998-10-21\n7     7 Conv   F       0        0 1929-02-17 1977-12-18 1993-03-28 2000-12-27\n8     8  Int   M       0        0 1932-10-30 1982-10-26 1993-04-10 2011-11-16\n9     9  Int   M       0        1 1930-06-26 1962-06-21 1993-03-22 1995-01-21\n10   10  Int   M       0        0 1937-06-28 1983-01-24 1993-01-09 2011-05-23\n11   11  Int   M       0        1 1930-02-08 1991-10-27 1993-02-11 2005-08-27\n12   12 Conv   M       1        1 1928-04-03 1981-05-12 1993-03-11 1993-12-14\n13   13  Int   M       0        0 1945-09-07 1990-03-23 1993-03-01       &lt;NA&gt;\n14   14  Int   M       0        0 1929-10-08 1981-03-26 1993-02-10 2014-02-09\n15   15  Int   F       0        0 1936-03-29 1988-06-10 1993-01-05       &lt;NA&gt;\n16   16  Int   M       1        0 1934-12-14 1987-07-18 1993-01-19 2000-01-20\n17   17  Int   F       0        0 1942-12-16 1984-10-27 1993-06-11       &lt;NA&gt;\n18   18 Conv   F       0        0 1939-04-11 1966-06-11 1993-06-08 1998-04-05\n19   19 Conv   M       0        0 1945-12-29 1992-10-21 1993-06-02       &lt;NA&gt;\n20   20 Conv   M       0        0 1946-05-08 1988-07-27 1993-06-07       &lt;NA&gt;\n21   21 Conv   F       0        0 1935-09-16 1981-11-03 1993-06-11 1994-05-05\n22   22  Int   F       0        0 1933-07-13 1975-08-11 1993-05-16       &lt;NA&gt;\n23   23 Conv   M       0        1 1934-06-19 1980-02-22 1993-01-12 1993-03-17\n24   24  Int   M       0        0 1937-06-16 1989-08-04 1993-03-11 2005-05-28\n25   25 Conv   M       0        0 1935-09-04 1991-01-17 1993-04-04       &lt;NA&gt;\n26   26 Conv   M       0        1 1929-08-18 1990-03-17 1993-04-09 1999-09-18\n27   27 Conv   M       0        0 1931-03-02 1983-02-17 1993-04-16 2002-07-29\n28   28  Int   M       0        0 1945-09-26 1990-03-14 1993-03-13       &lt;NA&gt;\n29   29 Conv   M       0        1 1939-03-11 1969-08-03 1993-03-26 2005-02-22\n30   30  Int   M       1        0 1932-07-09 1982-05-02 1993-05-03 2004-05-04\n31   31 Conv   F       0        1 1942-06-30 1986-04-29 1993-03-12 2006-02-26\n32   32 Conv   F       0        0 1944-10-25 1991-10-01 1993-03-21 2000-02-23\n33   33  Int   M       0        0 1941-05-03 1988-10-14 1993-03-16       &lt;NA&gt;\n34   34  Int   M       0        0 1941-10-30 1990-02-07 1993-02-28 1996-01-06\n35   35 Conv   F       0        1 1928-08-29 1991-09-19 1993-06-30 2001-08-23\n36   36 Conv   F       1        0 1947-02-12 1964-02-07 1993-04-14 2001-12-02\n37   37  Int   M       0        0 1948-02-01 1987-03-27 1993-01-24 2008-09-28\n38   38 Conv   M       0        1 1932-06-14 1986-10-14 1993-03-07 1998-09-05\n39   39  Int   M       0        0 1944-06-23 1985-07-10 1993-01-20 2010-10-27\n40   40 Conv   F       0        1 1937-12-12 1984-08-12 1993-04-02 1997-09-08\n41   41  Int   M       0        0 1928-11-12 1991-06-28 1992-12-29 2002-06-26\n42   42 Conv   M       0        1 1936-02-03 1974-11-23 1993-03-12 2002-10-01\n43   43 Conv   M       1        0 1930-08-28 1979-03-04 1993-01-14 1994-01-29\n44   44 Conv   M       1        0 1948-06-14 1985-04-28 1993-02-23       &lt;NA&gt;\n45   45 Conv   M       0        0 1945-08-27 1990-11-01 1993-02-28 2010-03-17\n46   47 Conv   F       0        0 1937-05-25 1974-12-14 1993-04-30 1997-05-15\n47   48 Conv   F       0        0 1926-09-12 1984-07-19 1993-06-24 2010-11-04\n48   50 Conv   M       0        1 1929-12-21 1993-04-18 1993-05-28 2002-07-06\n49   51  Int   M       0        0 1946-05-20 1981-09-04 1993-06-04 2010-07-20\n50   52  Int   M       0        0 1944-07-14 1989-07-01 1993-02-03 1996-12-01\n51   53 Conv   F       0        0 1927-06-17 1982-07-04 1993-03-05 1996-12-16\n52   54  Int   F       1        0 1945-04-26 1989-06-21 1993-02-26       &lt;NA&gt;\n53   55  Int   F       0        0 1948-04-10 1987-05-13 1993-04-15       &lt;NA&gt;\n54   56  Int   M       0        0 1932-09-16 1989-03-06 1993-03-11 2012-04-16\n55   57 Conv   F       0        0 1928-11-22 1974-02-16 1993-02-05 1995-08-17\n56   58 Conv   M       0        0 1938-03-11 1987-08-26 1993-02-04 2005-05-22\n57   59  Int   M       0        0 1929-04-17 1985-10-31 1993-07-02 2004-10-19\n58   60  Int   M       0        1 1936-02-27 1986-04-05 1993-06-24 1997-09-22\n59   61 Conv   M       0        0 1941-06-28 1986-08-29 1993-05-16       &lt;NA&gt;\n60   62 Conv   F       0        1 1935-01-09 1985-07-16 1993-06-22 1999-04-06\n61   63 Conv   M       0        1 1931-10-21 1987-07-25 1993-04-04 1999-01-01\n62   64 Conv   M       0        0 1944-06-07 1990-01-01 1993-03-19 2006-10-22\n63   65  Int   F       0        0 1946-01-30 1975-02-02 1993-05-05       &lt;NA&gt;\n64   66 Conv   M       0        1 1932-10-14 1979-09-30 1993-03-01 1995-12-23\n65   67  Int   F       0        0 1927-08-15 1978-08-23 1993-05-11 1994-02-11\n66   68  Int   M       0        0 1947-06-18 1989-10-11 1993-03-30 2000-10-09\n67   69  Int   M       0        0 1933-01-08 1991-11-08 1993-03-04       &lt;NA&gt;\n68   70 Conv   M       0        0 1934-10-24 1978-10-12 1993-01-25 1995-03-26\n69   71 Conv   M       0        0 1937-03-10 1982-05-14 1993-05-18 2004-08-21\n70   72 Conv   M       0        0 1947-10-27 1990-09-03 1993-05-03       &lt;NA&gt;\n71   73  Int   M       0        0 1930-04-17 1977-02-16 1993-01-22 1997-01-29\n72   74  Int   F       1        0 1928-08-14 1974-05-16 1993-05-12 1997-03-06\n73   75 Conv   F       0        1 1935-03-04 1981-01-21 1993-03-19 2008-11-21\n74   76  Int   M       0        0 1936-06-29 1980-04-16 1993-02-17 1996-09-05\n75   77  Int   M       0        0 1938-04-03 1991-10-10 1993-03-23       &lt;NA&gt;\n76   78 Conv   M       0        0 1928-05-30 1985-07-21 1993-04-08       &lt;NA&gt;\n77   79  Int   M       0        1 1931-12-03 1974-02-14 1993-04-07 1994-10-31\n78   80 Conv   M       0        0 1940-07-03 1990-03-22 1993-04-04 2008-05-21\n79   81  Int   M       0        1 1943-12-17 1990-03-22 1993-03-21 1999-12-18\n80   82  Int   M       1        0 1927-09-07 1990-10-13 1993-01-02 1999-02-24\n81   83  Int   M       0        1 1930-12-23 1989-06-28 1992-12-22 1994-12-04\n82   84  Int   F       0        0 1932-07-21 1983-11-16 1993-03-29 2009-04-03\n83   85 Conv   M       0        0 1943-05-22 1987-04-22 1993-03-31 2009-09-26\n84   86  Int   F       0        0 1932-11-18 1973-04-09 1993-06-01       &lt;NA&gt;\n85   87  Int   M       0        1 1933-03-21 1984-06-25 1993-04-20 1993-10-17\n86   88 Conv   M       0        0 1942-01-13 1986-05-16 1993-02-15 1998-01-18\n87   89  Int   M       1        0 1935-08-18 1992-06-30 1993-06-30 1995-05-20\n88   90 Conv   M       0        1 1947-01-22 1988-10-16 1993-07-27 2001-07-03\n89   91  Int   M       0        0 1930-10-24 1990-03-02 1993-08-23 2001-08-05\n90   92  Int   M       1        0 1946-03-10 1992-09-02 1993-08-25       &lt;NA&gt;\n91   93 Conv   M       0        0 1951-05-04 1980-07-10 1993-08-20 1998-08-22\n92   94 Conv   M       0        1 1940-03-21 1993-05-14 1993-07-19 2007-09-24\n93   95  Int   M       0        0 1938-01-14 1992-02-25 1993-09-01 2004-03-20\n94   96  Int   M       0        1 1942-02-13 1988-05-17 1993-08-26 1998-12-31\n95   97  Int   F       0        0 1946-05-12 1992-12-19 1993-08-24       &lt;NA&gt;\n96   98  Int   M       0        1 1933-09-20 1975-08-29 1993-08-27 1997-05-04\n97  100  Int   M       1        0 1949-11-13 1985-12-15 1993-09-10       &lt;NA&gt;\n98  101  Int   M       0        0 1936-06-03 1988-12-28 1994-02-07       &lt;NA&gt;\n99  102  Int   M       0        1 1935-04-09 1990-03-19 1993-07-17 1995-04-23\n100 103  Int   M       0        0 1932-10-18 1981-07-20 1993-08-28       &lt;NA&gt;\n101 104  Int   M       0        0 1946-12-12 1986-02-02 1993-07-22 2014-05-30\n102 105  Int   M       1        0 1949-01-14 1993-12-06 1994-01-15       &lt;NA&gt;\n103 107  Int   F       0        0 1936-07-26 1992-04-10 1993-09-01 2001-06-30\n104 108  Int   M       0        0 1956-04-07 1992-05-20 1993-08-28       &lt;NA&gt;\n105 110  Int   M       0        0 1934-07-13 1989-01-27 1993-09-14 2009-12-09\n106 111  Int   M       0        0 1951-05-15 1993-05-30 1993-08-16       &lt;NA&gt;\n107 112 Conv   F       0        0 1949-01-18 1988-09-06 1993-11-02       &lt;NA&gt;\n108 113 Conv   F       0        0 1931-05-19 1966-10-24 1993-10-05 1999-02-17\n109 114 Conv   M       0        0 1934-07-21 1982-11-05 1993-10-02 1998-06-01\n110 115 Conv   M       0        0 1940-11-12 1984-05-15 1993-10-30       &lt;NA&gt;\n111 116 Conv   M       0        1 1928-09-06 1987-05-12 1993-09-12 1998-07-22\n112 118 Conv   M       1        1 1933-09-07 1988-10-20 1993-10-09 2006-04-18\n113 119 Conv   M       0        0 1949-08-17 1993-03-07 1993-09-14 1998-02-28\n114 120 Conv   M       0        0 1939-04-06 1984-01-23 1993-10-27 1996-06-20\n115 121 Conv   M       1        0 1927-04-30 1978-01-22 1993-11-16 1997-01-28\n116 123  Int   M       0        0 1948-03-02 1991-01-13 1993-10-10 2013-02-04\n117 126  Int   F       0        0 1934-07-27 1986-06-28 1993-10-21       &lt;NA&gt;\n118 128 Conv   M       0        0 1937-03-14 1989-02-16 1993-10-03 1995-11-13\n119 129 Conv   M       0        1 1936-06-24 1988-07-23 1993-10-17 2002-11-20\n120 130 Conv   M       0        1 1939-07-01 1980-11-23 1993-10-15 1996-01-16\n121 131 Conv   M       1        0 1931-10-07 1992-02-15 1993-11-27 2003-12-18\n122 133  Int   M       0        0 1945-08-12 1992-09-24 1994-02-21 2007-12-24\n123 134 Conv   F       0        1 1930-01-31 1980-08-28 1993-11-14 1994-12-01\n124 135 Conv   F       0        0 1931-07-05 1993-04-16 1993-10-12 1997-06-05\n125 136 Conv   M       0        0 1949-10-23 1983-03-15 1993-12-16 2000-06-12\n126 137 Conv   M       0        0 1932-09-11 1993-09-04 1993-10-31 1998-09-07\n127 138 Conv   F       0        0 1947-10-05 1993-04-24 1993-12-14       &lt;NA&gt;\n128 139 Conv   M       0        0 1949-05-08 1988-12-30 1993-11-22 1995-07-14\n129 140 Conv   M       1        0 1951-04-14 1987-08-10 1993-11-28 2001-04-03\n130 142  Int   M       0        0 1942-05-15 1989-06-26 1993-12-03 2007-09-04\n131 143  Int   M       0        0 1936-10-31 1985-11-01 1993-12-12 2002-05-30\n132 144 Conv   M       1        0 1932-11-21 1987-04-20 1993-12-19 2001-05-17\n133 145  Int   M       0        1 1936-11-21 1985-12-17 1994-02-01 1999-04-25\n134 146 Conv   M       0        1 1944-05-25 1983-11-18 1994-01-31 1994-07-17\n135 147  Int   M       1        0 1945-02-16 1994-01-04 1994-01-04 2012-08-25\n136 148 Conv   F       0        0 1938-12-29 1986-04-07 1993-12-26 2012-06-20\n137 150 Conv   F       0        0 1946-02-18 1989-10-29 1993-12-24       &lt;NA&gt;\n138 151 Conv   M       0        1 1929-02-04 1988-05-12 1994-01-25 2004-02-28\n139 152  Int   F       1        0 1947-11-08 1993-06-08 1994-01-13 2009-04-09\n140 153  Int   M       0        0 1934-10-31 1985-04-01 1994-02-17 1995-09-22\n141 155 Conv   M       0        0 1949-11-21 1994-02-07 1994-02-12       &lt;NA&gt;\n142 156  Int   M       0        0 1942-09-10 1993-05-13 1994-02-10 2002-04-25\n143 157 Conv   M       0        1 1938-03-08 1987-05-24 1994-02-27 2007-07-03\n144 158  Int   M       1        0 1931-06-17 1986-09-07 1994-01-22 2009-12-10\n145 159  Int   M       0        1 1926-07-13 1990-10-20 1994-01-10 2010-06-08\n146 160 Conv   M       0        0 1928-06-20 1986-12-03 1994-02-02 1994-12-13\n147 161  Int   M       0        0 1939-03-16 1989-11-10 1994-02-25       &lt;NA&gt;\n148 163  Int   M       0        0 1929-12-31 1990-11-12 1994-01-24 2002-02-17\n149 165 Conv   F       0        0 1930-08-16 1992-09-29 1994-02-21 2002-04-04\n150 166 Conv   M       0        1 1936-09-03 1986-08-23 1994-01-25 2007-04-19\n151 167  Int   F       0        0 1952-11-12 1980-07-07 1994-03-09       &lt;NA&gt;\n152 168 Conv   M       0        0 1937-05-19 1986-03-12 1994-02-13 2002-03-10\n153 169  Int   M       0        0 1946-10-08 1993-09-03 1994-05-12       &lt;NA&gt;\n154 170  Int   F       0        0 1933-04-02 1991-10-24 1994-05-19       &lt;NA&gt;\n155 171  Int   M       0        0 1939-07-02 1992-09-22 1994-04-23       &lt;NA&gt;\n156 172 Conv   F       0        0 1950-11-23 1983-09-27 1994-05-05 2011-12-25\n157 173  Int   F       0        0 1943-10-29 1982-07-03 1994-05-19       &lt;NA&gt;\n158 174  Int   M       0        0 1931-10-27 1989-04-01 1994-06-14       &lt;NA&gt;\n159 175  Int   M       0        0 1936-12-26 1991-12-17 1994-05-06 1998-10-01\n160 176 Conv   F       0        0 1943-02-12 1987-10-23 1994-05-25       &lt;NA&gt;\n        doCVD2     doCVD3     doESRD      doEnd      doDth\n1         &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-12       &lt;NA&gt;\n2   2009-07-02 2010-02-16        NaN 2014-08-04       &lt;NA&gt;\n3         &lt;NA&gt;       &lt;NA&gt;        NaN 2001-08-21 2001-09-20\n4   1997-06-26 2003-06-17        NaN 2003-06-11 2003-06-24\n5   1994-08-05 1997-12-25 1998-02-18 1998-01-18 1998-02-25\n6         &lt;NA&gt;       &lt;NA&gt; 2014-07-07 2014-09-07       &lt;NA&gt;\n7   2006-02-27       &lt;NA&gt;        NaN 2006-03-28 2006-04-24\n8   2012-02-12       &lt;NA&gt; 2009-03-06 2012-02-25 2012-02-27\n9   1995-11-25       &lt;NA&gt;        NaN 1995-12-14 1995-12-05\n10        &lt;NA&gt;       &lt;NA&gt;        NaN 2011-05-28 2011-05-30\n11  2006-01-23       &lt;NA&gt; 2006-01-11 2006-01-26 2006-02-03\n12  1995-04-27 2002-05-16        NaN 2002-09-28 2002-09-15\n13        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-25       &lt;NA&gt;\n14        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-01-15 2014-02-16\n15        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-01       &lt;NA&gt;\n16  2002-03-05 2009-08-30        NaN 2009-09-05 2009-09-07\n17        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-01       &lt;NA&gt;\n18  1998-06-11 1998-08-20        NaN 2014-12-30       &lt;NA&gt;\n19        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n20        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-02       &lt;NA&gt;\n21        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-16       &lt;NA&gt;\n22        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-13       &lt;NA&gt;\n23  1998-12-22       &lt;NA&gt;        NaN 1998-12-30 1998-12-29\n24  2007-09-07 2010-03-02        NaN 2010-02-26 2010-03-09\n25        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-14       &lt;NA&gt;\n26  2000-01-20 2001-11-09        NaN 2001-11-09 2001-11-16\n27  2006-02-15 2006-05-07        NaN 2009-03-25 2009-03-17\n28        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-06       &lt;NA&gt;\n29        &lt;NA&gt;       &lt;NA&gt;        NaN 2005-03-22 2005-03-23\n30  2005-08-11       &lt;NA&gt;        NaN 2005-07-18 2005-08-22\n31  2007-04-04 2008-07-03        NaN 2010-05-06 2010-04-21\n32        &lt;NA&gt;       &lt;NA&gt;        NaN 2000-03-03 2000-03-01\n33        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-01       &lt;NA&gt;\n34  2005-05-28 2012-07-21        NaN 2014-09-29       &lt;NA&gt;\n35  2002-11-01 2002-12-14 2003-10-23 2005-02-02 2005-03-09\n36        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-26       &lt;NA&gt;\n37        &lt;NA&gt;       &lt;NA&gt;        NaN 2008-09-12 2008-10-05\n38  2000-03-22 2000-05-08        NaN 2003-04-28 2003-04-17\n39        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-27       &lt;NA&gt;\n40  2000-01-13 2000-02-04        NaN 2000-01-05 2000-02-11\n41        &lt;NA&gt;       &lt;NA&gt;        NaN 2002-06-21 2002-07-18\n42  2002-12-05 2004-12-13        NaN 2005-11-02 2005-11-03\n43  2002-03-08 2002-06-10        NaN 2002-05-14 2002-06-17\n44        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-14       &lt;NA&gt;\n45        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-29       &lt;NA&gt;\n46  2001-10-08 2001-12-07        NaN 2002-07-09 2002-07-09\n47        &lt;NA&gt;       &lt;NA&gt;        NaN 2010-10-16 2010-11-11\n48  2002-07-28       &lt;NA&gt;        NaN 2002-08-10 2002-09-11\n49  2011-03-04       &lt;NA&gt;        NaN 2014-10-08       &lt;NA&gt;\n50  2006-06-05 2006-10-01        NaN 2014-09-01       &lt;NA&gt;\n51  2001-11-28 2002-03-08        NaN 2002-02-22 2002-03-15\n52        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-24       &lt;NA&gt;\n53        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-07       &lt;NA&gt;\n54  2012-04-22       &lt;NA&gt; 2008-05-08 2012-04-21 2012-05-20\n55  1996-01-27 2000-04-15        NaN 2000-03-23 2000-04-22\n56        &lt;NA&gt;       &lt;NA&gt;        NaN 2005-05-22 2005-06-17\n57        &lt;NA&gt;       &lt;NA&gt;        NaN 2004-09-20 2004-10-26\n58  1997-12-26       &lt;NA&gt;        NaN 1997-12-09 1998-01-02\n59        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-01       &lt;NA&gt;\n60  2003-01-04 2004-04-09        NaN 2009-12-23 2010-01-18\n61  1999-02-21 2000-10-11        NaN 2000-09-28 2000-10-18\n62        &lt;NA&gt;       &lt;NA&gt;        NaN 2006-10-14 2006-10-29\n63        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n64  1996-02-05       &lt;NA&gt;        NaN 1996-01-17 1996-02-12\n65  2000-12-16       &lt;NA&gt;        NaN 2001-01-01 2000-12-23\n66        &lt;NA&gt;       &lt;NA&gt;        NaN 2000-09-17 2000-10-16\n67        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-25       &lt;NA&gt;\n68  2009-03-07       &lt;NA&gt;        NaN 2009-03-16 2009-03-29\n69  2009-05-01 2010-02-11        NaN 2014-12-15       &lt;NA&gt;\n70        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-06       &lt;NA&gt;\n71  1998-09-13       &lt;NA&gt;        NaN 1998-09-11 1998-09-20\n72  1998-05-27 1998-12-12        NaN 2001-08-05 2001-07-20\n73        &lt;NA&gt;       &lt;NA&gt;        NaN 2008-11-12 2008-12-19\n74        &lt;NA&gt;       &lt;NA&gt;        NaN 1996-09-09 1996-09-12\n75        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-09       &lt;NA&gt;\n76        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n77  1995-09-16 1995-12-16        NaN 1999-07-30 1999-08-07\n78        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n79        &lt;NA&gt;       &lt;NA&gt;        NaN 1999-12-16 1999-12-25\n80        &lt;NA&gt;       &lt;NA&gt;        NaN 1999-03-13 1999-03-23\n81  1994-12-12       &lt;NA&gt;        NaN 1994-11-15 1994-12-19\n82  2009-06-12       &lt;NA&gt;        NaN 2009-05-16 2009-06-19\n83  2009-11-28 2010-02-12 2009-12-19 2010-01-26 2010-02-27\n84        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-03       &lt;NA&gt;\n85  2009-05-10 2009-05-20 2008-06-27 2009-06-11 2009-06-08\n86  2005-05-11 2009-08-04        NaN 2014-06-24       &lt;NA&gt;\n87  2009-08-12       &lt;NA&gt;        NaN 2014-10-21       &lt;NA&gt;\n88  2004-04-18       &lt;NA&gt;        NaN 2004-03-25 2004-04-26\n89  2005-01-23       &lt;NA&gt;        NaN 2004-12-30 2005-01-30\n90        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-06-23       &lt;NA&gt;\n91        &lt;NA&gt;       &lt;NA&gt;        NaN 1998-08-15 1998-08-29\n92  2007-10-11       &lt;NA&gt;        NaN 2007-10-05 2007-10-18\n93        &lt;NA&gt;       &lt;NA&gt;        NaN 2004-04-13 2004-05-15\n94  2001-06-19       &lt;NA&gt;        NaN 2001-07-02 2001-06-26\n95        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-01       &lt;NA&gt;\n96  2000-11-10 2012-03-24        NaN 2012-03-23 2012-03-31\n97        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-04       &lt;NA&gt;\n98        &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-19       &lt;NA&gt;\n99        &lt;NA&gt;       &lt;NA&gt;        NaN 1995-03-30 1995-05-02\n100       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-01       &lt;NA&gt;\n101       &lt;NA&gt;       &lt;NA&gt; 2013-03-17 2014-10-29       &lt;NA&gt;\n102       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-28       &lt;NA&gt;\n103       &lt;NA&gt;       &lt;NA&gt;        NaN 2001-06-14 2001-07-07\n104       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-26       &lt;NA&gt;\n105       &lt;NA&gt;       &lt;NA&gt;        NaN 2009-11-21 2009-12-16\n106       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-06-19       &lt;NA&gt;\n107       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n108       &lt;NA&gt;       &lt;NA&gt;        NaN 1999-03-10 1999-03-21\n109 1999-06-09 2002-12-17 2002-04-26 2002-12-19 2003-01-06\n110       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-11       &lt;NA&gt;\n111 1999-05-07 2000-10-30        NaN 2003-03-22 2003-03-25\n112       &lt;NA&gt;       &lt;NA&gt; 2003-02-25 2006-04-06 2006-04-25\n113       &lt;NA&gt;       &lt;NA&gt;        NaN 1998-03-26 1998-03-21\n114 2005-01-21 2005-02-07        NaN 2005-03-06 2005-03-17\n115 1998-03-02 2006-12-11        NaN 2006-11-23       &lt;NA&gt;\n116 2014-03-26       &lt;NA&gt;        NaN 2014-09-02       &lt;NA&gt;\n117       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-06       &lt;NA&gt;\n118 2003-08-07 2004-05-31 1999-12-26 2005-05-19 2005-05-18\n119 2003-07-21       &lt;NA&gt;        NaN 2003-07-11 2003-07-28\n120 2000-05-26 2007-01-12        NaN 2009-05-08 2009-06-04\n121 2008-12-04 2009-06-16        NaN 2009-05-17 2009-06-23\n122 2010-03-25 2011-01-28        NaN 2014-12-30       &lt;NA&gt;\n123 1996-05-18       &lt;NA&gt;        NaN 1996-05-16 1996-05-25\n124 2002-08-31       &lt;NA&gt;        NaN 2002-08-19 2002-09-07\n125 2000-08-05 2005-07-19 2009-03-07 2009-10-08 2009-10-26\n126       &lt;NA&gt;       &lt;NA&gt;        NaN 1998-08-19 1998-09-21\n127       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-06-25       &lt;NA&gt;\n128 1998-04-20       &lt;NA&gt;        NaN 1998-03-26 1998-04-27\n129 2001-12-24 2005-01-29        NaN 2014-09-01       &lt;NA&gt;\n130 2011-08-23 2012-01-12        NaN 2012-01-31 2012-02-19\n131       &lt;NA&gt;       &lt;NA&gt;        NaN 2002-06-28 2002-06-14\n132 2011-11-10       &lt;NA&gt;        NaN 2011-11-30 2011-12-25\n133       &lt;NA&gt;       &lt;NA&gt;        NaN 1999-03-29 1999-05-02\n134 1996-05-22 1997-09-20 2000-04-27 2000-05-03 2000-05-21\n135       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n136       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-12-30       &lt;NA&gt;\n137       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-27       &lt;NA&gt;\n138       &lt;NA&gt;       &lt;NA&gt;        NaN 2004-02-15 2004-03-06\n139 2012-09-26       &lt;NA&gt;        NaN 2014-10-02       &lt;NA&gt;\n140 2004-06-30 2004-08-21        NaN 2004-09-13 2004-10-01\n141       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-09       &lt;NA&gt;\n142       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-06-16       &lt;NA&gt;\n143 2011-01-25       &lt;NA&gt;        NaN 2011-01-29 2011-02-01\n144       &lt;NA&gt;       &lt;NA&gt;        NaN 2009-12-20 2009-12-31\n145       &lt;NA&gt;       &lt;NA&gt;        NaN 2010-06-20 2010-07-07\n146 1998-08-30       &lt;NA&gt;        NaN 1998-09-13 1998-09-06\n147       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-11-04       &lt;NA&gt;\n148 2004-06-23       &lt;NA&gt;        NaN 2004-06-09 2004-06-30\n149       &lt;NA&gt;       &lt;NA&gt;        NaN 2002-03-09 2002-04-13\n150 2008-06-05 2008-07-05        NaN 2010-05-18 2010-05-04\n151       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-22       &lt;NA&gt;\n152 2013-11-05       &lt;NA&gt;        NaN 2014-10-28       &lt;NA&gt;\n153       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-07-30       &lt;NA&gt;\n154       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-14       &lt;NA&gt;\n155       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-28       &lt;NA&gt;\n156       &lt;NA&gt;       &lt;NA&gt; 2011-06-21 2012-01-20 2012-01-10\n157       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-09       &lt;NA&gt;\n158       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-09-29       &lt;NA&gt;\n159 2013-02-16       &lt;NA&gt;        NaN 2014-08-11       &lt;NA&gt;\n160       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-10-20       &lt;NA&gt;\n\n[[2]]\n     id        doV   a1c chol crea\n1     1 1993-05-07  87.3  3.9   83\n2     2 1993-05-05  66.5  6.6   83\n3     3 1993-05-11  73.0  5.6   68\n4     4 1993-05-12  61.2  5.2   97\n5     5 1993-02-25 102.7  6.0  149\n6     6 1993-03-18  93.4  4.8   55\n7     7 1993-03-16  94.5  8.6   56\n8     8 1993-03-12 103.9  5.1   78\n9     9 1993-03-16  46.4  4.2  123\n10   10 1993-01-21  84.4  5.4   79\n11   11 1993-01-28  47.9  8.9   74\n12   12 1993-02-19  86.4  5.3   84\n13   13 1993-03-23  80.9  6.0   65\n14   14 1993-02-03  88.3  5.7   52\n15   15 1993-01-19  83.8  6.3   61\n16   16 1993-01-20  47.9  6.8   61\n17   17 1993-05-19  78.0  5.7   52\n18   18 1993-05-20 106.2  4.1   62\n19   19 1993-05-11  53.1  9.5   98\n20   20 1993-05-13  65.4  5.7   46\n21   21 1993-05-23  73.0  4.9   82\n22   22 1993-05-21  72.1  5.3   74\n23   23 1993-01-25  88.8  6.6   79\n24   24 1993-03-12  69.4  6.9   52\n25   25 1993-03-04  65.6  7.4   88\n26   26 1993-03-20  66.5  5.4   44\n27   27 1993-03-24  76.2  4.0   71\n28   28 1993-02-25  38.3  5.3   61\n29   29 1993-04-16 113.7  4.7   54\n30   30 1993-05-27  66.7  5.9  149\n31   31 1993-03-28  88.1  7.8   99\n32   32 1993-03-17 111.0  7.1   90\n33   33 1993-03-10  35.8  3.3   91\n34   34 1993-02-04  56.5  5.2   86\n35   35 1993-06-28  56.3  6.8   88\n36   36 1993-03-19  65.6  4.8   48\n37   37 1993-01-29  51.5  5.8   71\n38   38 1993-02-13  63.9  5.4   94\n39   39 1993-02-04  56.2  5.9   67\n40   40 1993-03-28  86.6  8.5   41\n41   41 1993-01-22  51.0  5.1  137\n42   42 1993-04-04  48.1  5.6   96\n43   43 1993-02-11  70.6  6.2   73\n44   44 1993-02-08  94.8  5.7   71\n45   45 1993-02-23  52.8  4.9   55\n46   47 1993-05-20  60.5  5.9   61\n47   48 1993-06-02  44.6  4.0   47\n48   50 1993-05-31  55.6  5.0  110\n49   51 1993-06-01  79.3  6.5   63\n50   52 1993-02-24 109.6  6.9   53\n51   53 1993-03-04  40.4  7.1   62\n52   54 1993-02-26  51.4  5.6   44\n53   55 1993-04-28  85.8  5.6   60\n54   56 1993-03-15  98.9  5.3  106\n55   57 1993-01-24  76.6  7.0   28\n56   58 1993-02-12  60.0  5.3  144\n57   59 1993-06-04  74.8  5.5   62\n58   60 1993-06-11  73.2  5.6  120\n59   61 1993-06-10  43.1  4.1   72\n60   62 1993-06-03 104.1  7.9   49\n61   63 1993-04-02 107.9  6.2   85\n62   64 1993-02-18 108.3  4.7  109\n63   65 1993-04-19  92.4  4.7   71\n64   66 1993-02-26  55.4  5.5  125\n65   67 1993-04-14  67.8  6.0   84\n66   68 1993-04-14  38.2  8.4  108\n67   69 1993-03-01  48.6  3.2  112\n68   70 1993-02-16  74.3  7.7  123\n69   71 1993-04-18  97.5  6.7  121\n70   72 1993-04-15  97.3  5.9  121\n71   73 1993-02-09  89.6  5.7   65\n72   74 1993-04-21  63.6  6.3   41\n73   75 1993-02-28  71.6  6.7   85\n74   76 1993-03-01 100.9  5.9   62\n75   77 1993-04-23  52.9  6.8   99\n76   78 1993-05-02  47.7  5.5   82\n77   79 1993-05-01  82.9  6.5  108\n78   80 1993-03-19  57.3  6.6   55\n79   81 1993-03-24  99.8  6.2   79\n80   82 1993-01-25  54.1  5.3  127\n81   83 1993-01-13  53.8  4.9   77\n82   84 1993-03-03  63.7  5.7   60\n83   85 1993-04-26  56.5  5.7   45\n84   86 1993-05-03  67.7  5.0   86\n85   87 1993-04-19  54.2  4.6   90\n86   88 1993-03-15  78.7  8.3  101\n87   89 1993-06-20  61.2  4.0   82\n88   90 1993-08-02  59.4  3.6   79\n89   91 1993-07-25  69.5  4.5   48\n90   92 1993-08-29  53.2  5.7   94\n91   93 1993-08-05  73.3  5.4  124\n92   94 1993-08-02  70.1  3.6  107\n93   95 1993-08-27  56.7  5.2  109\n94   96 1993-07-31  60.6  6.0   92\n95   97 1993-08-01  45.5  4.6   80\n96   98 1993-07-31  80.7  4.3   72\n97  100 1993-08-10  88.2  7.7   90\n98  101 1994-01-19  72.8  4.0   61\n99  102 1993-08-11  60.4  4.2   60\n100 103 1993-08-21  83.3  5.8  107\n101 104 1993-08-22  69.5  4.7   80\n102 105 1994-02-04  59.6  4.9   57\n103 107 1993-08-27  63.7  7.0   54\n104 108 1993-08-24  38.6  4.5  103\n105 110 1993-08-24  42.4  4.8   52\n106 111 1993-09-03  41.0  6.3   80\n107 112 1993-10-09  74.3  6.8   69\n108 113 1993-10-09  50.9  5.3   89\n109 114 1993-10-08  83.6  5.8  127\n110 115 1993-10-01  47.7  6.0   63\n111 116 1993-10-08  88.1  5.2   62\n112 118 1993-10-19  63.5  6.3   73\n113 119 1993-10-10  52.9  7.4   78\n114 120 1993-10-09  77.5  3.1   76\n115 121 1993-10-18  48.9  4.6   83\n116 123 1993-11-06  67.5  5.1   58\n117 126 1993-10-27  70.4  5.7   87\n118 128 1993-11-08  48.7  5.0  100\n119 129 1993-11-01  91.5  4.4   93\n120 130 1993-11-02  89.2  6.2   86\n121 131 1993-11-02  48.4  4.1   98\n122 133 1994-03-22  72.3  6.8   51\n123 134 1993-11-09  82.6  5.5   87\n124 135 1993-11-15  75.0  6.3   56\n125 136 1993-11-20 100.1  5.7   94\n126 137 1993-11-25  53.7  5.2   75\n127 138 1993-11-28  52.0  4.7   47\n128 139 1993-11-30  69.6  7.8   76\n129 140 1993-12-10  76.9  5.7   58\n130 142 1993-11-27  81.7  4.6   73\n131 143 1993-12-10  69.3  4.6   93\n132 144 1993-12-05  74.2  6.6   91\n133 145 1994-01-14  65.1  4.5   57\n134 146 1994-01-25  72.1  4.3   58\n135 147 1994-01-25  48.6  4.6  126\n136 148 1993-12-31  82.2  8.4   67\n137 150 1994-01-13  57.6  6.4   47\n138 151 1994-01-22  50.1  3.6   61\n139 152 1994-01-25  89.5  6.8   95\n140 153 1994-01-23  69.8  4.3   48\n141 155 1994-01-21  46.5  3.5  126\n142 156 1994-01-19  56.4  4.1   59\n143 157 1994-03-15  81.2  6.4   66\n144 158 1994-02-02  47.7  4.6   87\n145 159 1994-01-22  64.0  4.4   62\n146 160 1994-01-26  90.9  5.9  126\n147 161 1994-01-24  65.6  4.9   83\n148 163 1994-02-07  61.0  6.2  141\n149 165 1994-02-21  66.5  6.7   66\n150 166 1994-02-20  62.2  6.1   80\n151 167 1994-02-17  73.6  6.0   96\n152 168 1994-02-25  86.2  6.5  101\n153 169 1994-05-02  51.7  5.3   62\n154 170 1994-05-04  77.1  5.0   54\n155 171 1994-04-30  61.2  5.7  108\n156 172 1994-05-11  49.7  5.0   48\n157 173 1994-05-14  70.5  4.1   59\n158 174 1994-05-23 103.0  4.6  110\n159 175 1994-05-30  51.3  4.3  106\n160 176 1994-05-20  86.7  5.7   62\n161   1 1995-06-25  68.9  4.2   73\n162   2 1995-06-28  54.8  5.7  105\n163   3 1995-07-01  79.6  4.1   78\n164   4 1995-06-27 102.3  4.8  106\n165   5 1995-07-07  54.7  8.8  140\n166   6 1995-07-02 100.1  8.8  184\n167   7 1995-07-04 103.4  6.0   60\n168   8 1995-07-06  52.8  4.0  197\n169   9 1995-06-24  73.7  3.3  216\n170  10 1995-07-07  60.5  5.5  176\n171  11 1995-06-28  63.2  4.9   91\n172  12 1995-06-29 109.8  4.6   63\n173  13 1995-07-05  55.1  4.9   67\n174  14 1995-07-07  79.5  4.2   51\n175  15 1995-07-04  59.5  4.8  123\n176  16 1995-07-06  87.3  4.5  155\n177  17 1995-07-06  66.1  3.6   49\n178  18 1995-06-24  90.2  4.7   59\n179  19 1995-07-02  74.4  8.7   72\n180  20 1995-07-04  84.2  6.0  174\n181  21 1995-06-25  94.7  4.9   75\n182  22 1995-07-05  57.7  4.9   60\n183  23 1995-06-24 110.4  5.9  167\n184  24 1995-07-03  58.5  4.8   85\n185  25 1995-06-25  80.4  7.4   75\n186  26 1995-07-02  89.5  5.8  100\n187  27 1995-07-05  58.0  3.8   61\n188  28 1995-06-24  37.9  7.0   75\n189  29 1995-06-27  97.4  3.2   82\n190  30 1995-07-04  62.2  5.7   81\n191  31 1995-06-26  91.8  6.1  149\n192  32 1995-07-01 104.1  5.6   87\n193  33 1995-06-26  45.9  4.0   83\n194  34 1995-07-04  48.9  4.3   65\n195  35 1995-07-02 109.4  7.5  109\n196  36 1995-07-01  69.2  4.7   70\n197  37 1995-06-26  47.6  3.8   61\n198  38 1995-07-03  82.0  6.7  178\n199  39 1995-07-06  71.1  4.5  123\n200  40 1995-06-26  68.0  6.6   94\n201  41 1995-07-03  43.5  4.5   97\n202  42 1995-07-03  78.1  5.5  113\n203  43 1995-06-28  80.0  7.1  138\n204  44 1995-06-30 104.5  6.3   99\n205  45 1995-06-29  61.1  4.5   63\n206  47 1995-06-29  89.9  5.4   55\n207  48 1995-06-29  57.9  3.2   79\n208  50 1995-06-26  92.8  4.7   94\n209  51 1995-06-27  66.7  5.5   81\n210  52 1995-06-25  88.3  4.7  102\n211  53 1995-06-26  59.0  6.9   48\n212  54 1995-07-05  40.9  4.8   44\n213  55 1995-06-25  57.0  4.7   61\n214  56 1995-07-04  54.5  5.5  142\n215  57 1995-07-02  69.0  5.6   35\n216  58 1995-06-29  77.0  4.8  144\n217  59 1995-06-26  97.0  4.5   43\n218  60 1995-07-03  78.9  4.9   76\n219  61 1995-06-30  48.8  3.8  105\n220  62 1995-06-28  84.5  7.2   72\n221  63 1995-06-29  82.8  5.4   56\n222  64 1995-07-06  96.5  4.1  131\n223  65 1995-07-02  81.2  4.7   54\n224  66 1995-06-27  63.8  5.6  146\n225  67 1995-06-25  61.5  5.1   78\n226  68 1995-07-01  91.3  5.7   62\n227  69 1995-06-28  75.8  4.0   93\n228  70 1995-07-02  73.2  4.7  148\n229  71 1995-06-26  71.3  6.1   75\n230  72 1995-06-27  85.2  4.9  102\n231  73 1995-06-27  71.2  4.8  101\n232  74 1995-06-27  65.1  3.5   54\n233  75 1995-07-04  81.4  5.3  109\n234  76 1995-06-27  46.5  4.9  100\n235  77 1995-07-05  50.9  5.3   85\n236  78 1995-06-30  68.4  4.6  167\n237  79 1995-06-24  68.2  5.5  138\n238  80 1995-06-28  73.1  5.5   63\n239  81 1995-06-29  78.5  6.1   86\n240  82 1995-06-25  70.2  5.5   86\n241  84 1995-07-03  81.5  5.1   68\n242  85 1995-06-26  84.8  4.6   88\n243  86 1995-07-01  61.4  7.1   65\n244  87 1995-06-25  85.9  4.1  117\n245  88 1995-07-05  74.8  3.9   99\n246  89 1995-06-26  50.3  3.4  220\n247  90 1995-07-03  90.1  4.5   52\n248  91 1995-07-03  74.1  4.6   87\n249  92 1995-06-25  57.5  4.9   81\n250  93 1995-07-01  59.3  4.6   53\n251  94 1995-07-04  78.7  3.9   72\n252  95 1995-07-07  43.8  4.7  122\n253  96 1995-07-07  65.5  4.4  103\n254  97 1995-07-07  43.2  4.5  110\n255  98 1995-07-01  67.5  4.2  116\n256 100 1995-06-29  77.7  6.1  111\n257 101 1995-06-24  62.3  4.3   61\n258 103 1995-07-02  80.4  5.1   54\n259 104 1995-06-24  66.3  5.6   68\n260 105 1995-07-04  81.1  4.8  126\n261 107 1995-07-01  60.2  5.8   53\n262 108 1995-06-28  65.9  5.8   88\n263 110 1995-06-25  36.3  4.7  111\n264 111 1995-06-25  60.2  5.3  131\n265 112 1995-06-26  95.0  5.7   57\n266 113 1995-06-24  63.0  4.9   64\n267 114 1995-07-06  97.8  5.9  121\n268 115 1995-07-01  46.7  4.5   68\n269 116 1995-06-24 109.3  5.2   88\n270 118 1995-07-02  73.5  6.1   92\n271 119 1995-07-07  52.6  4.0   67\n272 120 1995-07-02  78.9  3.6   54\n273 121 1995-07-03  61.0  5.1  118\n274 123 1995-07-03  63.1  5.0  105\n275 126 1995-06-29  74.1  5.8  114\n276 129 1995-07-01  66.2  4.4   94\n277 130 1995-06-29  87.0  5.9  136\n278 131 1995-07-06  44.2  4.5   69\n279 133 1995-07-03  44.4  6.1   83\n280 134 1995-06-26  84.7  5.7   71\n281 135 1995-07-07  64.7  4.3   55\n282 136 1995-06-28 119.7  5.9  106\n283 137 1995-06-25  52.6  4.5  106\n284 138 1995-07-04  56.8  3.9   75\n285 139 1995-07-01  61.9  5.8  125\n286 140 1995-07-07  89.6  5.7   47\n287 142 1995-07-01 121.1  4.5   62\n288 143 1995-07-02  55.2  4.4  124\n289 144 1995-06-30  92.6  7.0  138\n290 145 1995-07-04  96.1  5.3  121\n291 146 1995-07-03  72.3  4.4  115\n292 147 1995-06-27  52.3  4.4  142\n293 148 1995-07-07 101.7  6.5   45\n294 151 1995-07-05  63.3  3.3  114\n295 152 1995-06-30  73.1  6.2  101\n296 153 1995-06-27  52.7  3.7  100\n297 155 1995-06-28  46.3  3.8   96\n298 156 1995-07-06  49.2  4.2  116\n299 157 1995-07-05  80.6  5.5   66\n300 158 1995-06-25  47.6  4.9  145\n301 159 1995-07-05  50.5  5.0   67\n302 160 1995-06-28  99.3  6.7   96\n303 161 1995-06-30  57.2  4.5  110\n304 163 1995-06-27  55.7  6.2   94\n305 165 1995-07-05  58.3  6.1   63\n306 166 1995-06-29  62.3  5.2  102\n307 167 1995-07-01  67.5  5.1   50\n308 168 1995-06-27  76.1  5.3   73\n309 169 1995-07-05  44.4  5.5   85\n310 170 1995-06-25  57.0  5.6   90\n311 171 1995-06-24  49.5  5.0  103\n312 172 1995-07-07  76.4 14.0  126\n313 173 1995-07-03  57.7  3.9   59\n314 174 1995-07-05  58.1  5.0   67\n315 175 1995-06-25  60.0  3.9   86\n316 176 1995-06-25  77.9  4.9   52\n317   1 1997-06-24  63.7  4.4   63\n318   2 1997-07-05  49.5  6.8   50\n319   3 1997-07-07  73.0  4.7   83\n320   4 1997-07-06  73.6  5.7  118\n321   5 1997-07-01  41.9  5.8  141\n322   6 1997-07-01 108.6  8.7   67\n323   7 1997-06-30  88.4  5.8   53\n324   8 1997-06-26  52.9  3.5  287\n325  10 1997-06-24  75.8  5.8   73\n326  12 1997-06-27 129.8  4.7   79\n327  13 1997-06-25  65.2  4.7   83\n328  14 1997-07-05  59.1  3.5   77\n329  15 1997-07-03  49.9  3.5   74\n330  16 1997-06-30  61.8  6.5   96\n331  17 1997-07-02  61.9  3.8   60\n332  18 1997-07-03  72.8  6.9  100\n333  19 1997-06-27  36.1  7.0   82\n334  20 1997-07-04  93.4  6.3   95\n335  21 1997-07-03  52.8  5.0   74\n336  22 1997-06-25  49.5  4.5   48\n337  23 1997-06-29  42.3  4.7  158\n338  24 1997-06-24  69.1  5.0  107\n339  25 1997-07-07  81.4  6.8   85\n340  26 1997-06-29  86.5  6.0   64\n341  27 1997-06-28  58.0  4.4   75\n342  28 1997-06-30  60.3  4.8   85\n343  29 1997-07-03 110.4  4.5  112\n344  30 1997-06-24  63.9  4.4  150\n345  31 1997-06-30  83.5  6.4   69\n346  32 1997-07-02  83.7  5.1   57\n347  33 1997-07-07  50.5  3.8   92\n348  34 1997-07-04  51.4  5.0  108\n349  35 1997-07-06  76.6  7.3  148\n350  36 1997-06-30  74.0  5.4   77\n351  37 1997-06-27  61.0  4.6   70\n352  38 1997-07-02  83.5  7.9  129\n353  39 1997-07-05  62.1  5.1  112\n354  40 1997-06-26  79.4  6.0   68\n355  41 1997-06-28  37.5  4.3  151\n356  42 1997-06-27  83.9  5.6  157\n357  43 1997-07-06  96.5  7.1  166\n358  44 1997-07-04  76.9  6.6  119\n359  45 1997-07-06  88.8  4.8   56\n360  47 1997-07-06 105.5  7.8   79\n361  48 1997-06-29  71.8  2.8   71\n362  50 1997-07-02  74.6  5.2  124\n363  51 1997-07-02  55.0  4.5   86\n364  52 1997-06-27  70.1  4.6  100\n365  53 1997-06-25  66.4  6.1   74\n366  54 1997-06-28  44.1  5.6   51\n367  55 1997-07-01  49.2  5.2   51\n368  56 1997-06-30  42.2  4.1   86\n369  57 1997-07-05 101.1  6.7   54\n370  58 1997-06-27  72.1  5.0   97\n371  59 1997-07-03  92.5  4.8   80\n372  60 1997-06-25  69.0  5.4  107\n373  61 1997-06-28  63.7  3.9   88\n374  62 1997-06-26 106.2  9.1   72\n375  63 1997-06-24  68.6  5.7   61\n376  64 1997-07-01  90.4  4.1   77\n377  65 1997-06-25  78.2  4.1   85\n378  67 1997-07-02  52.5  5.2  112\n379  68 1997-07-05  56.9  4.8   71\n380  69 1997-07-03  53.8  4.1  120\n381  70 1997-06-27  80.1  5.8  123\n382  71 1997-07-05  78.6  6.6  110\n383  72 1997-07-07  79.1  6.0   58\n384  73 1997-07-01  47.3  5.6   63\n385  74 1997-07-02  50.2  5.0  102\n386  75 1997-06-29  51.1  4.9   75\n387  77 1997-06-24  56.2  5.2  112\n388  78 1997-06-29  58.9  4.2  147\n389  79 1997-06-25  73.0  4.9  137\n390  80 1997-07-03  72.2  5.0   93\n391  81 1997-06-24  70.0  5.1   82\n392  82 1997-06-27  72.2  5.7  132\n393  85 1997-07-06  87.1  3.4   91\n394  86 1997-07-05  54.9  4.9   61\n395  87 1997-07-02  74.5  4.1   65\n396  88 1997-06-30  79.8  5.2  104\n397  89 1997-07-03  48.5  4.0  191\n398  90 1997-06-25  59.6  4.4   67\n399  91 1997-07-02  61.4  3.7   79\n400  92 1997-07-01  57.9  5.0   94\n401  93 1997-06-25  82.6  5.3   50\n402  94 1997-07-07  61.6  4.0   77\n403  95 1997-07-03  68.0  4.6  147\n404  96 1997-07-06  60.5  5.6   65\n405  97 1997-07-03  56.0  4.8   49\n406  98 1997-07-05  49.5  4.4  155\n407 100 1997-06-25  81.9  7.8   77\n408 101 1997-06-26  50.6  4.6   75\n409 103 1997-07-06  59.7  4.5  106\n410 104 1997-06-30  60.5  3.8   64\n411 105 1997-06-30  51.4  4.6   92\n412 107 1997-06-26  52.0  5.6  111\n413 108 1997-06-29  46.9  4.0   81\n414 110 1997-06-30  57.1  4.7   75\n415 111 1997-07-02  48.6  5.7   74\n416 112 1997-07-06  73.7  5.6   35\n417 113 1997-06-28  78.5  5.7  101\n418 114 1997-06-25  70.4  5.5  114\n419 115 1997-07-02  47.8  5.2  100\n420 116 1997-07-03 123.1  5.7   60\n421 118 1997-07-01  48.9  6.3   69\n422 119 1997-07-07  34.7  3.4   65\n423 120 1997-06-30  65.0  3.4   90\n424 121 1997-07-04  61.8  5.6  128\n425 123 1997-07-02  79.0  5.1   89\n426 126 1997-06-25  68.7  4.6   81\n427 129 1997-06-29  70.5  4.3  116\n428 130 1997-06-24  70.8  5.6  119\n429 131 1997-07-07  47.3  5.2   90\n430 133 1997-07-02  62.9  5.9   96\n431 135 1997-06-27  98.8  5.2   56\n432 136 1997-07-03 113.7  5.9   69\n433 137 1997-06-24  34.3  4.9  120\n434 138 1997-07-04  44.2  3.9   96\n435 139 1997-06-25  65.6  5.7  133\n436 140 1997-06-26  81.2  6.2   93\n437 142 1997-06-30  68.9  4.3  103\n438 143 1997-07-05  73.6  5.4  236\n439 144 1997-06-27  77.0  6.2  138\n440 145 1997-07-04  60.9  4.5   60\n441 146 1997-06-26  72.3  5.2   66\n442 148 1997-07-04 119.0  7.0   51\n443 151 1997-07-01  85.4  4.4  104\n444 152 1997-06-26  62.5  5.0   55\n445 153 1997-06-30  45.6  3.8  106\n446 155 1997-07-06  50.8  3.8   84\n447 156 1997-06-29  59.4  4.3   75\n448 157 1997-07-03  70.1  5.5   84\n449 158 1997-06-27  54.4  4.4  109\n450 159 1997-06-26  43.9  3.9  111\n451 160 1997-06-24  93.7  5.9   99\n452 161 1997-06-25  52.9  4.5   91\n453 163 1997-06-29  42.8  5.1  117\n454 165 1997-07-03  57.7  5.6   78\n455 166 1997-06-27  85.6  5.6   87\n456 167 1997-06-29  81.8  5.1   68\n457 168 1997-07-04  78.9  6.7   92\n458 169 1997-06-30  52.3  4.7  126\n459 170 1997-07-05  59.8  4.6   61\n460 171 1997-07-02  82.0  5.5   59\n461 172 1997-06-28  92.3  5.6   85\n462 173 1997-07-07  69.8  4.7   68\n463 174 1997-06-26  52.6  4.8  118\n464 175 1997-07-07  78.3  4.8   75\n465 176 1997-06-24  91.3  6.3   86\n466   1 2001-01-31  50.6  3.0  105\n467   2 2001-10-29  69.4  4.2   63\n468   3 2001-02-01  70.5  5.8  158\n469   4 2001-01-28  74.4  4.2  141\n470   6 2001-01-10  91.5  7.4   59\n471   7 2001-01-17  57.6  5.7   70\n472   8 2001-08-27  75.2  3.3  130\n473  10 2001-01-14  63.4  4.7  124\n474  11 2001-11-08  89.0  9.1  138\n475  12 2001-04-07  76.3  5.7  196\n476  13 2001-02-05  69.8  3.8   59\n477  14 2001-01-09  58.3  4.3  150\n478  15 2001-08-11  80.2  4.1  189\n479  16 2001-01-09  93.1  5.6  167\n480  17 2001-01-17  68.7  3.6   45\n481  18 2001-02-03  80.0  4.6  107\n482  19 2001-03-30  59.3  7.7  110\n483  20 2001-02-08  90.4  6.7   65\n484  21 2001-10-22  66.5  5.9  124\n485  22 2001-03-26  53.2  2.7   99\n486  24 2001-10-27  62.8  4.0   84\n487  25 2001-01-24 114.1  7.9  128\n488  26 2001-04-18  75.5  5.4  113\n489  27 2001-04-05  41.8  6.8  132\n490  28 2001-11-04  62.7  5.1  122\n491  29 2001-09-04  76.6  5.0   83\n492  30 2001-08-12  54.6  4.3  106\n493  31 2001-03-29  73.5  5.2  112\n494  33 2001-08-20  42.9  3.4  105\n495  34 2001-01-16  37.7  3.6  154\n496  35 2001-04-02 110.7  7.6  114\n497  36 2001-08-27  64.0  6.3   61\n498  37 2001-01-09  62.0  4.1   91\n499  38 2001-01-08  85.1  3.9  161\n500  39 2001-03-16  62.2  4.4  144\n501  41 2001-04-25  44.7  3.8  156\n502  42 2001-03-17  91.7  4.3  109\n503  43 2001-03-25  76.8  5.6  243\n504  45 2001-03-28  57.8  5.3   96\n505  47 2001-01-11 147.6  7.9   95\n506  50 2001-03-29  80.4  5.2   97\n507  51 2001-04-07  62.5  4.3   73\n508  52 2001-04-18  84.9  5.0  121\n509  53 2001-01-18  69.0  5.6   98\n510  54 2001-04-06  51.5  3.9   70\n511  55 2001-03-31  52.1  4.6   58\n512  56 2001-01-13  36.6  3.1  167\n513  58 2001-01-19  79.2  5.7   76\n514  59 2001-10-26  88.6  3.6  114\n515  61 2001-04-07  51.3  3.9   88\n516  62 2001-04-20  93.7  6.3   82\n517  64 2001-03-31  80.9  4.4   85\n518  65 2001-02-11  76.0  3.1   91\n519  69 2001-01-18  75.9  3.5  117\n520  70 2001-01-07  68.9  6.5   77\n521  71 2001-01-27  86.1  7.4  157\n522  72 2001-02-05  69.0  5.3  110\n523  74 2001-05-08  45.5  3.6  119\n524  75 2001-09-02  75.8  5.3  157\n525  77 2001-01-12  35.7  3.6  129\n526  78 2001-03-20  65.0  5.5  141\n527  80 2001-01-25  67.8  6.8   97\n528  85 2001-02-11  97.6  5.3   84\n529  86 2001-01-28  65.3  4.5   70\n530  87 2001-10-31  63.0  3.1   78\n531  88 2001-03-21  76.9  4.3  168\n532  89 2001-08-19  53.0  4.0  143\n533  90 2001-10-07  99.7  3.8   75\n534  91 2001-03-14  65.1  3.4  177\n535  92 2001-02-06  56.3  3.6   69\n536  94 2001-08-24  54.9  4.6   87\n537  95 2001-08-29  50.5  3.0  110\n538  96 2001-04-07  62.8  3.7  157\n539  97 2001-04-01  57.9  3.2   70\n540  98 2001-04-15  49.1  3.3   72\n541 100 2001-09-09  68.7  4.5  105\n542 101 2001-09-13  58.4  3.9  106\n543 103 2001-02-02  58.1  3.2  132\n544 104 2001-02-08  88.3  4.0   77\n545 105 2001-09-15  50.6  3.3   65\n546 107 2001-04-05  54.1  4.8   60\n547 108 2001-04-02  52.7  4.0   67\n548 110 2001-01-11  52.3  5.7   78\n549 111 2001-09-10  70.4  5.1  114\n550 112 2001-08-23  73.7  6.1   65\n551 114 2001-10-25  61.1  3.1  179\n552 115 2001-03-14  48.8  5.4   90\n553 116 2001-10-18  44.2  5.7  162\n554 118 2001-11-12  42.7  6.3  101\n555 120 2001-09-12  54.4  3.7   74\n556 121 2001-08-24  56.4  4.0  153\n557 123 2001-08-30  51.6  4.4  132\n558 126 2001-03-22  62.0  4.2  125\n559 128 2001-07-12    NA  4.5 1067\n560 129 2001-10-22  65.7  4.6  116\n561 130 2001-08-15  74.8  5.3   85\n562 131 2001-08-24  70.6  5.4   88\n563 133 2001-10-09  85.5  5.6  100\n564 135 2001-08-30  72.3  3.9   85\n565 136 2001-08-14  93.7  4.0  146\n566 138 2001-04-20  54.8  4.3   62\n567 140 2001-12-09  50.2  7.9   84\n568 142 2001-09-03  75.6  3.7   88\n569 143 2001-04-14  75.5  3.5  305\n570 144 2001-04-22  72.6  5.9  127\n571 147 2001-10-31  67.0  4.0  112\n572 148 2001-08-19  79.5  5.1   74\n573 151 2001-09-10  64.7  4.3   64\n574 152 2001-04-04  70.7  4.1   59\n575 153 2001-08-13  57.1  2.7   70\n576 155 2001-09-23  47.6  4.8  133\n577 156 2001-08-23  57.8  4.3   79\n578 158 2001-09-17  55.8  4.6  107\n579 159 2001-08-08  49.8  3.8  100\n580 161 2001-10-28  68.0  4.7   84\n581 163 2001-08-12  50.9  3.7   75\n582 165 2001-11-04 114.1  9.7   84\n583 166 2001-09-04  75.0  6.2   95\n584 167 2001-04-06  66.8  4.2  105\n585 168 2001-05-08  74.8  5.4  144\n586 169 2001-09-09  56.9  5.1  104\n587 170 2001-08-25  73.4  3.9   69\n588 171 2001-10-20  73.6  5.0  151\n589 172 2001-04-09  69.5  6.6   57\n590 173 2001-08-22  68.5  4.2   82\n591 174 2001-09-12  83.4  4.1   63\n592 175 2001-09-10  59.8  4.5   99\n593 176 2001-10-21  79.9  5.6   70\n594   1 2006-09-19  74.9  3.2  107\n595   2 2006-11-17  57.9  3.5   87\n596   6 2006-10-03 104.8  5.1  139\n597   8 2006-11-22  73.6  3.7  446\n598  10 2006-08-31  75.9  3.7  307\n599  13 2006-10-17  76.2  3.6   38\n600  14 2007-01-06  68.7  3.2   87\n601  15 2006-12-07  74.9  4.2  198\n602  16 2006-09-12  65.6  3.1  200\n603  17 2006-10-05  72.7  3.5  129\n604  18 2006-09-21  94.5  4.3   95\n605  19 2006-12-03  61.6  4.7   45\n606  20 2006-09-12  83.3  3.4  132\n607  21 2007-01-24  64.4  4.1  155\n608  22 2006-10-08  58.4  3.2   89\n609  24 2006-11-17  63.6  3.5   74\n610  25 2006-09-03  89.7  4.4   97\n611  27 2006-12-12  58.7  3.3  104\n612  28 2006-11-20  46.7  6.3   89\n613  31 2006-10-09  79.6  5.3   81\n614  33 2006-10-06  36.2  4.4   56\n615  34 2006-09-10  63.4  3.5  136\n616  36 2006-11-13  68.3  3.5   97\n617  37 2006-10-23  60.7  3.2   65\n618  39 2006-09-25  65.2  4.2   56\n619  44 2007-01-19  93.0  4.0  115\n620  45 2006-09-30  63.1  3.1   79\n621  48 2006-11-24  50.6  3.3  106\n622  51 2006-10-09  67.0  4.8  103\n623  52 2006-11-22  94.9  2.9   54\n624  54 2006-12-12  50.4  4.1   52\n625  55 2006-10-09  66.2  4.2   34\n626  56 2006-09-24  59.2  3.1  170\n627  61 2006-11-02  51.0  4.2  151\n628  62 2006-11-05  88.1  4.4  116\n629  65 2006-10-06  80.2  3.0   45\n630  69 2006-10-24  63.9  2.5   65\n631  70 2006-11-20  68.3  4.1  160\n632  71 2006-10-07  87.0  4.7  141\n633  75 2006-11-10  62.4  3.9  115\n634  77 2006-09-29  42.8  3.0  109\n635  78 2006-12-10  58.8  3.1  154\n636  80 2006-09-30  67.1  4.0  111\n637  84 2006-12-09  67.7  4.2  193\n638  85 2006-09-22  85.5  3.4  344\n639  86 2006-11-02  66.0  3.6   48\n640  87 2006-12-18  51.8  2.8  261\n641  88 2006-09-04  80.3  3.6  142\n642  89 2006-10-18  54.3  4.4  184\n643  92 2006-08-29  60.6  5.4   53\n644  94 2006-12-05  74.4  2.9   52\n645  97 2006-12-18  42.5  3.4  109\n646  98 2006-12-09  67.2  2.5   94\n647 100 2006-12-13  79.0  5.2  102\n648 101 2006-11-13  59.8  3.8  130\n649 103 2006-10-24  60.5  3.5  160\n650 104 2006-09-11  71.3  5.2  211\n651 105 2006-12-10  55.9  2.9   66\n652 108 2006-10-14  44.8  3.7   72\n653 110 2007-01-14  46.5  4.4   71\n654 111 2006-10-06  51.1  3.8   45\n655 112 2006-12-23  83.4  3.5   63\n656 115 2006-09-26  54.6  3.8   62\n657 121 2006-11-24  55.3  4.8   95\n658 123 2006-12-01  70.0  4.1   73\n659 126 2006-10-04  61.1  3.6   98\n660 130 2006-10-19  86.8  2.6  119\n661 131 2006-11-23  55.6  2.7  104\n662 133 2007-01-15  65.1  5.4   53\n663 136 2007-01-16 117.4  3.2  195\n664 138 2006-10-03  46.1  3.9   39\n665 140 2007-01-09  82.2  5.6   48\n666 142 2006-11-05  83.1  2.8  115\n667 144 2006-11-27  81.8  3.9  119\n668 148 2006-09-13 104.7  4.3   80\n669 152 2006-11-06  70.3  4.1   83\n670 155 2006-10-24  41.4  4.7   78\n671 156 2006-11-03  56.9  2.9   78\n672 157 2006-12-23  86.9  5.5  231\n673 158 2007-01-18  50.0  3.7  189\n674 159 2006-09-11  59.7  3.1   83\n675 161 2006-11-11  63.2  3.8   65\n676 166 2006-09-05  66.4  5.6  146\n677 167 2006-11-01  70.8  3.6   70\n678 168 2006-12-17  82.5  3.5  159\n679 169 2006-11-07  48.3  4.3   77\n680 170 2006-09-29  74.3  3.0   56\n681 171 2006-11-17  67.7  3.0   91\n682 172 2006-10-07  74.6  3.3  133\n683 173 2006-10-12  73.0  5.7   44\n684 174 2006-10-24  77.5  3.4  117\n685 175 2006-11-20  57.6  5.3  150\n686 176 2006-11-06  86.6  3.5   96\n687   1 2014-10-06  32.8  4.2  171\n688   2 2014-07-31  58.1  3.7   63\n689   6 2014-09-01  43.6  6.0  555\n690  13 2014-09-20  50.1  2.7   73\n691  15 2014-12-04  75.5  3.7  213\n692  18 2014-12-25  78.6  3.7  135\n693  19 2014-12-25  45.7  6.1   99\n694  20 2014-08-30  58.5  3.7   88\n695  21 2014-10-19  53.4  3.3  120\n696  22 2014-10-14  38.3  3.2   81\n697  25 2014-08-07  63.8  4.8  255\n698  28 2014-07-30  51.7  4.7  110\n699  33 2014-10-30  58.8  3.2   73\n700  34 2014-09-23  49.8  3.5  251\n701  36 2014-12-02  48.3  3.6   77\n702  39 2014-09-02  85.8  4.4  145\n703  44 2014-08-18    NA   NA  118\n704  45 2014-09-28  64.3  3.6   60\n705  51 2014-10-12  53.1  3.0  114\n706  52 2014-09-02  57.2  3.6   82\n707  54 2014-09-22  61.8  3.5   36\n708  55 2014-08-12  44.7  4.2   74\n709  61 2014-10-28  68.1  4.7  122\n710  65 2014-12-31    NA  5.1   NA\n711  69 2014-09-18  59.0  2.2  101\n712  71 2014-12-09  47.7  2.8  109\n713  72 2014-10-04  63.1  4.1   58\n714  77 2014-09-05  37.7  3.6  176\n715  78 2014-12-25  53.4  2.5  158\n716  80 2015-01-03  81.7  2.4   95\n717  86 2014-09-09  49.6  5.2  117\n718  88 2014-06-28  39.1  3.1  157\n719  89 2014-10-25  64.8  2.7  141\n720  92 2014-06-18  61.2  5.7   72\n721  97 2014-10-26  57.1  3.7   66\n722 100 2014-11-02  38.3  5.5   69\n723 101 2014-08-24  86.4  3.5  153\n724 103 2014-11-01  49.0  3.5  139\n725 104 2014-10-27  51.8  4.4  492\n726 105 2014-11-01  41.8  3.4   86\n727 108 2014-08-19  42.6  3.4   78\n728 111 2014-06-21  53.8  5.1   56\n729 112 2014-12-25  72.4  4.5   91\n730 115 2014-11-16  42.7  3.5   74\n731 123 2014-09-06  48.3  3.1   52\n732 126 2014-10-10  56.7   NA   NA\n733 138 2014-06-29  78.3  4.7  135\n734 140 2014-09-03  68.1  5.4  110\n735 147 2014-12-29  56.5  3.2  146\n736 148 2014-12-24  41.2  5.8   46\n737 150 2014-08-23    NA   NA   58\n738 152 2014-09-26 108.5  6.3   43\n739 155 2014-10-03  48.9  4.5  154\n740 156 2014-06-10  38.9  3.4  106\n741 161 2014-11-02  39.8  4.9   84\n742 167 2014-09-18  37.6  4.8   48\n743 168 2014-11-02  55.5  3.7  211\n744 169 2014-08-04  60.3  3.9   47\n745 170 2014-10-16  76.3  4.3   54\n746 171 2014-10-31  53.3  3.4  136\n747 173 2014-10-13  84.4  4.3  134\n748 174 2014-09-25  62.0  3.1   74\n749 175 2014-08-07  78.5  2.2  149\n750 176 2014-10-16  86.3  3.9  126\n\n[[3]]\n     id       doTr state\n1     1 1993-06-12   Mic\n2     1 1995-05-13  Norm\n3     1 2000-01-26   Mic\n4     1 2001-12-26  Norm\n5     1 2007-04-27   Mic\n6     2 1993-10-15  Norm\n7     2 1997-09-15   Mic\n8     2 1997-10-14   Mac\n9     2 2003-06-05   Mic\n10    2 2009-05-29   Mic\n11    3 1993-07-22   Mic\n12    3 1995-07-06   Mic\n13    3 1997-03-18   Mac\n14    4 1994-09-05   Mic\n15    4 1996-05-05   Mic\n16    4 1999-03-12   Mic\n17    5 1993-10-09   Mac\n18    5 1997-01-28   Mac\n19    6 1993-08-17  Norm\n20    6 1996-02-16  Norm\n21    6 1998-07-17   Mic\n22    6 2004-05-28   Mac\n24    7 1994-01-16  Norm\n25    7 1995-07-09  Norm\n26    7 1999-03-22  Norm\n27    8 1994-05-14   Mac\n28    8 1996-10-11   Mac\n29    8 1997-12-25   Mac\n30    8 2003-10-05   Mac\n31    9 1993-07-10   Mic\n32   10 1994-09-26   Mac\n33   10 1995-10-30   Mac\n34   10 1998-01-30   Mac\n35   10 2004-08-30   Mac\n36   11 1994-07-12   Mic\n37   11 1996-12-23   Mac\n38   12 1994-09-25   Mic\n39   12 1995-05-02   Mic\n40   12 1998-11-17   Mac\n41   13 1993-12-25  Norm\n42   13 1996-01-12   Mic\n43   13 1998-11-21  Norm\n44   13 2002-04-05   Mic\n45   13 2014-08-14   Mic\n46   14 1993-07-14   Mic\n47   14 1996-05-07   Mic\n48   14 1997-11-30   Mic\n50   15 1994-05-15  Norm\n51   15 1995-06-28  Norm\n52   15 2000-07-12  Norm\n53   15 2002-11-09  Norm\n55   16 1994-07-16   Mic\n56   16 1996-11-27   Mic\n57   16 1997-03-27   Mac\n58   16 2004-04-15   Mic\n59   17 1993-09-08   Mic\n60   17 1996-11-09   Mic\n61   17 1999-10-20   Mac\n62   17 2006-09-12   Mic\n63   18 1994-07-08   Mic\n64   18 1995-01-18   Mic\n65   18 1998-02-01  Norm\n66   18 2006-06-05  Norm\n67   18 2013-10-18  Norm\n68   19 1994-02-09   Mic\n69   19 1997-02-03   Mic\n70   19 2001-03-15   Mic\n71   19 2001-09-20   Mac\n73   20 1994-09-18   Mac\n74   20 1995-03-13   Mic\n75   20 1998-11-01   Mic\n76   20 2001-02-12   Mic\n77   20 2008-12-24   Mac\n78   21 1994-04-29   Mic\n79   21 1996-12-15   Mac\n80   21 1998-05-01   Mic\n81   21 2006-01-26   Mic\n83   22 1993-06-09   Mic\n84   22 1995-11-16  Norm\n85   22 1997-09-05  Norm\n86   22 2004-02-06   Mic\n87   22 2007-01-15   Mic\n88   23 1994-12-30   Mac\n89   23 1996-10-16   Mac\n90   24 1993-09-25   Mic\n91   24 1996-07-23   Mic\n92   24 1999-09-09  Norm\n93   24 2005-06-07  Norm\n94   25 1994-01-11   Mic\n95   25 1997-01-09   Mic\n96   25 1997-06-02   Mic\n97   25 2002-01-05   Mic\n98   25 2008-10-22   Mac\n99   26 1994-02-17   Mic\n100  26 1995-06-14   Mic\n101  26 1999-10-03   Mac\n102  27 1995-03-01   Mic\n103  27 1995-09-09   Mic\n104  27 2000-06-02   Mic\n105  27 2005-05-06   Mic\n106  28 1993-12-21   Mac\n107  28 1996-12-26   Mac\n108  28 2000-04-05   Mic\n109  28 2003-04-21  Norm\n110  28 2007-12-08   Mic\n111  29 1993-12-20   Mic\n112  29 1996-06-28   Mac\n113  29 2000-10-17   Mac\n114  30 1993-12-14   Mic\n115  30 1995-08-30  Norm\n116  30 1999-06-24  Norm\n117  31 1994-10-03   Mic\n118  31 1996-05-15   Mic\n119  31 2001-03-03   Mic\n120  31 2001-09-04   Mac\n121  32 1994-07-13   Mac\n122  32 1996-04-27   Mic\n123  33 1994-08-02   Mic\n124  33 1995-05-01  Norm\n125  33 1997-08-27  Norm\n126  33 2005-05-28  Norm\n128  34 1993-05-28   Mic\n129  34 1996-11-13   Mic\n130  34 1998-07-23   Mic\n131  34 2002-04-14   Mac\n133  35 1995-02-12   Mic\n134  35 1995-06-22   Mic\n135  35 1999-07-25   Mac\n136  36 1994-10-01   Mic\n137  36 1995-02-02  Norm\n138  36 2001-06-06   Mic\n139  36 2002-03-22   Mic\n140  36 2013-07-28   Mic\n141  37 1993-10-06   Mic\n142  37 1996-02-06   Mic\n143  37 1998-05-17   Mic\n144  37 2004-04-09   Mic\n145  38 1994-02-23   Mac\n146  38 1995-11-16   Mic\n147  38 1999-09-13   Mac\n148  39 1993-09-25   Mic\n149  39 1996-11-16   Mic\n150  39 1998-06-10  Norm\n151  39 2001-11-17  Norm\n152  39 2006-10-26  Norm\n153  40 1994-09-03   Mic\n154  40 1996-05-12   Mic\n155  41 1994-09-01   Mic\n156  41 1997-04-18  Norm\n157  41 1998-07-07   Mic\n158  42 1995-02-16   Mic\n159  42 1995-08-08   Mic\n160  42 1997-09-07   Mic\n161  43 1993-11-24   Mac\n162  43 1995-08-27   Mic\n163  43 1998-02-05   Mac\n164  44 1994-06-23   Mic\n165  44 1996-09-28   Mic\n166  44 2000-06-01   Mic\n167  45 1993-12-24   Mic\n168  45 1996-04-14   Mic\n169  45 1998-08-17   Mac\n170  45 2002-04-01   Mic\n171  45 2009-10-22   Mic\n172  47 1993-06-06   Mic\n173  47 1997-01-09   Mic\n174  47 2000-08-07   Mic\n175  48 1993-09-02   Mic\n176  48 1996-01-15   Mic\n178  50 1995-02-20   Mic\n179  50 1995-06-11   Mic\n180  50 2001-01-28   Mac\n181  51 1994-03-27  Norm\n182  51 1995-07-17  Norm\n183  51 1999-05-23  Norm\n184  51 2001-04-14  Norm\n185  51 2010-07-12   Mic\n186  52 1993-12-19   Mic\n187  52 1995-12-11   Mic\n188  52 1997-10-03   Mic\n189  52 2003-02-28   Mic\n190  52 2009-10-26   Mic\n191  53 1993-04-21   Mac\n192  53 1996-07-29   Mic\n193  53 1999-05-07   Mac\n194  54 1994-06-09   Mic\n195  54 1996-10-14  Norm\n196  54 2000-10-11  Norm\n197  54 2003-09-10  Norm\n198  54 2008-05-10  Norm\n199  55 1995-01-08  Norm\n200  55 1995-09-05  Norm\n201  55 2000-04-09   Mic\n202  55 2005-08-27   Mic\n204  56 1993-11-11  Norm\n205  56 1996-05-17   Mic\n206  56 1998-10-21   Mic\n207  56 2004-08-19  Norm\n208  57 1993-11-09   Mac\n209  57 1996-01-22   Mic\n210  58 1994-10-26   Mic\n211  58 1997-01-13   Mic\n212  58 2001-01-04   Mic\n213  59 1993-11-19   Mac\n214  59 1997-09-01   Mac\n215  59 1998-02-25   Mac\n216  60 1993-12-26  Norm\n217  60 1996-05-26   Mic\n218  61 1994-09-25   Mac\n219  61 1995-05-08   Mac\n220  61 2001-03-24   Mac\n221  61 2005-03-18   Mic\n223  62 1993-07-25  Norm\n224  62 1995-10-20  Norm\n225  62 1998-04-13   Mic\n226  62 2001-06-07   Mac\n227  63 1994-02-16   Mac\n228  63 1997-02-07   Mac\n229  64 1994-07-09   Mic\n230  64 1995-12-12   Mic\n231  64 2000-01-01   Mic\n232  65 1994-08-28  Norm\n233  65 1995-09-20  Norm\n234  65 1999-09-30  Norm\n235  65 2001-04-29   Mic\n236  65 2012-07-05   Mic\n237  66 1994-04-15   Mic\n238  67 1993-10-11   Mic\n239  67 1995-10-12   Mac\n240  68 1995-10-07   Mic\n241  68 1997-04-12   Mic\n242  69 1993-09-19   Mic\n243  69 1996-06-27   Mic\n244  69 1998-03-12   Mic\n245  69 2003-06-11   Mic\n246  69 2012-01-09   Mic\n247  70 1994-04-15   Mic\n248  70 1996-06-17   Mic\n249  70 1999-06-28   Mac\n250  70 2002-03-01  Norm\n251  71 1994-02-27   Mic\n252  71 1995-11-03   Mic\n253  71 2000-10-04   Mic\n254  71 2005-11-14   Mic\n255  71 2013-07-14  Norm\n256  72 1995-03-04   Mic\n257  72 1996-09-16   Mic\n258  72 1999-04-28   Mic\n260  73 1993-05-02   Mic\n261  73 1997-07-24   Mic\n262  74 1995-03-09   Mic\n263  74 1997-05-02   Mic\n264  74 2000-02-21   Mac\n265  75 1993-12-08   Mic\n266  75 1997-04-26   Mic\n267  75 2001-02-04  Norm\n268  75 2002-10-17   Mic\n269  76 1993-10-18  Norm\n270  77 1993-05-20   Mic\n271  77 1996-10-31   Mic\n272  77 1998-10-21  Norm\n273  77 2001-07-28   Mic\n274  77 2012-12-10  Norm\n275  78 1993-12-10  Norm\n276  78 1995-11-03  Norm\n277  78 1998-04-10  Norm\n278  78 2005-03-12  Norm\n280  79 1994-04-24   Mic\n281  79 1995-08-03  Norm\n282  80 1993-11-04   Mic\n283  80 1996-10-01   Mic\n284  80 1999-10-05   Mic\n285  80 2006-02-16   Mic\n287  81 1994-11-20   Mic\n288  81 1996-01-31   Mic\n289  82 1993-05-17   Mic\n290  82 1995-08-19   Mic\n291  84 1993-09-13   Mic\n292  84 2004-09-28   Mac\n293  85 1993-10-02   Mic\n294  85 1995-11-30   Mic\n295  85 1997-06-24   Mac\n296  85 2004-05-27   Mac\n297  86 1993-12-06  Norm\n298  86 1996-01-15  Norm\n299  86 1997-11-10  Norm\n300  86 2001-10-05  Norm\n301  86 2014-07-30   Mac\n302  87 1994-04-05   Mic\n303  87 1996-08-23   Mic\n304  87 1997-09-07   Mac\n305  87 2002-05-17   Mac\n306  88 1995-03-06   Mic\n307  88 1995-11-30   Mic\n308  88 1998-07-27   Mic\n309  88 2004-04-18   Mic\n310  88 2011-10-16   Mac\n311  89 1993-12-12   Mic\n312  89 1997-02-03   Mic\n313  89 1999-01-17   Mic\n314  89 2004-07-02   Mic\n315  89 2008-01-27   Mic\n316  90 1995-06-21   Mic\n318  90 2000-03-22   Mac\n319  91 1994-03-26   Mic\n320  91 1996-05-02   Mic\n321  91 2000-11-29   Mic\n322  92 1994-09-01   Mic\n323  92 1996-01-02   Mic\n324  92 1998-08-12   Mic\n325  92 2003-11-10   Mic\n326  92 2012-06-19   Mac\n327  93 1993-10-19  Norm\n328  93 1996-12-10  Norm\n329  94 1995-03-23   Mic\n330  94 1995-11-04   Mic\n331  94 1999-01-01  Norm\n332  94 2002-02-03  Norm\n333  95 1995-01-20   Mic\n334  95 1996-09-13   Mic\n335  95 2000-01-26   Mic\n336  96 1994-02-08  Norm\n337  96 1996-09-07  Norm\n338  96 1998-08-03  Norm\n339  97 1994-06-10  Norm\n340  97 1996-02-24  Norm\n341  97 1998-05-20   Mic\n342  97 2002-01-19   Mic\n344  98 1994-12-11  Norm\n345  98 1995-09-03  Norm\n346  98 2000-09-12   Mic\n347  98 2002-07-26   Mic\n348 100 1995-01-12   Mic\n349 100 1996-11-28   Mic\n350 100 1999-02-19   Mic\n351 100 2004-02-08   Mic\n353 101 1995-06-10   Mic\n354 101 1996-08-16  Norm\n355 101 1999-01-26  Norm\n356 101 2001-11-25   Mic\n357 101 2010-06-29   Mac\n358 103 1993-10-17   Mic\n359 103 1996-05-22   Mic\n360 103 1998-09-25  Norm\n361 103 2004-05-18   Mic\n363 104 1994-12-05   Mic\n364 104 1995-11-10   Mic\n365 104 1999-08-20   Mic\n366 104 2005-12-19   Mac\n367 104 2009-01-21   Mac\n368 105 1994-03-29   Mic\n369 105 1995-10-28   Mic\n370 105 1997-09-28   Mic\n371 105 2005-01-26   Mic\n372 105 2012-10-09   Mic\n373 107 1994-08-29   Mic\n374 107 1996-12-20   Mic\n375 107 1997-10-31  Norm\n376 108 1994-11-07  Norm\n377 108 1995-10-10  Norm\n378 108 1998-12-12  Norm\n379 108 2005-01-26   Mic\n380 108 2011-05-19  Norm\n381 110 1994-02-16   Mic\n382 110 1996-04-28  Norm\n383 110 1999-08-26  Norm\n384 110 2003-12-08   Mic\n385 111 1995-02-26   Mic\n386 111 1996-04-23   Mic\n387 111 1998-10-01  Norm\n388 111 2001-09-28  Norm\n389 111 2007-07-21  Norm\n390 112 1993-10-09   Mic\n391 112 1996-09-21   Mic\n392 112 1998-04-28   Mic\n393 112 2004-07-29   Mic\n394 112 2011-12-18   Mic\n395 113 1994-12-12   Mic\n396 113 1996-04-02  Norm\n397 114 1994-07-25   Mic\n398 114 1996-03-07   Mic\n399 114 2000-08-11   Mic\n400 115 1995-07-05  Norm\n401 115 1997-02-25  Norm\n402 115 1998-09-07  Norm\n403 115 2002-07-16  Norm\n404 115 2009-02-21  Norm\n405 116 1994-02-23   Mic\n406 116 1997-01-01   Mic\n407 116 1997-09-06   Mic\n408 118 1995-06-17   Mic\n409 118 1996-12-29   Mac\n410 118 1997-11-08   Mac\n411 119 1994-06-22  Norm\n412 119 1996-11-12  Norm\n413 120 1994-01-19   Mic\n414 120 1996-05-28   Mic\n415 120 1999-03-04  Norm\n416 121 1994-05-24   Mic\n417 121 1996-06-11   Mac\n418 121 2000-07-24   Mac\n419 121 2005-08-03   Mic\n420 123 1995-02-12   Mic\n421 123 1996-11-16   Mic\n422 123 1998-09-28  Norm\n423 123 2004-11-21   Mic\n424 123 2006-12-04  Norm\n425 126 1994-12-13   Mic\n426 126 1997-01-14   Mic\n427 126 1998-06-24   Mic\n428 126 2003-01-21   Mic\n429 126 2012-01-06   Mic\n431 129 1995-07-02   Mic\n432 129 1997-08-19   Mic\n433 129 1998-07-12  Norm\n434 130 1994-06-09   Mic\n435 130 1996-07-18   Mic\n436 130 2000-11-29   Mac\n437 130 2002-10-16  Norm\n438 131 1995-08-11  Norm\n439 131 1997-05-05  Norm\n440 131 1997-10-04  Norm\n441 131 2001-12-29   Mac\n442 133 1995-09-29  Norm\n444 133 1998-09-23  Norm\n445 133 2003-01-01  Norm\n446 134 1994-09-27   Mic\n447 135 1995-03-16   Mic\n448 135 1996-03-26   Mic\n449 135 1999-02-25   Mic\n450 136 1995-08-17   Mic\n451 136 1997-03-20   Mac\n452 136 1997-09-08  Norm\n453 136 2001-12-06   Mac\n454 137 1995-07-13  Norm\n455 137 1995-11-22  Norm\n456 138 1994-06-27  Norm\n457 138 1997-03-28  Norm\n458 138 1997-06-25  Norm\n459 138 2004-03-25  Norm\n460 138 2009-05-13  Norm\n461 139 1995-07-07   Mic\n462 139 1996-09-30   Mic\n463 140 1994-10-24   Mic\n464 140 1995-10-26  Norm\n465 140 1999-06-04  Norm\n467 140 2008-08-12   Mic\n468 142 1995-08-05   Mic\n469 142 1995-09-13   Mic\n470 142 2001-08-03   Mic\n471 142 2002-03-03   Mic\n472 143 1993-12-31   Mac\n473 143 1996-05-07   Mic\n474 143 1999-06-01  Norm\n475 144 1994-10-09   Mic\n476 144 1996-03-10   Mic\n477 144 2001-03-29   Mic\n478 144 2005-11-30   Mic\n479 145 1995-03-23   Mic\n480 145 1997-07-17  Norm\n481 146 1994-12-13   Mic\n482 146 1997-06-08  Norm\n483 147 1994-08-05   Mic\n484 147 1996-08-16   Mic\n485 147 2007-05-21   Mic\n486 148 1995-02-21   Mic\n487 148 1996-02-01   Mac\n488 148 1998-09-05   Mic\n489 148 2003-01-17  Norm\n491 151 1995-05-10   Mic\n492 151 1997-06-12   Mac\n493 151 1998-05-09   Mac\n494 152 1994-10-31   Mic\n495 152 1996-05-26   Mic\n496 152 2000-06-22   Mic\n497 152 2003-11-23   Mic\n498 152 2009-03-19   Mic\n499 153 1994-05-08   Mac\n500 153 1997-04-18   Mac\n501 153 1999-09-13   Mic\n502 155 1995-02-07   Mic\n503 155 1997-01-07   Mac\n504 155 2000-03-22   Mic\n505 155 2005-05-17   Mac\n506 155 2007-06-20   Mac\n507 156 1995-06-23   Mic\n508 156 1997-07-30   Mic\n509 156 1998-12-06   Mic\n510 156 2001-10-05   Mic\n511 156 2010-12-26   Mac\n512 157 1994-05-16   Mic\n513 157 1996-02-21   Mac\n514 157 2006-10-26   Mac\n515 158 1995-04-07   Mic\n516 158 1997-09-19   Mic\n517 158 2001-02-04   Mac\n518 158 2005-09-20   Mic\n519 159 1994-02-28   Mic\n520 159 1996-10-28  Norm\n521 159 1999-03-14   Mic\n522 159 2006-07-09   Mac\n523 160 1994-10-15   Mic\n524 160 1996-06-06  Norm\n525 161 1995-05-03   Mic\n526 161 1995-12-21   Mic\n527 161 1999-03-08   Mic\n528 161 2003-07-10   Mic\n529 161 2013-01-28   Mac\n530 163 1994-12-20   Mic\n531 163 1996-12-18   Mac\n532 163 2000-04-21   Mac\n533 165 1995-01-29  Norm\n534 165 1996-01-01  Norm\n535 165 1999-09-20   Mic\n536 166 1994-12-05   Mic\n537 166 1996-07-01   Mic\n538 166 2000-03-17  Norm\n539 166 2003-01-16   Mic\n540 167 1994-03-30  Norm\n541 167 1996-04-29  Norm\n542 167 1998-05-16  Norm\n543 167 2006-06-29  Norm\n544 167 2010-08-28  Norm\n545 168 1994-03-19   Mic\n546 168 1996-04-17   Mac\n547 168 1999-07-17   Mac\n548 168 2003-04-10   Mac\n550 169 1995-05-02   Mic\n551 169 1995-12-18  Norm\n552 169 1998-08-24  Norm\n553 169 2005-10-28   Mic\n554 169 2013-05-23   Mic\n555 170 1994-09-07   Mic\n556 170 1997-04-03  Norm\n557 170 2000-02-07  Norm\n558 170 2006-01-09   Mic\n560 171 1994-10-09   Mic\n561 171 1996-05-23  Norm\n562 171 2001-09-24   Mac\n563 171 2004-08-24   Mac\n564 171 2011-04-13   Mic\n565 172 1994-06-23   Mic\n566 172 1996-02-12   Mic\n567 172 1998-04-02   Mac\n568 172 2002-02-23   Mac\n569 173 1995-04-26  Norm\n570 173 1996-06-12  Norm\n571 173 1999-11-21  Norm\n572 173 2002-09-08  Norm\n574 174 1995-02-05   Mic\n576 174 2000-07-25   Mic\n577 174 2003-04-10   Mic\n578 174 2009-04-16   Mic\n579 175 1994-09-19   Mic\n580 175 1997-02-27   Mic\n581 175 1998-10-13   Mic\n582 175 2004-08-02  Norm\n583 175 2014-06-21   Mac\n584 176 1994-11-10  Norm\n585 176 1997-02-08  Norm\n586 176 1998-12-07  Norm\n587 176 2005-09-17  Norm\n588 176 2013-05-22  Norm\n\n\n[[1]]\n  records max_id\n1     160    176\n\n[[2]]\n  records max_id\n1     160    176\n\n[[3]]\n  records max_id\n1     156    176\n\n\n[[1]]\n   id allo sex baseCVD deathCVD      doBth       doDM     doBase     doCVD1\n1  83  Int   M       0        1 1930-12-23 1989-06-28 1992-12-22 1994-12-04\n2 102  Int   M       0        1 1935-04-09 1990-03-19 1993-07-17 1995-04-23\n3 128 Conv   M       0        0 1937-03-14 1989-02-16 1993-10-03 1995-11-13\n4 150 Conv   F       0        0 1946-02-18 1989-10-29 1993-12-24       &lt;NA&gt;\n      doCVD2     doCVD3     doESRD      doEnd      doDth\n1 1994-12-12       &lt;NA&gt;        NaN 1994-11-15 1994-12-19\n2       &lt;NA&gt;       &lt;NA&gt;        NaN 1995-03-30 1995-05-02\n3 2003-08-07 2004-05-31 1999-12-26 2005-05-19 2005-05-18\n4       &lt;NA&gt;       &lt;NA&gt;        NaN 2014-08-27       &lt;NA&gt;\n\n[[2]]\n [1] id       allo     sex      baseCVD  deathCVD doBth    doDM     doBase  \n [9] doCVD1   doCVD2   doCVD3   doESRD   doEnd    doDth   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n       id           allo    sex        baseCVD          deathCVD     \n Min.   :  1.00   Int :80   F: 41   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 40.75   Conv:80   M:119   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 82.50                     Median :0.0000   Median :0.0000  \n Mean   : 85.31                     Mean   :0.1375   Mean   :0.2375  \n 3rd Qu.:130.25                     3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :176.00                     Max.   :1.0000   Max.   :1.0000  \n                                                                     \n     doBth                 doDM                doBase          \n Min.   :1926-07-13   Min.   :1962-06-21   Min.   :1992-12-22  \n 1st Qu.:1932-07-18   1st Qu.:1982-10-01   1st Qu.:1993-03-23  \n Median :1937-03-12   Median :1987-05-02   Median :1993-06-11  \n Mean   :1938-06-03   Mean   :1986-01-15   Mean   :1993-07-22  \n 3rd Qu.:1944-11-28   3rd Qu.:1990-05-03   3rd Qu.:1993-11-15  \n Max.   :1956-04-07   Max.   :1994-02-07   Max.   :1994-06-14  \n                                                               \n     doCVD1               doCVD2               doCVD3          \n Min.   :1993-03-17   Min.   :1994-08-05   Min.   :1995-12-16  \n 1st Qu.:1997-02-16   1st Qu.:1999-04-18   1st Qu.:2001-11-23  \n Median :2001-04-03   Median :2002-12-20   Median :2004-12-13  \n Mean   :2001-12-18   Mean   :2003-10-04   Mean   :2005-01-11  \n 3rd Qu.:2006-07-20   3rd Qu.:2008-07-20   3rd Qu.:2009-06-03  \n Max.   :2014-05-30   Max.   :2014-03-26   Max.   :2012-07-21  \n NA's   :41           NA's   :84           NA's   :117         \n     doESRD               doEnd                doDth           \n Min.   :1998-02-18   Min.   :1994-11-16   Min.   :1994-12-19  \n 1st Qu.:2002-09-25   1st Qu.:2002-09-18   1st Qu.:2000-05-21  \n Median :2008-05-08   Median :2010-02-11   Median :2004-03-06  \n Mean   :2006-07-17   Mean   :2008-08-29   Mean   :2004-05-24  \n 3rd Qu.:2009-07-29   3rd Qu.:2014-09-25   3rd Qu.:2009-06-04  \n Max.   :2014-07-07   Max.   :2014-12-31   Max.   :2014-02-16  \n NA's   :145                               NA's   :67          \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 454 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ 1, data = steno2)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    160       0    1.000 0.00000        1.000        1.000\n    1    160       0    1.000 0.00000        1.000        1.000\n    2    158       2    0.988 0.00878        0.970        1.000\n    3    155       3    0.969 0.01376        0.942        0.996\n    4    154       1    0.963 0.01502        0.934        0.992\n    5    148       6    0.925 0.02082        0.885        0.967\n    6    143       5    0.894 0.02436        0.847        0.943\n    7    137       6    0.856 0.02774        0.804        0.912\n    8    131       6    0.819 0.03045        0.761        0.881\n    9    125       6    0.781 0.03268        0.720        0.848\n   10    116       9    0.725 0.03530        0.659        0.798\nCall: survfit(formula = Surv(time_yrs, event_num) ~ allo, data = steno2)\n\n                allo=Int \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     80       0    1.000  0.0000        1.000        1.000\n    1     80       0    1.000  0.0000        1.000        1.000\n    2     78       2    0.975  0.0175        0.941        1.000\n    3     77       1    0.963  0.0212        0.922        1.000\n    4     76       1    0.950  0.0244        0.903        0.999\n    5     75       1    0.938  0.0271        0.886        0.992\n    6     73       2    0.913  0.0316        0.853        0.977\n    7     70       3    0.875  0.0370        0.805        0.951\n    8     66       4    0.825  0.0425        0.746        0.913\n    9     64       2    0.800  0.0447        0.717        0.893\n   10     63       1    0.788  0.0457        0.703        0.882\n\n                allo=Conv \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     80       0    1.000  0.0000        1.000        1.000\n    1     80       0    1.000  0.0000        1.000        1.000\n    2     80       0    1.000  0.0000        1.000        1.000\n    3     78       2    0.975  0.0175        0.941        1.000\n    4     78       0    0.975  0.0175        0.941        1.000\n    5     73       5    0.913  0.0316        0.853        0.977\n    6     70       3    0.875  0.0370        0.805        0.951\n    7     67       3    0.838  0.0412        0.760        0.922\n    8     65       2    0.813  0.0436        0.731        0.903\n    9     61       4    0.763  0.0476        0.675        0.862\n   10     53       8    0.663  0.0529        0.567        0.775\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ sex, data = steno2)\n\n                sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     41       0    1.000  0.0000        1.000        1.000\n    1     41       0    1.000  0.0000        1.000        1.000\n    2     41       0    1.000  0.0000        1.000        1.000\n    3     40       1    0.976  0.0241        0.930        1.000\n    4     40       0    0.976  0.0241        0.930        1.000\n    5     40       0    0.976  0.0241        0.930        1.000\n    6     39       1    0.951  0.0336        0.888        1.000\n    7     37       2    0.902  0.0463        0.816        0.998\n    8     34       3    0.829  0.0588        0.722        0.953\n    9     31       3    0.756  0.0671        0.635        0.900\n   10     29       2    0.707  0.0711        0.581        0.861\n\n                sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    119       0    1.000  0.0000        1.000        1.000\n    1    119       0    1.000  0.0000        1.000        1.000\n    2    117       2    0.983  0.0118        0.960        1.000\n    3    115       2    0.966  0.0165        0.935        0.999\n    4    114       1    0.958  0.0184        0.923        0.995\n    5    108       6    0.908  0.0266        0.857        0.961\n    6    104       4    0.874  0.0304        0.816        0.936\n    7    100       4    0.840  0.0336        0.777        0.909\n    8     97       3    0.815  0.0356        0.748        0.888\n    9     94       3    0.790  0.0373        0.720        0.867\n   10     87       7    0.731  0.0406        0.656        0.815\n\nCall: survfit(formula = Surv(time_yrs, event_num) ~ allo + sex, data = steno2)\n\n                allo=Int, sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     17       0    1.000  0.0000        1.000            1\n    1     17       0    1.000  0.0000        1.000            1\n    2     17       0    1.000  0.0000        1.000            1\n    3     17       0    1.000  0.0000        1.000            1\n    4     17       0    1.000  0.0000        1.000            1\n    5     17       0    1.000  0.0000        1.000            1\n    6     17       0    1.000  0.0000        1.000            1\n    7     17       0    1.000  0.0000        1.000            1\n    8     15       2    0.882  0.0781        0.742            1\n    9     14       1    0.824  0.0925        0.661            1\n   10     14       0    0.824  0.0925        0.661            1\n\n                allo=Int, sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     63       0    1.000  0.0000        1.000        1.000\n    1     63       0    1.000  0.0000        1.000        1.000\n    2     61       2    0.968  0.0221        0.926        1.000\n    3     60       1    0.952  0.0268        0.901        1.000\n    4     59       1    0.937  0.0307        0.878        0.999\n    5     58       1    0.921  0.0341        0.856        0.990\n    6     56       2    0.889  0.0396        0.815        0.970\n    7     53       3    0.841  0.0460        0.756        0.937\n    8     51       2    0.810  0.0495        0.718        0.913\n    9     50       1    0.794  0.0510        0.700        0.900\n   10     49       1    0.778  0.0524        0.682        0.888\n\n                allo=Conv, sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     24       0    1.000  0.0000        1.000        1.000\n    1     24       0    1.000  0.0000        1.000        1.000\n    2     24       0    1.000  0.0000        1.000        1.000\n    3     23       1    0.958  0.0408        0.882        1.000\n    4     23       0    0.958  0.0408        0.882        1.000\n    5     23       0    0.958  0.0408        0.882        1.000\n    6     22       1    0.917  0.0564        0.813        1.000\n    7     20       2    0.833  0.0761        0.697        0.997\n    8     19       1    0.792  0.0829        0.645        0.972\n    9     17       2    0.708  0.0928        0.548        0.916\n   10     15       2    0.625  0.0988        0.458        0.852\n\n                allo=Conv, sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     56       0    1.000  0.0000        1.000        1.000\n    1     56       0    1.000  0.0000        1.000        1.000\n    2     56       0    1.000  0.0000        1.000        1.000\n    3     55       1    0.982  0.0177        0.948        1.000\n    4     55       0    0.982  0.0177        0.948        1.000\n    5     50       5    0.893  0.0413        0.815        0.978\n    6     48       2    0.857  0.0468        0.770        0.954\n    7     47       1    0.839  0.0491        0.748        0.941\n    8     46       1    0.821  0.0512        0.727        0.928\n    9     44       2    0.786  0.0548        0.685        0.901\n   10     38       6    0.679  0.0624        0.567        0.813\n\n\n[[1]]\nCall: survfit(formula = Surv(time_yrs, event_num) ~ 1, data = steno2)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    160       0    1.000 0.00000        1.000        1.000\n    1    160       0    1.000 0.00000        1.000        1.000\n    2    158       2    0.988 0.00878        0.970        1.000\n    3    155       3    0.969 0.01376        0.942        0.996\n    4    154       1    0.963 0.01502        0.934        0.992\n    5    148       6    0.925 0.02082        0.885        0.967\n    6    143       5    0.894 0.02436        0.847        0.943\n    7    137       6    0.856 0.02774        0.804        0.912\n    8    131       6    0.819 0.03045        0.761        0.881\n    9    125       6    0.781 0.03268        0.720        0.848\n   10    116       9    0.725 0.03530        0.659        0.798\n\n[[2]]\nCall: survfit(formula = Surv(time_yrs, event_num) ~ allo, data = steno2)\n\n                allo=Int \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     80       0    1.000  0.0000        1.000        1.000\n    1     80       0    1.000  0.0000        1.000        1.000\n    2     78       2    0.975  0.0175        0.941        1.000\n    3     77       1    0.963  0.0212        0.922        1.000\n    4     76       1    0.950  0.0244        0.903        0.999\n    5     75       1    0.938  0.0271        0.886        0.992\n    6     73       2    0.913  0.0316        0.853        0.977\n    7     70       3    0.875  0.0370        0.805        0.951\n    8     66       4    0.825  0.0425        0.746        0.913\n    9     64       2    0.800  0.0447        0.717        0.893\n   10     63       1    0.788  0.0457        0.703        0.882\n\n                allo=Conv \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     80       0    1.000  0.0000        1.000        1.000\n    1     80       0    1.000  0.0000        1.000        1.000\n    2     80       0    1.000  0.0000        1.000        1.000\n    3     78       2    0.975  0.0175        0.941        1.000\n    4     78       0    0.975  0.0175        0.941        1.000\n    5     73       5    0.913  0.0316        0.853        0.977\n    6     70       3    0.875  0.0370        0.805        0.951\n    7     67       3    0.838  0.0412        0.760        0.922\n    8     65       2    0.813  0.0436        0.731        0.903\n    9     61       4    0.763  0.0476        0.675        0.862\n   10     53       8    0.663  0.0529        0.567        0.775\n\n\n[[3]]\nCall: survfit(formula = Surv(time_yrs, event_num) ~ sex, data = steno2)\n\n                sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     41       0    1.000  0.0000        1.000        1.000\n    1     41       0    1.000  0.0000        1.000        1.000\n    2     41       0    1.000  0.0000        1.000        1.000\n    3     40       1    0.976  0.0241        0.930        1.000\n    4     40       0    0.976  0.0241        0.930        1.000\n    5     40       0    0.976  0.0241        0.930        1.000\n    6     39       1    0.951  0.0336        0.888        1.000\n    7     37       2    0.902  0.0463        0.816        0.998\n    8     34       3    0.829  0.0588        0.722        0.953\n    9     31       3    0.756  0.0671        0.635        0.900\n   10     29       2    0.707  0.0711        0.581        0.861\n\n                sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    119       0    1.000  0.0000        1.000        1.000\n    1    119       0    1.000  0.0000        1.000        1.000\n    2    117       2    0.983  0.0118        0.960        1.000\n    3    115       2    0.966  0.0165        0.935        0.999\n    4    114       1    0.958  0.0184        0.923        0.995\n    5    108       6    0.908  0.0266        0.857        0.961\n    6    104       4    0.874  0.0304        0.816        0.936\n    7    100       4    0.840  0.0336        0.777        0.909\n    8     97       3    0.815  0.0356        0.748        0.888\n    9     94       3    0.790  0.0373        0.720        0.867\n   10     87       7    0.731  0.0406        0.656        0.815\n\n\n[[4]]\nCall: survfit(formula = Surv(time_yrs, event_num) ~ allo + sex, data = steno2)\n\n                allo=Int, sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     17       0    1.000  0.0000        1.000            1\n    1     17       0    1.000  0.0000        1.000            1\n    2     17       0    1.000  0.0000        1.000            1\n    3     17       0    1.000  0.0000        1.000            1\n    4     17       0    1.000  0.0000        1.000            1\n    5     17       0    1.000  0.0000        1.000            1\n    6     17       0    1.000  0.0000        1.000            1\n    7     17       0    1.000  0.0000        1.000            1\n    8     15       2    0.882  0.0781        0.742            1\n    9     14       1    0.824  0.0925        0.661            1\n   10     14       0    0.824  0.0925        0.661            1\n\n                allo=Int, sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     63       0    1.000  0.0000        1.000        1.000\n    1     63       0    1.000  0.0000        1.000        1.000\n    2     61       2    0.968  0.0221        0.926        1.000\n    3     60       1    0.952  0.0268        0.901        1.000\n    4     59       1    0.937  0.0307        0.878        0.999\n    5     58       1    0.921  0.0341        0.856        0.990\n    6     56       2    0.889  0.0396        0.815        0.970\n    7     53       3    0.841  0.0460        0.756        0.937\n    8     51       2    0.810  0.0495        0.718        0.913\n    9     50       1    0.794  0.0510        0.700        0.900\n   10     49       1    0.778  0.0524        0.682        0.888\n\n                allo=Conv, sex=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     24       0    1.000  0.0000        1.000        1.000\n    1     24       0    1.000  0.0000        1.000        1.000\n    2     24       0    1.000  0.0000        1.000        1.000\n    3     23       1    0.958  0.0408        0.882        1.000\n    4     23       0    0.958  0.0408        0.882        1.000\n    5     23       0    0.958  0.0408        0.882        1.000\n    6     22       1    0.917  0.0564        0.813        1.000\n    7     20       2    0.833  0.0761        0.697        0.997\n    8     19       1    0.792  0.0829        0.645        0.972\n    9     17       2    0.708  0.0928        0.548        0.916\n   10     15       2    0.625  0.0988        0.458        0.852\n\n                allo=Conv, sex=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0     56       0    1.000  0.0000        1.000        1.000\n    1     56       0    1.000  0.0000        1.000        1.000\n    2     56       0    1.000  0.0000        1.000        1.000\n    3     55       1    0.982  0.0177        0.948        1.000\n    4     55       0    0.982  0.0177        0.948        1.000\n    5     50       5    0.893  0.0413        0.815        0.978\n    6     48       2    0.857  0.0468        0.770        0.954\n    7     47       1    0.839  0.0491        0.748        0.941\n    8     46       1    0.821  0.0512        0.727        0.928\n    9     44       2    0.786  0.0548        0.685        0.901\n   10     38       6    0.679  0.0624        0.567        0.813\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nCall:\nsurvdiff(formula = Surv(time_yrs, event_num) ~ allo, data = steno2)\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\nallo=Int  80       38     51.7      3.64      8.29\nallo=Conv 80       55     41.3      4.56      8.29\n\n Chisq= 8.3  on 1 degrees of freedom, p= 0.004 \n\n\nCall:\nsurvdiff(formula = Surv(time_yrs, event_num) ~ sex, data = steno2)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F  41       20     25.5     1.182      1.63\nsex=M 119       73     67.5     0.446      1.63\n\n Chisq= 1.6  on 1 degrees of freedom, p= 0.2 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 4 × 8\n  model term     estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 allo  alloConv    0.602     0.212      2.84 0.00455    0.186     1.02 \n2 sex   sexM        0.321     0.253      1.27 0.203     -0.174     0.816\n3 both  alloConv    0.628     0.213      2.95 0.00316    0.211     1.05 \n4 both  sexM        0.379     0.253      1.50 0.135     -0.118     0.875\n\n\n# A tibble: 4 × 8\n  model term     estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 allo  alloConv     1.83     0.212      2.84 0.00455    1.20       2.77\n2 sex   sexM         1.38     0.253      1.27 0.203      0.840      2.26\n3 both  alloConv     1.87     0.213      2.95 0.00316    1.23       2.84\n4 both  sexM         1.46     0.253      1.50 0.135      0.889      2.40\n\n\n# A tibble: 2 × 3\n  allo  .fitted .se.fit\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Conv     1.83   0.287\n2 Int      1      0    \n\n\n# A tibble: 4 × 4\n  allo  sex   .fitted .se.fit\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Conv  M        2.74   0.567\n2 Int   M        1.46   0.306\n3 Conv  F        1.87   0.291\n4 Int   F        1      0"
  },
  {
    "objectID": "posts/2025-05-31-movingon/index.html",
    "href": "posts/2025-05-31-movingon/index.html",
    "title": "Moving on",
    "section": "",
    "text": "I’m BACK!\nAfter dabbling in an internship in an SMO and spending some time on other activities I am back to blogging, and here is what I want to look at the coming posts:\n\nFinding some Danish Data where I can play around with join() and its variants, probably the good old Epi::steno2, and its corresponding st2clin, and st2alb datasets.\nExploring what kinds of demographics data I can get from Statistics Denmark\nRecreation of the “increase in piracy due to increase in watermelon sales” example of a correlation =!= causation, but with Danish data from Statistics Denmark\nPlaying around with the safetyData() package and understanding how a CDISC-formatted clinical trial dataset can look like\n\nThis will be marked as an outlier from the usual focus on Danish data\n\nComparison of demographic characteristics of those who use the 5 most expensive drugs (in DDD)\nMaking a revealjs presentation of one of the blog posts\n\nAnd this post will also hold a promise to myself to better label the elements of the quarto document, for easier navigation and better output, by:\n\nlabeling, and using other chunk options\nusing snippets in Rstudio\nbetter hiding chunks that dont provide necessary info\nlearning from other Quarto-bloggers and definitely linking to them when I learn\nattempting to use visual studio code to produce the blog instead of Rstudio\nadd a refactored or abbreviated chunk with all the important code in the bottom\n\nThere is SO much to do!"
  },
  {
    "objectID": "posts/2024-09-20-medstat-function/index.html",
    "href": "posts/2024-09-20-medstat-function/index.html",
    "title": "Using R functions on medstat data",
    "section": "",
    "text": "I want to follow up the previous post, and the promise of showing how to put it all in to a function.\nAs has been written before here, if you do something several times in R, you might as well write a function.\nSo I am going to condense the code I wrote for the purpose of presenting turnover for some diabetes drug classes in the previous post, and turn it in to a function."
  },
  {
    "objectID": "posts/2024-09-20-medstat-function/index.html#purpose",
    "href": "posts/2024-09-20-medstat-function/index.html#purpose",
    "title": "Using R functions on medstat data",
    "section": "",
    "text": "I want to follow up the previous post, and the promise of showing how to put it all in to a function.\nAs has been written before here, if you do something several times in R, you might as well write a function.\nSo I am going to condense the code I wrote for the purpose of presenting turnover for some diabetes drug classes in the previous post, and turn it in to a function."
  },
  {
    "objectID": "posts/2024-09-20-medstat-function/index.html#turning-it-into-a-function",
    "href": "posts/2024-09-20-medstat-function/index.html#turning-it-into-a-function",
    "title": "Using R functions on medstat data",
    "section": "Turning it into a function",
    "text": "Turning it into a function\nSo, I don’t need to take all of the code from the previous post, just what I need for the function. I also slim the data down a bit with the select() function.\n\nlibrary(tidyverse)\n\nprocess_atc_data &lt;- function(year, url) {\n  # Read the dataset directly from the URL\n  df &lt;- read_delim(url, delim = \";\")  # Assuming the file is semicolon-delimited\n\n  # Attach column names\n  colnames(df) &lt;- c(\"atc\",\"year\",\"sector\",\"region\",\"sex\",\"agegroup\",\"count_persons\",\n                        \"count_persons_per1kpop\",\"turnover\",\"reimbursement\",\n                        \"sold_amount\", \"sold_amount_1kpop_day\", \"personreferabledata_perc\")\n\n  # the column with missing values messes stuff up\n  colnames(df)[is.na(colnames(df))] &lt;- \"missing_name\"\n  df &lt;- df |&gt;  select(-missing_name)\n\n  # Filter the dataset to get only what I need\n  df_filtered &lt;- df %&gt;%\n    filter(str_detect(agegroup, \"-\") & str_starts(atc, \"A10\")) |&gt;\n    select(atc,year,agegroup,turnover)\n\n  # Define drug classes\n  DCs &lt;- c(\"A10BJ\", \"A10BK\", \"A10BH\", \"A10BA\", \"A10BB\", \"A10BG\", \"A10BX\", \"A10A\")\n\n  # Further filter and add a new drug class variable\n  df_filtered &lt;- df_filtered %&gt;%\n    filter(atc %in% DCs) %&gt;%\n    mutate(\n      DC = case_when(\n        atc == \"A10BJ\" ~ \"GLP1\",\n        atc == \"A10BK\" ~ \"SGLT2\",\n        atc == \"A10BH\" ~ \"DPP4\",\n        atc == \"A10BA\" ~ \"Metformin\",\n        atc == \"A10BB\" ~ \"SU\",\n        atc == \"A10BG\" ~ \"Thiazo\",\n        atc == \"A10BX\" ~ \"Others\",\n        atc == \"A10A\"  ~ \"INSULIN\",\n        TRUE ~ NA_character_\n      ),\n      turnover1000k = turnover / 1000  # Turnover in 1000k units\n    )\n\n  # Assign the names to datasets and plots using the year variable\n  df_name &lt;- paste0(\"df_\", year)\n  assign(df_name, df_filtered, envir = .GlobalEnv)\n\n  # Return the processed data and the plots\n  return(list(data = df_filtered)) #, total_turnover_plot = p1, proportional_turnover_plot = p2\n}\n\nI have not brought over the code for making the plots, because I aim to use the facet_wrap() from ggplot2 to make some plots where its easier to see how the turnover has developed over the years.\nTo achieve that, I need to append the datasets I can create with the above function."
  },
  {
    "objectID": "posts/2024-09-20-medstat-function/index.html#creating-the-datasets-for-the-plots",
    "href": "posts/2024-09-20-medstat-function/index.html#creating-the-datasets-for-the-plots",
    "title": "Using R functions on medstat data",
    "section": "Creating the datasets for the plots",
    "text": "Creating the datasets for the plots\nNow, I have the list of datasets and their corresponding URLs from the last post. There should be a way to automate fetching the URLs, for example by searching the site and matching the line with the dataset name and the line with the link to download it.\nFor now, I just use what I already found.\n\n# get data from years 2016-2023\nprocess_atc_data(2016,\"https://medstat.dk/da/download/file/MjAxNl9hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1861363 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 88 dbl (9): 2016, 0...3, 0...4, ...7, ...8, 1925435,\n1159429, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 619 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2016 00-17        9354 INSULIN          9.35\n 2 A10A   2016 00-17       10103 INSULIN         10.1 \n 3 A10A   2016 00-17       19456 INSULIN         19.5 \n 4 A10A   2016 18-24        7880 INSULIN          7.88\n 5 A10A   2016 18-24       10883 INSULIN         10.9 \n 6 A10A   2016 18-24       18763 INSULIN         18.8 \n 7 A10A   2016 25-44       27658 INSULIN         27.7 \n 8 A10A   2016 25-44       41457 INSULIN         41.5 \n 9 A10A   2016 25-44       69115 INSULIN         69.1 \n10 A10A   2016 45-64       67858 INSULIN         67.9 \n# ℹ 609 more rows\n\nprocess_atc_data(2017,\"https://medstat.dk/da/download/file/MjAxN19hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1851679 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 88 dbl (9): 2017, 0...3, 0...4, ...7, ...8, 2008320,\n1212005, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 621 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2017 00-17        9197 INSULIN          9.20\n 2 A10A   2017 00-17       10691 INSULIN         10.7 \n 3 A10A   2017 00-17       19888 INSULIN         19.9 \n 4 A10A   2017 18-24        8042 INSULIN          8.04\n 5 A10A   2017 18-24       10468 INSULIN         10.5 \n 6 A10A   2017 18-24       18509 INSULIN         18.5 \n 7 A10A   2017 25-44       27295 INSULIN         27.3 \n 8 A10A   2017 25-44       41429 INSULIN         41.4 \n 9 A10A   2017 25-44       68724 INSULIN         68.7 \n10 A10A   2017 45-64       70076 INSULIN         70.1 \n# ℹ 611 more rows\n\nprocess_atc_data(2018,\"https://medstat.dk/da/download/file/MjAxOF9hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1841721 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 88 dbl (9): 2018, 0...3, 0...4, ...7, ...8, 2154536,\n1311885, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 607 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2018 00-17        9467 INSULIN          9.47\n 2 A10A   2018 00-17       11006 INSULIN         11.0 \n 3 A10A   2018 00-17       20473 INSULIN         20.5 \n 4 A10A   2018 18-24        8215 INSULIN          8.22\n 5 A10A   2018 18-24       10421 INSULIN         10.4 \n 6 A10A   2018 18-24       18636 INSULIN         18.6 \n 7 A10A   2018 25-44       28126 INSULIN         28.1 \n 8 A10A   2018 25-44       42639 INSULIN         42.6 \n 9 A10A   2018 25-44       70765 INSULIN         70.8 \n10 A10A   2018 45-64       72902 INSULIN         72.9 \n# ℹ 597 more rows\n\nprocess_atc_data(2019,\"https://medstat.dk/da/download/file/MjAxOV9hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1833788 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 89 dbl (9): 2019, 0...3, 0...4, ...7, ...8, 2395766,\n1467188, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 601 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2019 00-17        9599 INSULIN          9.60\n 2 A10A   2019 00-17       11231 INSULIN         11.2 \n 3 A10A   2019 00-17       20830 INSULIN         20.8 \n 4 A10A   2019 18-24        7867 INSULIN          7.87\n 5 A10A   2019 18-24       10313 INSULIN         10.3 \n 6 A10A   2019 18-24       18180 INSULIN         18.2 \n 7 A10A   2019 25-44       26903 INSULIN         26.9 \n 8 A10A   2019 25-44       41402 INSULIN         41.4 \n 9 A10A   2019 25-44       68305 INSULIN         68.3 \n10 A10A   2019 45-64       69854 INSULIN         69.9 \n# ℹ 591 more rows\n\nprocess_atc_data(2020,\"https://medstat.dk/da/download/file/MjAyMF9hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1817857 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 90 dbl (9): 2020, 0...3, 0...4, ...7, ...8, 2456459,\n1511380, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 609 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2020 00-17        9272 INSULIN          9.27\n 2 A10A   2020 00-17       10962 INSULIN         11.0 \n 3 A10A   2020 00-17       20234 INSULIN         20.2 \n 4 A10A   2020 18-24        7564 INSULIN          7.56\n 5 A10A   2020 18-24        9789 INSULIN          9.79\n 6 A10A   2020 18-24       17353 INSULIN         17.4 \n 7 A10A   2020 25-44       26183 INSULIN         26.2 \n 8 A10A   2020 25-44       39902 INSULIN         39.9 \n 9 A10A   2020 25-44       66085 INSULIN         66.1 \n10 A10A   2020 45-64       66259 INSULIN         66.3 \n# ℹ 599 more rows\n\nprocess_atc_data(2021,\"https://medstat.dk/da/download/file/MjAyMV9hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1802119 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 90 dbl (9): 2021, 0...3, 0...4, ...7, ...8, 2719403,\n1650909, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 613 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2021 00-17        9202 INSULIN          9.20\n 2 A10A   2021 00-17       10907 INSULIN         10.9 \n 3 A10A   2021 00-17       20109 INSULIN         20.1 \n 4 A10A   2021 18-24        7484 INSULIN          7.48\n 5 A10A   2021 18-24        9554 INSULIN          9.55\n 6 A10A   2021 18-24       17038 INSULIN         17.0 \n 7 A10A   2021 25-44       24773 INSULIN         24.8 \n 8 A10A   2021 25-44       38364 INSULIN         38.4 \n 9 A10A   2021 25-44       63137 INSULIN         63.1 \n10 A10A   2021 45-64       61082 INSULIN         61.1 \n# ℹ 603 more rows\n\nprocess_atc_data(2022,\"https://medstat.dk/da/download/file/MjAyMl9hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1814587 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 91 dbl (9): 2022, 0...3, 0...4, ...7, ...8, 3112077,\n1863246, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 609 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2022 00-17        7502 INSULIN          7.50\n 2 A10A   2022 00-17        8733 INSULIN          8.73\n 3 A10A   2022 00-17       16235 INSULIN         16.2 \n 4 A10A   2022 18-24        5899 INSULIN          5.90\n 5 A10A   2022 18-24        7943 INSULIN          7.94\n 6 A10A   2022 18-24       13841 INSULIN         13.8 \n 7 A10A   2022 25-44       19962 INSULIN         20.0 \n 8 A10A   2022 25-44       30580 INSULIN         30.6 \n 9 A10A   2022 25-44       50543 INSULIN         50.5 \n10 A10A   2022 45-64       49506 INSULIN         49.5 \n# ℹ 599 more rows\n\nprocess_atc_data(2023,\"https://medstat.dk/da/download/file/MjAyM19hdGNfY29kZV9kYXRhLnR4dA==\")\n\nNew names:\nRows: 1806544 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \";\" chr\n(4): A...1, A...5, A...6, 94 dbl (9): 2023, 0...3, 0...4, ...7, ...8, 5009988,\n2661589, ...11, ...12 lgl (1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `A` -&gt; `A...1`\n• `0` -&gt; `0...3`\n• `0` -&gt; `0...4`\n• `A` -&gt; `A...5`\n• `A` -&gt; `A...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...14`\n\n\n$data\n# A tibble: 563 × 6\n   atc    year agegroup turnover DC      turnover1000k\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 A10A   2023 00-17        7510 INSULIN          7.51\n 2 A10A   2023 00-17        8599 INSULIN          8.60\n 3 A10A   2023 00-17       16109 INSULIN         16.1 \n 4 A10A   2023 18-24        5980 INSULIN          5.98\n 5 A10A   2023 18-24        8058 INSULIN          8.06\n 6 A10A   2023 18-24       14039 INSULIN         14.0 \n 7 A10A   2023 25-44       19680 INSULIN         19.7 \n 8 A10A   2023 25-44       30630 INSULIN         30.6 \n 9 A10A   2023 25-44       50311 INSULIN         50.3 \n10 A10A   2023 45-64       46624 INSULIN         46.6 \n# ℹ 553 more rows\n\n\nActually, I could just use the purr::map() function.\n\n# Load the purr library\nlibrary(purrr)\n\n# Define a list of URLs corresponding to each year (2016 to 2023)\nurls &lt;- list(\n  \"2016\" = \"https://medstat.dk/da/download/file/MjAxNl9hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2017\" = \"https://medstat.dk/da/download/file/MjAxN19hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2018\" = \"https://medstat.dk/da/download/file/MjAxOF9hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2019\" = \"https://medstat.dk/da/download/file/MjAxOV9hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2020\" = \"https://medstat.dk/da/download/file/MjAyMF9hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2021\" = \"https://medstat.dk/da/download/file/MjAyMV9hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2022\" = \"https://medstat.dk/da/download/file/MjAyMl9hdGNfY29kZV9kYXRhLnR4dA==\",\n  \"2023\" = \"https://medstat.dk/da/download/file/MjAyM19hdGNfY29kZV9kYXRhLnR4dA==\"\n)\n\n# Vector of years you want to process\nyears &lt;- 2016:2023\n\n# Use map to iterate over years and URLs\ndf_allyears &lt;- map2(years, urls, process_atc_data)\n\n# Example: Access the result for a specific year\ndf_2023 &lt;- df_allyears[[1]] # The first year of the bunch, 2016\n\n# Combine all the datasets into one - and avoiding getting \"$\" in all the the variable names while doing it\ndf_allyears &lt;- bind_rows(df_allyears)\ndf_allyears &lt;- map(df_allyears, as_tibble)\ndf_allyears &lt;- bind_rows(df_allyears)\n\nThis is using even less space (if you look past the part about defining a list of URLs). I swear, I do not know why i need to repeat the bind rows function for it to work."
  },
  {
    "objectID": "posts/2024-09-20-medstat-function/index.html#making-the-plots",
    "href": "posts/2024-09-20-medstat-function/index.html#making-the-plots",
    "title": "Using R functions on medstat data",
    "section": "Making the plots",
    "text": "Making the plots\nNow that we have a combined dataset, df_allyears, with all the data to recreate the plots from the last post, we can try to make a plot. Let’s see if facet_wrap() makes a reasonable graph with all eight years.\n\n# Making the base graph to add unto\np_base &lt;- ggplot(df_allyears, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  labs(caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\")\n\n# Just seeing how it looks with ALL data summarised over the years\np_base + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\np_base + geom_bar(stat = \"identity\", position = \"fill\")\n\n\n\n\n\n\n\n# Testing facet_wrap on the total turnover\np_base +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\np_base +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\nNow, that is a LOT of information squeezed down on little space. So lets subset it and do the facetwrap() on four years at a time.\n\n# Subset the data\ndf_early &lt;- df_allyears |&gt;\n  filter(year &lt; 2020)\ndf_late &lt;- df_allyears |&gt;\n  filter(year &gt;= 2020)\n\n# Now for the graphs\n  # New bases\np_base_early &lt;- ggplot(df_early, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  labs(caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\")\np_base_late &lt;- ggplot(df_late, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  labs(caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\")\n\n  # New graphs\n    # Early\np_base_early +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\np_base_early +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n    # Late\np_base_late +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\np_base_late +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\nOk, but we can do this better, making it more easily comparable by having the same y-axis on both the early and late dataset.\n\np_base_early &lt;- ggplot(df_early, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  labs(caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\")\np_base_late &lt;- ggplot(df_late, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  labs(caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\")\n\n  # New graphs\n    # Early\np_base_early +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~year) + ylim(0,7500)\n\n\n\n\n\n\n\np_base_early +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n    # Late\np_base_late +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~year) + ylim(0,7500)\n\n\n\n\n\n\n\np_base_late +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  facet_wrap(~year) +\n  labs(y = \"\", x = \"\")\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saving p_base_late as thumbnail"
  },
  {
    "objectID": "posts/2024-09-20-medstat-function/index.html#commenting-on-the-output",
    "href": "posts/2024-09-20-medstat-function/index.html#commenting-on-the-output",
    "title": "Using R functions on medstat data",
    "section": "Commenting on the output",
    "text": "Commenting on the output\nSo, the graphs overwhelmingly show a tremendous increase in the spending on GLP1.\nWhere insulin in the early period of 2016-2019 rivaled or was higher than GLP1, in the late period, GLP1 completely overshadows all other drugs within the chosen classes.\nBetween 2022 and 2023 there seem to be a doubling of the turnover. Turnover typically represents the overall revenue generated in the pharmacy sector.\nThis can mean both increased spending, and increased prices. We will look at that in the next post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "A short intro to your site. Use the navbar to reach the blog and about pages."
  },
  {
    "objectID": "posts/2024-09-19-medstatexplore/index.html",
    "href": "posts/2024-09-19-medstatexplore/index.html",
    "title": "Making sense of Medstat.dk data",
    "section": "",
    "text": "In this post, I’ll showcase how to work with publicly available drug prescription data from medstat.dk. This is a great way to practice data manipulation and analysis, using real-world data.\n\n\nYou can download all the datasets from medstat.dk’s download section. If you want to download multiple files and not have to work with the URLs, I recommend using the downloadthemall extension to batch download the data for easier local manipulation.\nSince I’m using GitHub to store my work, I’ll download the data directly from Medstat. GitHub has a 2GB storage limit per repository, which means large public datasets can quickly exceed this. Therefore, it’s important to manage the size of the files efficiently.\n\n\n\nGood old trusty tidyverse has a lot of what I need, specifically the readr package, “secretly” loaded the library in an invisible chunk below, with #| include: FALSE, but also #| warning: FALSE because it wants to warn me about conflicts with the tidyverse package.\n#| include: FALSE\n#| warning: FALSE\nlibrary(tidyverse)\n\n\n\nYou can use tools like the MarkDownload extension to find the download links from the resulting markdown file it hands you.\nNow lets load an example dataset.\n\n# Define the URL for the dataset ()\nurl_atc_2023 &lt;- \"https://medstat.dk/da/download/file/MjAyM19hdGNfY29kZV9kYXRhLnR4dA==\"\n\n# Read the dataset directly from the URL\ndf_atc_2023 &lt;- read_delim(url_atc_2023, delim = \";\", show_col_types = FALSE)  # Assuming the file is tab-delimited\n\n# Print the first few rows of the dataset\nhead(df_atc_2023)\n\n# A tibble: 6 × 14\n  A...1 `2023` `0...3` `0...4` A...5 A...6  ...7  ...8 `5009988` `2661589` ...11\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 A       2023       0       0 2     00-17 25116  44.6     32451     21308    NA\n2 A       2023       0       0 1     00-17 21962  37.0     40295     29871    NA\n3 A       2023       0       0 0     00-17 47080  40.7     72746     51179    NA\n4 A       2023       0       0 2     18-24 23313  92.3     51797     21707    NA\n5 A       2023       0       0 1     18-24 13137  50.1     29808     15882    NA\n6 A       2023       0       0 0     18-24 36450  70.8     81606     37590    NA\n# ℹ 3 more variables: ...12 &lt;dbl&gt;, `94` &lt;chr&gt;, ...14 &lt;lgl&gt;\n\n\nOk, this data is without headers. A way to save space probably.\n\n\n\nThankfully there is a documentation file the website: Downloadbeskrivelse medstat. This file explains the strucutre of the datasets and for the data that we look at, YYYY_atc_code_data.txt, the following variables apply (translated to English from the original Danish):\n\natc: Anatomical Therapeutic Chemical code\nyear: Year of data\nsector: Healthcare sector\nregion: Geographic region\nsex: Gender\nagegroup: Age category\ncount_persons: Number of individuals\ncount_persons_per1kpop: Number of individuals per 1,000 population\nturnover: Sales turnover\nreimbursement: Reimbursements provided\nsold_amount: Amount of drugs sold\nsold_amount_1kpop_day: Amount sold per 1,000 population per day\npersonreferabledata_perc: Percentage of data referable to individuals\n\nSo we can use colnames() to transfer these column names to our dataset.\n\n# attach colnames\ncolnames(df_atc_2023) &lt;- c(\"atc\",\"year\",\"sector\",\"region\",\"sex\",\"agegroup\",\"count_persons\",\"count_persons_per1kpop\",\"turnover\",\"reimbursement\", \"sold_amount\", \"sold_amount_1kpop_day\", \"personreferabledata_perc\")\n\n# View data\nhead(df_atc_2023)\n\n# A tibble: 6 × 14\n  atc    year sector region sex   agegroup count_persons count_persons_per1kpop\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;                  &lt;dbl&gt;\n1 A      2023      0      0 2     00-17            25116                   44.6\n2 A      2023      0      0 1     00-17            21962                   37.0\n3 A      2023      0      0 0     00-17            47080                   40.7\n4 A      2023      0      0 2     18-24            23313                   92.3\n5 A      2023      0      0 1     18-24            13137                   50.1\n6 A      2023      0      0 0     18-24            36450                   70.8\n# ℹ 6 more variables: turnover &lt;dbl&gt;, reimbursement &lt;dbl&gt;, sold_amount &lt;dbl&gt;,\n#   sold_amount_1kpop_day &lt;dbl&gt;, personreferabledata_perc &lt;chr&gt;, `` &lt;lgl&gt;\n\nglimpse(df_atc_2023)\n\nRows: 1,806,544\nColumns: 14\n$ atc                      &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …\n$ year                     &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 202…\n$ sector                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ region                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sex                      &lt;chr&gt; \"2\", \"1\", \"0\", \"2\", \"1\", \"0\", \"2\", \"1\", \"0\", …\n$ agegroup                 &lt;chr&gt; \"00-17\", \"00-17\", \"00-17\", \"18-24\", \"18-24\", …\n$ count_persons            &lt;dbl&gt; 25116, 21962, 47080, 23313, 13137, 36450, 124…\n$ count_persons_per1kpop   &lt;dbl&gt; 44.61, 37.04, 40.73, 92.26, 50.10, 70.79, 169…\n$ turnover                 &lt;dbl&gt; 32451, 40295, 72746, 51797, 29808, 81606, 419…\n$ reimbursement            &lt;dbl&gt; 21308, 29871, 51179, 21707, 15882, 37590, 104…\n$ sold_amount              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ sold_amount_1kpop_day    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ personreferabledata_perc &lt;chr&gt; \"94\", \"94\", \"94\", \"94\", \"94\", \"94\", \"94\", \"94…\n$ NA                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nMissing name for one column that doesnt fit with the “Downloadbeskrivelse medstat”. What is it?\n\n# call it something\ncolnames(df_atc_2023)[is.na(colnames(df_atc_2023))] &lt;- \"missing_name\"\n\n# table of content\ntable(df_atc_2023$missing_name)\n\n&lt; table of extent 0 &gt;\n\n\nThis column is empty. Maybe just an artifact from the delim file. Lets assume we can safely remove it.\n\ndf_atc_2023 &lt;- df_atc_2023 |&gt;  select(-missing_name)\n\n\n\n\nTime to figure out how this data is structured, and organise the character variables into factors. Factors, as you may know, are a data structure in R, which is akin to a categorisation, ordered or unordered. Since we have age, we probably have at least some age categories that are ordered in intervals. Lets find out what the different variables likely to be factors are.\n\n# Finding all unique values even though i could just consult \"Downloadbeskrivelse medstat\"\nunique_values_all_columns &lt;- unique(unlist(sapply(df_atc_2023, function(x) if(is.character(x)) unique(x))))\nprint(unique_values_all_columns)\n\nWoops. I have hidden the massive output of this via #| results: 'hide'. I forgot that ATC has QUITE a lot of unique values. Lets make a data frame without the atc value.\n\n# Remove atc\ndf_atc_2023_mini &lt;- df_atc_2023 |&gt;  select(-atc)\n\n# Go through all variables that could be categories (often stored as characters by default) and display unique values\nunique(unlist(sapply(df_atc_2023_mini, function(x) if(is.character(x)) unique(x))))\n\n  [1] \"2\"     \"1\"     \"0\"     \"A\"     \"00-17\" \"18-24\" \"25-44\" \"45-64\" \"65-79\"\n [10] \"80+\"   \"000\"   \"001\"   \"002\"   \"003\"   \"004\"   \"005\"   \"006\"   \"007\"  \n [19] \"008\"   \"009\"   \"010\"   \"011\"   \"012\"   \"013\"   \"014\"   \"015\"   \"016\"  \n [28] \"017\"   \"018\"   \"019\"   \"020\"   \"021\"   \"022\"   \"023\"   \"024\"   \"025\"  \n [37] \"026\"   \"027\"   \"028\"   \"029\"   \"030\"   \"031\"   \"032\"   \"033\"   \"034\"  \n [46] \"035\"   \"036\"   \"037\"   \"038\"   \"039\"   \"040\"   \"041\"   \"042\"   \"043\"  \n [55] \"044\"   \"045\"   \"046\"   \"047\"   \"048\"   \"049\"   \"050\"   \"051\"   \"052\"  \n [64] \"053\"   \"054\"   \"055\"   \"056\"   \"057\"   \"058\"   \"059\"   \"060\"   \"061\"  \n [73] \"062\"   \"063\"   \"064\"   \"065\"   \"066\"   \"067\"   \"068\"   \"069\"   \"070\"  \n [82] \"071\"   \"072\"   \"073\"   \"074\"   \"075\"   \"076\"   \"077\"   \"078\"   \"079\"  \n [91] \"080\"   \"081\"   \"082\"   \"083\"   \"084\"   \"085\"   \"086\"   \"087\"   \"088\"  \n[100] \"089\"   \"090\"   \"091\"   \"092\"   \"093\"   \"094\"   \"95+\"   \"T\"     \"94\"   \n[109] NA      \"92\"    \"95\"    \"&gt;99\"   \"86\"    \"15\"    \"18\"    \"60\"    \"96\"   \n[118] \"19\"    \"98\"    \"97\"    \"91\"    \"11\"    \"20\"    \"10\"    \"83\"    \"65\"   \n[127] \"99\"    \"8\"     \"55\"    \"46\"    \"40\"    \"36\"    \"44\"    \"51\"    \"45\"   \n[136] \"54\"    \"50\"    \"48\"    \"89\"    \"81\"    \"34\"    \"57\"    \"53\"    \"87\"   \n[145] \"39\"    \"41\"    \"9\"     \"64\"    \"56\"    \"93\"    \"42\"    \"70\"    \"75\"   \n[154] \"61\"    \"84\"    \"26\"    \"43\"    \"59\"    \"90\"    \"14\"    \"28\"    \"30\"   \n[163] \"52\"    \"32\"    \"3\"     \"27\"    \"69\"    \"16\"    \"71\"    \"6\"     \"25\"   \n[172] \"67\"    \"78\"    \"77\"    \"23\"    \"5\"     \"22\"    \"13\"    \"72\"    \"17\"   \n[181] \"21\"    \"&lt;1\"    \"66\"    \"38\"    \"62\"    \"73\"    \"88\"    \"76\"    \"12\"   \n[190] \"4\"     \"58\"    \"33\"    \"82\"    \"29\"    \"63\"    \"31\"    \"80\"    \"79\"   \n[199] \"74\"    \"85\"    \"49\"    \"68\"    \"35\"   \n\n\nOr, in a little more legible fashion, use some apply functions to first find the ones that are characters with sapply(), and then lapply() to create a list of all unique values of the character vars.\n\n# find character variables\nchar_columns &lt;- df_atc_2023_mini[sapply(df_atc_2023_mini, is.character)]\n\n# apply the unique() function to each character variable\nunique_char_values &lt;- lapply(char_columns, unique)\nunique_char_values\n\n$sex\n[1] \"2\" \"1\" \"0\" \"A\"\n\n$agegroup\n  [1] \"00-17\" \"18-24\" \"25-44\" \"45-64\" \"65-79\" \"80+\"   \"000\"   \"001\"   \"002\"  \n [10] \"003\"   \"004\"   \"005\"   \"006\"   \"007\"   \"008\"   \"009\"   \"010\"   \"011\"  \n [19] \"012\"   \"013\"   \"014\"   \"015\"   \"016\"   \"017\"   \"018\"   \"019\"   \"020\"  \n [28] \"021\"   \"022\"   \"023\"   \"024\"   \"025\"   \"026\"   \"027\"   \"028\"   \"029\"  \n [37] \"030\"   \"031\"   \"032\"   \"033\"   \"034\"   \"035\"   \"036\"   \"037\"   \"038\"  \n [46] \"039\"   \"040\"   \"041\"   \"042\"   \"043\"   \"044\"   \"045\"   \"046\"   \"047\"  \n [55] \"048\"   \"049\"   \"050\"   \"051\"   \"052\"   \"053\"   \"054\"   \"055\"   \"056\"  \n [64] \"057\"   \"058\"   \"059\"   \"060\"   \"061\"   \"062\"   \"063\"   \"064\"   \"065\"  \n [73] \"066\"   \"067\"   \"068\"   \"069\"   \"070\"   \"071\"   \"072\"   \"073\"   \"074\"  \n [82] \"075\"   \"076\"   \"077\"   \"078\"   \"079\"   \"080\"   \"081\"   \"082\"   \"083\"  \n [91] \"084\"   \"085\"   \"086\"   \"087\"   \"088\"   \"089\"   \"090\"   \"091\"   \"092\"  \n[100] \"093\"   \"094\"   \"95+\"   \"T\"     \"A\"    \n\n$personreferabledata_perc\n [1] \"94\"  NA    \"92\"  \"95\"  \"&gt;99\" \"86\"  \"15\"  \"18\"  \"2\"   \"60\"  \"96\"  \"19\" \n[13] \"98\"  \"97\"  \"91\"  \"11\"  \"20\"  \"10\"  \"83\"  \"65\"  \"99\"  \"8\"   \"55\"  \"46\" \n[25] \"40\"  \"36\"  \"44\"  \"51\"  \"45\"  \"54\"  \"50\"  \"48\"  \"89\"  \"81\"  \"34\"  \"57\" \n[37] \"53\"  \"87\"  \"39\"  \"41\"  \"9\"   \"64\"  \"56\"  \"93\"  \"42\"  \"70\"  \"75\"  \"61\" \n[49] \"84\"  \"26\"  \"43\"  \"59\"  \"90\"  \"14\"  \"1\"   \"28\"  \"30\"  \"52\"  \"32\"  \"3\"  \n[61] \"27\"  \"69\"  \"16\"  \"71\"  \"6\"   \"25\"  \"67\"  \"78\"  \"77\"  \"23\"  \"5\"   \"22\" \n[73] \"13\"  \"72\"  \"17\"  \"21\"  \"&lt;1\"  \"66\"  \"38\"  \"62\"  \"73\"  \"88\"  \"76\"  \"12\" \n[85] \"4\"   \"58\"  \"33\"  \"82\"  \"29\"  \"63\"  \"31\"  \"80\"  \"79\"  \"74\"  \"85\"  \"49\" \n[97] \"68\"  \"35\" \n\n\nMore manageable amount of unique values. Although it seems there are multiple age categories within the agegroup variable. I will remove df_atc_2023_mini and unique_char_values as they were created for the purpose of creating an overview of the unique values.\n\nrm(df_atc_2023_mini, unique_char_values)\n\nLets focus on the unique age groups. The data for age groups must be duplicated for the ranges 000 to 017, and the age category “00-17”. This would not be included if the goal was to save space, as a single variable containing the age from 000 to 95+ could be grouped into intervals with a mutate() or cut() function.\nFrom here, I will use dplyr, as it is easier to read the operations performed on the data.\nI will explore the following questions:\n\nAre the age range equal to the age intervals?\nWhat do the sex categories represent?\nExactly to what degree is the dataset full of duplicated data?\n\n\n\nTo figure out whether the age range versus the prespecified intervals have the same data, we can summarise a variable, here turnover, for the two groups, and see if they are the same.\nFirst, lets summarize turnover for the age interval “00-17”.\n\n# Make a datset by filtering on 00-17\ndf_atc_2023_interval &lt;- df_atc_2023 |&gt;\n  filter(agegroup == \"00-17\")\n\n# Summarise it\nturnover_sum_interval &lt;- df_atc_2023_interval |&gt;\n  summarize(turnover = sum(turnover, na.rm = TRUE))\n\n15.373.244 in 1000 DKK, so 15.373.244.000, or more than 15 billion DKK in 2023. For one age interval. Woav.\nNow, lets try to do it for the age range.\n\n# make vector to include in the filter\nnumbers &lt;- sprintf(\"%03d\", 0:17)\n\n# Now filter in the same way as above, but for 000 to 017\ndf_atc_2023_range &lt;- df_atc_2023 |&gt;\n  filter(agegroup %in% numbers)\n\n# Summarise\nturnover_sum_range  &lt;- df_atc_2023_range |&gt; mutate(\n    agegroup_combined =\n      ifelse(\n        agegroup %in% numbers,\n        \"000-017\",\n        agegroup)) |&gt;\n  ungroup() |&gt;\n  summarize(turnover = sum(turnover, na.rm = TRUE)) #, .groups = \"drop\")\nturnover_sum_range\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 14857936\n\n\nNow lets compare.\n\n# Summary\nprint(turnover_sum_interval)\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 15373247\n\nprint(turnover_sum_range)\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 14857936\n\n# There is a difference.. How much?\n(print(turnover_sum_interval)-print(turnover_sum_range))\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 15373247\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 14857936\n\n\n  turnover\n1   515311\n\n\nOkay, these two should be equal. Where did those 515.309.000 go? Thats a difference of 3.47%. The number calculated with inline R code - with “{r} round(((turnover_sum_interval-turnover_sum_range)/turnover_sum_range)*100,2)” surrounded by backticks - because that is nice to have in case you want to have your numbers in the text change accordingly with any change in the data you have done. Anyway.. what went wrong?\nHow very strange. How do I figure out why there is a difference?\nAre there the same amount of people in each category?\n\n# Count amount of people in each category\ncount_persons_range &lt;- df_atc_2023 |&gt;\n  filter(agegroup %in% numbers) |&gt;\n  summarise(total = sum(count_persons))\ncount_persons_int &lt;- df_atc_2023 |&gt;\n  filter(agegroup == \"00-17\") |&gt;\n  summarise(total = sum(count_persons))\n\n# Difference\ncount_persons_int-count_persons_range\n\n   total\n1 131104\n\n\nOk. 131.087 people are missing from the “range” dataset. Thats a difference of 0.47% I might have done something wrong. Or there is a mistake in the data.\n\n\n\n\nLet’s drop the other questions I wanted to ask above, atleast for now. A patient Data Scientist with a lot of time on their hands should probably find the cause of the error by doing some of the following:\n\nCheck the discrepancies across multiple variables\nRecheck the “Downloadbeskrivelse” to see if the metadata for the data has a description of why there would be a descrepancy\n\nBecause if it was an error, this 3.47% difference in Turnover and 0.47% difference in amount of people in the category is something I will let sit.\nMy main hypothesis which just popped into my head just now is that since the data is missing from the count_persons_range dataset, it has something to do with removing observations from the original dataset that contains too much person-referable [personhenførbart] data.\nNow, with that in mind, I, an aspiring Data Scientist with a goal of making a blog post and not taking forever, will just use the data with age intervals, as according to my hypothesis, that is the most “complete” data.\nWith that dataset, I will provide a brief overview of the drug classes with the highest turnover, by age group.\n\n\nLet’s start with the df_atc_2023 dataset and:\n\nFilter just the age-interval group, starting with “00-17”\nFilter it down to just the following drug classes: GLP1, SGLT2, DPP4, BIGUANIDES (Metformin in this case), and INSULIN. ATC’s are A10BJ, A10BK, A10BH, A10BA, and A10A.X, respectively\nSummarize the drug classes over the age intervals, with a nice plot\n\n\n# Filter\ndf_atc_2023_filtered &lt;- df_atc_2023 |&gt;\n  filter(\n    str_detect(agegroup, \"-\") &\n    str_starts(atc, \"A10\")\n    )\nhead(df_atc_2023_filtered)\n\n# A tibble: 6 × 13\n  atc    year sector region sex   agegroup count_persons count_persons_per1kpop\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;                  &lt;dbl&gt;\n1 A10    2023      0      0 2     00-17             1780                   3.16\n2 A10    2023      0      0 1     00-17             1832                   3.09\n3 A10    2023      0      0 0     00-17             3610                   3.13\n4 A10    2023      0      0 2     18-24             4716                  18.7 \n5 A10    2023      0      0 1     18-24             2777                  10.6 \n6 A10    2023      0      0 0     18-24             7495                  14.6 \n# ℹ 5 more variables: turnover &lt;dbl&gt;, reimbursement &lt;dbl&gt;, sold_amount &lt;dbl&gt;,\n#   sold_amount_1kpop_day &lt;dbl&gt;, personreferabledata_perc &lt;chr&gt;\n\n\nOk - another observation that popped up: here I find evidence to support my hypothesis of data being deleted when it is too person-referable. Looking at the lowest ATC level (e.g. A10BJK01), there is “0” in count_persons, for at lot of the input. And the lowest number is “5”.\nThis replacement of too low values with “missing” or “0” is frequently done to mask the fact that perhaps there were just 2 people receiving that drug in this year, which makes the probability that someone can identify who it is that much higher. So it is masking the data to protect the individuals who provided the data.\nMoving on, and taking that into account, I choose to filter on the drug class level, as it repeats anyway. E.g. for observations with atc = A10BJ, the data is summarised over the age groups, and within each A10BJ01..02..0n, I will find all that is contained in A10BJ.\n\n\n\n\nNow, I have what I need to create the plots of turnover. I choose to make stacked bar charts, because I am used to making them, but there are probably better ways of representing the data.\nFirst, I prepare the data, by filtering out all the drug classes I do not wish to focus on, and making the y-axis more readable by dividing it with 1000, thus making it turnover in 1.000.000 of DKK\n\n# Create vector of drug classes\nDCs &lt;- c(\"A10BJ\", \"A10BK\", \"A10BH\", \"A10BA\", \"A10BB\", \"A10BG\", \"A10BX\", \"A10A\")\n\n# Filter\ndf_atc_2023_filtered &lt;- df_atc_2023_filtered |&gt;\n  filter(atc %in% DCs) |&gt;\n  mutate(\n    DC = case_when( # New drug class variable\n      atc == \"A10BJ\" ~ \"GLP1\", # GLP1\n      atc == \"A10BK\" ~ \"SGLT2\", # SGLT2\n      atc == \"A10BH\" ~ \"DPP4\", # DPP4\n      atc == \"A10BA\" ~ \"Metformin\", # MET\n      atc == \"A10BB\" ~ \"SU\", # SU\n      atc == \"A10BG\" ~ \"TZD\", # Thiazolidinediones\n      atc == \"A10BX\" ~ \"Others\", # Others\n      atc == \"A10A\"  ~ \"INSULIN\",  # INSULIN\n      TRUE ~ NA_character_  # Default to NA for all other values\n    ),\n    turnover1000k = turnover/1000 # 1000k because its already in 1000's.\n    )\n\nNow its time to make the plot.\n\n# Plot - stacked bar chart\n  # total turnover\np1 &lt;- ggplot(df_atc_2023_filtered, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Total turnover by drug class\",\n    caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\"\n    )\n  # propotional turnover for each group\np2 &lt;- ggplot(df_atc_2023_filtered, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(\n    y = \"\",\n    title = \"Proportional turnover by drug class\",\n    caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\"\n    )\n\np1\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # p1 saved as thumbnail\n\np2\n\n\n\n\n\n\n\n\nWell then.. The turnover for GLP1 in this country is quite impressive, with the 45 to 64 year olds taking by far the biggest piece of the pie. Thats about 6.000.000.000 DKK in turnover for that age group in 2023 alone.\nIn the next blog post I aim to show how to redo all that I did, as a function.\nAnd hopefully it will also be a bit more organised."
  },
  {
    "objectID": "posts/2024-09-19-medstatexplore/index.html#exploring-medstat-data-drug-prescriptions-in-denmark",
    "href": "posts/2024-09-19-medstatexplore/index.html#exploring-medstat-data-drug-prescriptions-in-denmark",
    "title": "Making sense of Medstat.dk data",
    "section": "",
    "text": "In this post, I’ll showcase how to work with publicly available drug prescription data from medstat.dk. This is a great way to practice data manipulation and analysis, using real-world data.\n\n\nYou can download all the datasets from medstat.dk’s download section. If you want to download multiple files and not have to work with the URLs, I recommend using the downloadthemall extension to batch download the data for easier local manipulation.\nSince I’m using GitHub to store my work, I’ll download the data directly from Medstat. GitHub has a 2GB storage limit per repository, which means large public datasets can quickly exceed this. Therefore, it’s important to manage the size of the files efficiently.\n\n\n\nGood old trusty tidyverse has a lot of what I need, specifically the readr package, “secretly” loaded the library in an invisible chunk below, with #| include: FALSE, but also #| warning: FALSE because it wants to warn me about conflicts with the tidyverse package.\n#| include: FALSE\n#| warning: FALSE\nlibrary(tidyverse)\n\n\n\nYou can use tools like the MarkDownload extension to find the download links from the resulting markdown file it hands you.\nNow lets load an example dataset.\n\n# Define the URL for the dataset ()\nurl_atc_2023 &lt;- \"https://medstat.dk/da/download/file/MjAyM19hdGNfY29kZV9kYXRhLnR4dA==\"\n\n# Read the dataset directly from the URL\ndf_atc_2023 &lt;- read_delim(url_atc_2023, delim = \";\", show_col_types = FALSE)  # Assuming the file is tab-delimited\n\n# Print the first few rows of the dataset\nhead(df_atc_2023)\n\n# A tibble: 6 × 14\n  A...1 `2023` `0...3` `0...4` A...5 A...6  ...7  ...8 `5009988` `2661589` ...11\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 A       2023       0       0 2     00-17 25116  44.6     32451     21308    NA\n2 A       2023       0       0 1     00-17 21962  37.0     40295     29871    NA\n3 A       2023       0       0 0     00-17 47080  40.7     72746     51179    NA\n4 A       2023       0       0 2     18-24 23313  92.3     51797     21707    NA\n5 A       2023       0       0 1     18-24 13137  50.1     29808     15882    NA\n6 A       2023       0       0 0     18-24 36450  70.8     81606     37590    NA\n# ℹ 3 more variables: ...12 &lt;dbl&gt;, `94` &lt;chr&gt;, ...14 &lt;lgl&gt;\n\n\nOk, this data is without headers. A way to save space probably.\n\n\n\nThankfully there is a documentation file the website: Downloadbeskrivelse medstat. This file explains the strucutre of the datasets and for the data that we look at, YYYY_atc_code_data.txt, the following variables apply (translated to English from the original Danish):\n\natc: Anatomical Therapeutic Chemical code\nyear: Year of data\nsector: Healthcare sector\nregion: Geographic region\nsex: Gender\nagegroup: Age category\ncount_persons: Number of individuals\ncount_persons_per1kpop: Number of individuals per 1,000 population\nturnover: Sales turnover\nreimbursement: Reimbursements provided\nsold_amount: Amount of drugs sold\nsold_amount_1kpop_day: Amount sold per 1,000 population per day\npersonreferabledata_perc: Percentage of data referable to individuals\n\nSo we can use colnames() to transfer these column names to our dataset.\n\n# attach colnames\ncolnames(df_atc_2023) &lt;- c(\"atc\",\"year\",\"sector\",\"region\",\"sex\",\"agegroup\",\"count_persons\",\"count_persons_per1kpop\",\"turnover\",\"reimbursement\", \"sold_amount\", \"sold_amount_1kpop_day\", \"personreferabledata_perc\")\n\n# View data\nhead(df_atc_2023)\n\n# A tibble: 6 × 14\n  atc    year sector region sex   agegroup count_persons count_persons_per1kpop\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;                  &lt;dbl&gt;\n1 A      2023      0      0 2     00-17            25116                   44.6\n2 A      2023      0      0 1     00-17            21962                   37.0\n3 A      2023      0      0 0     00-17            47080                   40.7\n4 A      2023      0      0 2     18-24            23313                   92.3\n5 A      2023      0      0 1     18-24            13137                   50.1\n6 A      2023      0      0 0     18-24            36450                   70.8\n# ℹ 6 more variables: turnover &lt;dbl&gt;, reimbursement &lt;dbl&gt;, sold_amount &lt;dbl&gt;,\n#   sold_amount_1kpop_day &lt;dbl&gt;, personreferabledata_perc &lt;chr&gt;, `` &lt;lgl&gt;\n\nglimpse(df_atc_2023)\n\nRows: 1,806,544\nColumns: 14\n$ atc                      &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …\n$ year                     &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 202…\n$ sector                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ region                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sex                      &lt;chr&gt; \"2\", \"1\", \"0\", \"2\", \"1\", \"0\", \"2\", \"1\", \"0\", …\n$ agegroup                 &lt;chr&gt; \"00-17\", \"00-17\", \"00-17\", \"18-24\", \"18-24\", …\n$ count_persons            &lt;dbl&gt; 25116, 21962, 47080, 23313, 13137, 36450, 124…\n$ count_persons_per1kpop   &lt;dbl&gt; 44.61, 37.04, 40.73, 92.26, 50.10, 70.79, 169…\n$ turnover                 &lt;dbl&gt; 32451, 40295, 72746, 51797, 29808, 81606, 419…\n$ reimbursement            &lt;dbl&gt; 21308, 29871, 51179, 21707, 15882, 37590, 104…\n$ sold_amount              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ sold_amount_1kpop_day    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ personreferabledata_perc &lt;chr&gt; \"94\", \"94\", \"94\", \"94\", \"94\", \"94\", \"94\", \"94…\n$ NA                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nMissing name for one column that doesnt fit with the “Downloadbeskrivelse medstat”. What is it?\n\n# call it something\ncolnames(df_atc_2023)[is.na(colnames(df_atc_2023))] &lt;- \"missing_name\"\n\n# table of content\ntable(df_atc_2023$missing_name)\n\n&lt; table of extent 0 &gt;\n\n\nThis column is empty. Maybe just an artifact from the delim file. Lets assume we can safely remove it.\n\ndf_atc_2023 &lt;- df_atc_2023 |&gt;  select(-missing_name)\n\n\n\n\nTime to figure out how this data is structured, and organise the character variables into factors. Factors, as you may know, are a data structure in R, which is akin to a categorisation, ordered or unordered. Since we have age, we probably have at least some age categories that are ordered in intervals. Lets find out what the different variables likely to be factors are.\n\n# Finding all unique values even though i could just consult \"Downloadbeskrivelse medstat\"\nunique_values_all_columns &lt;- unique(unlist(sapply(df_atc_2023, function(x) if(is.character(x)) unique(x))))\nprint(unique_values_all_columns)\n\nWoops. I have hidden the massive output of this via #| results: 'hide'. I forgot that ATC has QUITE a lot of unique values. Lets make a data frame without the atc value.\n\n# Remove atc\ndf_atc_2023_mini &lt;- df_atc_2023 |&gt;  select(-atc)\n\n# Go through all variables that could be categories (often stored as characters by default) and display unique values\nunique(unlist(sapply(df_atc_2023_mini, function(x) if(is.character(x)) unique(x))))\n\n  [1] \"2\"     \"1\"     \"0\"     \"A\"     \"00-17\" \"18-24\" \"25-44\" \"45-64\" \"65-79\"\n [10] \"80+\"   \"000\"   \"001\"   \"002\"   \"003\"   \"004\"   \"005\"   \"006\"   \"007\"  \n [19] \"008\"   \"009\"   \"010\"   \"011\"   \"012\"   \"013\"   \"014\"   \"015\"   \"016\"  \n [28] \"017\"   \"018\"   \"019\"   \"020\"   \"021\"   \"022\"   \"023\"   \"024\"   \"025\"  \n [37] \"026\"   \"027\"   \"028\"   \"029\"   \"030\"   \"031\"   \"032\"   \"033\"   \"034\"  \n [46] \"035\"   \"036\"   \"037\"   \"038\"   \"039\"   \"040\"   \"041\"   \"042\"   \"043\"  \n [55] \"044\"   \"045\"   \"046\"   \"047\"   \"048\"   \"049\"   \"050\"   \"051\"   \"052\"  \n [64] \"053\"   \"054\"   \"055\"   \"056\"   \"057\"   \"058\"   \"059\"   \"060\"   \"061\"  \n [73] \"062\"   \"063\"   \"064\"   \"065\"   \"066\"   \"067\"   \"068\"   \"069\"   \"070\"  \n [82] \"071\"   \"072\"   \"073\"   \"074\"   \"075\"   \"076\"   \"077\"   \"078\"   \"079\"  \n [91] \"080\"   \"081\"   \"082\"   \"083\"   \"084\"   \"085\"   \"086\"   \"087\"   \"088\"  \n[100] \"089\"   \"090\"   \"091\"   \"092\"   \"093\"   \"094\"   \"95+\"   \"T\"     \"94\"   \n[109] NA      \"92\"    \"95\"    \"&gt;99\"   \"86\"    \"15\"    \"18\"    \"60\"    \"96\"   \n[118] \"19\"    \"98\"    \"97\"    \"91\"    \"11\"    \"20\"    \"10\"    \"83\"    \"65\"   \n[127] \"99\"    \"8\"     \"55\"    \"46\"    \"40\"    \"36\"    \"44\"    \"51\"    \"45\"   \n[136] \"54\"    \"50\"    \"48\"    \"89\"    \"81\"    \"34\"    \"57\"    \"53\"    \"87\"   \n[145] \"39\"    \"41\"    \"9\"     \"64\"    \"56\"    \"93\"    \"42\"    \"70\"    \"75\"   \n[154] \"61\"    \"84\"    \"26\"    \"43\"    \"59\"    \"90\"    \"14\"    \"28\"    \"30\"   \n[163] \"52\"    \"32\"    \"3\"     \"27\"    \"69\"    \"16\"    \"71\"    \"6\"     \"25\"   \n[172] \"67\"    \"78\"    \"77\"    \"23\"    \"5\"     \"22\"    \"13\"    \"72\"    \"17\"   \n[181] \"21\"    \"&lt;1\"    \"66\"    \"38\"    \"62\"    \"73\"    \"88\"    \"76\"    \"12\"   \n[190] \"4\"     \"58\"    \"33\"    \"82\"    \"29\"    \"63\"    \"31\"    \"80\"    \"79\"   \n[199] \"74\"    \"85\"    \"49\"    \"68\"    \"35\"   \n\n\nOr, in a little more legible fashion, use some apply functions to first find the ones that are characters with sapply(), and then lapply() to create a list of all unique values of the character vars.\n\n# find character variables\nchar_columns &lt;- df_atc_2023_mini[sapply(df_atc_2023_mini, is.character)]\n\n# apply the unique() function to each character variable\nunique_char_values &lt;- lapply(char_columns, unique)\nunique_char_values\n\n$sex\n[1] \"2\" \"1\" \"0\" \"A\"\n\n$agegroup\n  [1] \"00-17\" \"18-24\" \"25-44\" \"45-64\" \"65-79\" \"80+\"   \"000\"   \"001\"   \"002\"  \n [10] \"003\"   \"004\"   \"005\"   \"006\"   \"007\"   \"008\"   \"009\"   \"010\"   \"011\"  \n [19] \"012\"   \"013\"   \"014\"   \"015\"   \"016\"   \"017\"   \"018\"   \"019\"   \"020\"  \n [28] \"021\"   \"022\"   \"023\"   \"024\"   \"025\"   \"026\"   \"027\"   \"028\"   \"029\"  \n [37] \"030\"   \"031\"   \"032\"   \"033\"   \"034\"   \"035\"   \"036\"   \"037\"   \"038\"  \n [46] \"039\"   \"040\"   \"041\"   \"042\"   \"043\"   \"044\"   \"045\"   \"046\"   \"047\"  \n [55] \"048\"   \"049\"   \"050\"   \"051\"   \"052\"   \"053\"   \"054\"   \"055\"   \"056\"  \n [64] \"057\"   \"058\"   \"059\"   \"060\"   \"061\"   \"062\"   \"063\"   \"064\"   \"065\"  \n [73] \"066\"   \"067\"   \"068\"   \"069\"   \"070\"   \"071\"   \"072\"   \"073\"   \"074\"  \n [82] \"075\"   \"076\"   \"077\"   \"078\"   \"079\"   \"080\"   \"081\"   \"082\"   \"083\"  \n [91] \"084\"   \"085\"   \"086\"   \"087\"   \"088\"   \"089\"   \"090\"   \"091\"   \"092\"  \n[100] \"093\"   \"094\"   \"95+\"   \"T\"     \"A\"    \n\n$personreferabledata_perc\n [1] \"94\"  NA    \"92\"  \"95\"  \"&gt;99\" \"86\"  \"15\"  \"18\"  \"2\"   \"60\"  \"96\"  \"19\" \n[13] \"98\"  \"97\"  \"91\"  \"11\"  \"20\"  \"10\"  \"83\"  \"65\"  \"99\"  \"8\"   \"55\"  \"46\" \n[25] \"40\"  \"36\"  \"44\"  \"51\"  \"45\"  \"54\"  \"50\"  \"48\"  \"89\"  \"81\"  \"34\"  \"57\" \n[37] \"53\"  \"87\"  \"39\"  \"41\"  \"9\"   \"64\"  \"56\"  \"93\"  \"42\"  \"70\"  \"75\"  \"61\" \n[49] \"84\"  \"26\"  \"43\"  \"59\"  \"90\"  \"14\"  \"1\"   \"28\"  \"30\"  \"52\"  \"32\"  \"3\"  \n[61] \"27\"  \"69\"  \"16\"  \"71\"  \"6\"   \"25\"  \"67\"  \"78\"  \"77\"  \"23\"  \"5\"   \"22\" \n[73] \"13\"  \"72\"  \"17\"  \"21\"  \"&lt;1\"  \"66\"  \"38\"  \"62\"  \"73\"  \"88\"  \"76\"  \"12\" \n[85] \"4\"   \"58\"  \"33\"  \"82\"  \"29\"  \"63\"  \"31\"  \"80\"  \"79\"  \"74\"  \"85\"  \"49\" \n[97] \"68\"  \"35\" \n\n\nMore manageable amount of unique values. Although it seems there are multiple age categories within the agegroup variable. I will remove df_atc_2023_mini and unique_char_values as they were created for the purpose of creating an overview of the unique values.\n\nrm(df_atc_2023_mini, unique_char_values)\n\nLets focus on the unique age groups. The data for age groups must be duplicated for the ranges 000 to 017, and the age category “00-17”. This would not be included if the goal was to save space, as a single variable containing the age from 000 to 95+ could be grouped into intervals with a mutate() or cut() function.\nFrom here, I will use dplyr, as it is easier to read the operations performed on the data.\nI will explore the following questions:\n\nAre the age range equal to the age intervals?\nWhat do the sex categories represent?\nExactly to what degree is the dataset full of duplicated data?\n\n\n\nTo figure out whether the age range versus the prespecified intervals have the same data, we can summarise a variable, here turnover, for the two groups, and see if they are the same.\nFirst, lets summarize turnover for the age interval “00-17”.\n\n# Make a datset by filtering on 00-17\ndf_atc_2023_interval &lt;- df_atc_2023 |&gt;\n  filter(agegroup == \"00-17\")\n\n# Summarise it\nturnover_sum_interval &lt;- df_atc_2023_interval |&gt;\n  summarize(turnover = sum(turnover, na.rm = TRUE))\n\n15.373.244 in 1000 DKK, so 15.373.244.000, or more than 15 billion DKK in 2023. For one age interval. Woav.\nNow, lets try to do it for the age range.\n\n# make vector to include in the filter\nnumbers &lt;- sprintf(\"%03d\", 0:17)\n\n# Now filter in the same way as above, but for 000 to 017\ndf_atc_2023_range &lt;- df_atc_2023 |&gt;\n  filter(agegroup %in% numbers)\n\n# Summarise\nturnover_sum_range  &lt;- df_atc_2023_range |&gt; mutate(\n    agegroup_combined =\n      ifelse(\n        agegroup %in% numbers,\n        \"000-017\",\n        agegroup)) |&gt;\n  ungroup() |&gt;\n  summarize(turnover = sum(turnover, na.rm = TRUE)) #, .groups = \"drop\")\nturnover_sum_range\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 14857936\n\n\nNow lets compare.\n\n# Summary\nprint(turnover_sum_interval)\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 15373247\n\nprint(turnover_sum_range)\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 14857936\n\n# There is a difference.. How much?\n(print(turnover_sum_interval)-print(turnover_sum_range))\n\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 15373247\n# A tibble: 1 × 1\n  turnover\n     &lt;dbl&gt;\n1 14857936\n\n\n  turnover\n1   515311\n\n\nOkay, these two should be equal. Where did those 515.309.000 go? Thats a difference of 3.47%. The number calculated with inline R code - with “{r} round(((turnover_sum_interval-turnover_sum_range)/turnover_sum_range)*100,2)” surrounded by backticks - because that is nice to have in case you want to have your numbers in the text change accordingly with any change in the data you have done. Anyway.. what went wrong?\nHow very strange. How do I figure out why there is a difference?\nAre there the same amount of people in each category?\n\n# Count amount of people in each category\ncount_persons_range &lt;- df_atc_2023 |&gt;\n  filter(agegroup %in% numbers) |&gt;\n  summarise(total = sum(count_persons))\ncount_persons_int &lt;- df_atc_2023 |&gt;\n  filter(agegroup == \"00-17\") |&gt;\n  summarise(total = sum(count_persons))\n\n# Difference\ncount_persons_int-count_persons_range\n\n   total\n1 131104\n\n\nOk. 131.087 people are missing from the “range” dataset. Thats a difference of 0.47% I might have done something wrong. Or there is a mistake in the data.\n\n\n\n\nLet’s drop the other questions I wanted to ask above, atleast for now. A patient Data Scientist with a lot of time on their hands should probably find the cause of the error by doing some of the following:\n\nCheck the discrepancies across multiple variables\nRecheck the “Downloadbeskrivelse” to see if the metadata for the data has a description of why there would be a descrepancy\n\nBecause if it was an error, this 3.47% difference in Turnover and 0.47% difference in amount of people in the category is something I will let sit.\nMy main hypothesis which just popped into my head just now is that since the data is missing from the count_persons_range dataset, it has something to do with removing observations from the original dataset that contains too much person-referable [personhenførbart] data.\nNow, with that in mind, I, an aspiring Data Scientist with a goal of making a blog post and not taking forever, will just use the data with age intervals, as according to my hypothesis, that is the most “complete” data.\nWith that dataset, I will provide a brief overview of the drug classes with the highest turnover, by age group.\n\n\nLet’s start with the df_atc_2023 dataset and:\n\nFilter just the age-interval group, starting with “00-17”\nFilter it down to just the following drug classes: GLP1, SGLT2, DPP4, BIGUANIDES (Metformin in this case), and INSULIN. ATC’s are A10BJ, A10BK, A10BH, A10BA, and A10A.X, respectively\nSummarize the drug classes over the age intervals, with a nice plot\n\n\n# Filter\ndf_atc_2023_filtered &lt;- df_atc_2023 |&gt;\n  filter(\n    str_detect(agegroup, \"-\") &\n    str_starts(atc, \"A10\")\n    )\nhead(df_atc_2023_filtered)\n\n# A tibble: 6 × 13\n  atc    year sector region sex   agegroup count_persons count_persons_per1kpop\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;                  &lt;dbl&gt;\n1 A10    2023      0      0 2     00-17             1780                   3.16\n2 A10    2023      0      0 1     00-17             1832                   3.09\n3 A10    2023      0      0 0     00-17             3610                   3.13\n4 A10    2023      0      0 2     18-24             4716                  18.7 \n5 A10    2023      0      0 1     18-24             2777                  10.6 \n6 A10    2023      0      0 0     18-24             7495                  14.6 \n# ℹ 5 more variables: turnover &lt;dbl&gt;, reimbursement &lt;dbl&gt;, sold_amount &lt;dbl&gt;,\n#   sold_amount_1kpop_day &lt;dbl&gt;, personreferabledata_perc &lt;chr&gt;\n\n\nOk - another observation that popped up: here I find evidence to support my hypothesis of data being deleted when it is too person-referable. Looking at the lowest ATC level (e.g. A10BJK01), there is “0” in count_persons, for at lot of the input. And the lowest number is “5”.\nThis replacement of too low values with “missing” or “0” is frequently done to mask the fact that perhaps there were just 2 people receiving that drug in this year, which makes the probability that someone can identify who it is that much higher. So it is masking the data to protect the individuals who provided the data.\nMoving on, and taking that into account, I choose to filter on the drug class level, as it repeats anyway. E.g. for observations with atc = A10BJ, the data is summarised over the age groups, and within each A10BJ01..02..0n, I will find all that is contained in A10BJ.\n\n\n\n\nNow, I have what I need to create the plots of turnover. I choose to make stacked bar charts, because I am used to making them, but there are probably better ways of representing the data.\nFirst, I prepare the data, by filtering out all the drug classes I do not wish to focus on, and making the y-axis more readable by dividing it with 1000, thus making it turnover in 1.000.000 of DKK\n\n# Create vector of drug classes\nDCs &lt;- c(\"A10BJ\", \"A10BK\", \"A10BH\", \"A10BA\", \"A10BB\", \"A10BG\", \"A10BX\", \"A10A\")\n\n# Filter\ndf_atc_2023_filtered &lt;- df_atc_2023_filtered |&gt;\n  filter(atc %in% DCs) |&gt;\n  mutate(\n    DC = case_when( # New drug class variable\n      atc == \"A10BJ\" ~ \"GLP1\", # GLP1\n      atc == \"A10BK\" ~ \"SGLT2\", # SGLT2\n      atc == \"A10BH\" ~ \"DPP4\", # DPP4\n      atc == \"A10BA\" ~ \"Metformin\", # MET\n      atc == \"A10BB\" ~ \"SU\", # SU\n      atc == \"A10BG\" ~ \"TZD\", # Thiazolidinediones\n      atc == \"A10BX\" ~ \"Others\", # Others\n      atc == \"A10A\"  ~ \"INSULIN\",  # INSULIN\n      TRUE ~ NA_character_  # Default to NA for all other values\n    ),\n    turnover1000k = turnover/1000 # 1000k because its already in 1000's.\n    )\n\nNow its time to make the plot.\n\n# Plot - stacked bar chart\n  # total turnover\np1 &lt;- ggplot(df_atc_2023_filtered, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Total turnover by drug class\",\n    caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\"\n    )\n  # propotional turnover for each group\np2 &lt;- ggplot(df_atc_2023_filtered, aes(x = agegroup, y = turnover1000k, fill = DC)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(\n    y = \"\",\n    title = \"Proportional turnover by drug class\",\n    caption = \"Source: own calculations based on data from medstat.dk via the Danish Health Data Authority\"\n    )\n\np1\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # p1 saved as thumbnail\n\np2\n\n\n\n\n\n\n\n\nWell then.. The turnover for GLP1 in this country is quite impressive, with the 45 to 64 year olds taking by far the biggest piece of the pie. Thats about 6.000.000.000 DKK in turnover for that age group in 2023 alone.\nIn the next blog post I aim to show how to redo all that I did, as a function.\nAnd hopefully it will also be a bit more organised."
  },
  {
    "objectID": "posts/2025-01-19-explanation/index.html",
    "href": "posts/2025-01-19-explanation/index.html",
    "title": "Got the PhD",
    "section": "",
    "text": "Here with two things:"
  },
  {
    "objectID": "posts/2025-01-19-explanation/index.html#explanation-hiatus---got-the-phd",
    "href": "posts/2025-01-19-explanation/index.html#explanation-hiatus---got-the-phd",
    "title": "Got the PhD",
    "section": "Explanation Hiatus - Got the PhD!",
    "text": "Explanation Hiatus - Got the PhD!\nNot all blog posts are created equal. Some, like this one, are just me writing and explaining.\nIn this case, I’m just here to explain that I’ve been busy preparing for my PhD defence which occurred the 16th of December 2024, and which went well above my expectations.\nI am now a Doctor of Philosophy (PhD, DPhil; Latin: philosophiae doctor or doctor in philosophia) [^phd]: wiki.\nThe space between the Ph.D. defence and now has been spent taking some well-deserved time off and being with family. Got to have that sweet sweet meaningful family time once in a while. Especially after getting a PhD."
  },
  {
    "objectID": "posts/2025-01-19-explanation/index.html#a-promise",
    "href": "posts/2025-01-19-explanation/index.html#a-promise",
    "title": "Got the PhD",
    "section": "A promise",
    "text": "A promise\nSo, yeah. Other than all the R code, Quarto, and testing, hypotheses and so on, I will also practice the following on my blog:\n\nBeing as clear, concise, and precise in my writing as possible\nWriting with a clear purpose, which certainly may develop for each post\n\nThis blog is a work in progress, and I may not be successful in the two above endeavors.\nUntil next time, everyone!"
  },
  {
    "objectID": "posts/2025-07-10-montecarlo/index.html",
    "href": "posts/2025-07-10-montecarlo/index.html",
    "title": "Monte Carlo, Life paths, and AI",
    "section": "",
    "text": "Warning\n\n\n\nThis blogpost was concieved as a short look at monte carlo simulation, but turned into both musings on some health economic decision principles, and discussion of the advantages and drawbacks of using AI."
  },
  {
    "objectID": "posts/2025-07-10-montecarlo/index.html#purpose",
    "href": "posts/2025-07-10-montecarlo/index.html#purpose",
    "title": "Monte Carlo, Life paths, and AI",
    "section": "1 Purpose",
    "text": "1 Purpose\n\n1.1 A little intro text\nInspired by Fooled by Randomness by Nassim Taleb, I decided to try doing some Monte Carlo simulations of different potential life paths ahead of me.\nMonte Carlo simulations are not foreign to me, as they are frequently used in health economic models to take uncertainty into account when presenting a comparison between different health interventions or strategies.\nThe way that is usually done is by having point estimates and confidence intervals - or credibility intervals if you are bayesian, which those types of analyses mostly are - for certain outcomes, both in terms of their costs and effects, associated with each option being considered.\nThese estimates and intervals are then sampled randomly in simulations around at least 1000 times, to provide estimates of which option is better than the other, in probability clouds.\nThese probability clouds represent the spread of the different possible outcomes, given the uncertainty associated with the confidence intervals.\nImagine two dots on a plot, each representing the cost and effects of the interventions. One seems to be in better place than the other. Is the intervention with seemingly the best cost/effect ratio compared to the other always the better choice? Not necessarily.\nWith the monte carlo simulation, you can see thousands of dots of the two different colours. The clouds may overlap, or the cloud representing the seemingly worse intervention might have more estimates that seem to be in more advantagous spots.\nAdvantageous how? Well, more of the estimates may be in the “much higher effect, at a sligthly higher cost” than the single point estimate mentioned earlier. Visualising how the possible estimates spread out, in many different “worlds” or scenarios, can help decision-makers determine what is likely to be the best option, at least much better than a single point estimate, and a range of uncertainty with two numbers.\nSo. That would be nice to try to transfer to different life choices, As right now, I am looking for a job, and hey, who knows what kind of job I can get.\n\n\n1.2 Distilled purpose\nWe will start simple with a comparison of 3 different life paths from on here:\n\nPublic sector job\nPrivate sector job\nStart own business / “Startup”\n\nNot to conclude what is best, but to see what the possible consequences can be.\nThis is of course ridiculously simplified, as I am likely to switch jobs more than once after the initial choice. That could be the theme of a different post - a decision tree or markov model where spending time in different states is possible.\n\n\n1.3 Analysis plan\nIn brief:\n\n3 different options\n20 year into the future\n1000 simulations of “life paths”\n\nBecause the tool is available and I would be insane not to use it, I use GPT to help me generate the template, and then learn how to change it from there to fit my needs.\nAlso, I learn how it works, preferably by both searching for different ways of doing it online."
  },
  {
    "objectID": "posts/2025-07-10-montecarlo/index.html#process",
    "href": "posts/2025-07-10-montecarlo/index.html#process",
    "title": "Monte Carlo, Life paths, and AI",
    "section": "2 Process",
    "text": "2 Process\nNo need to load and clean data, as I will generate it myself.\n\n2.1 Setup\nJust need the tidyverse package for this. Might try adding health economic packages like heemod, dampak, or hesim in the future. Maybe there is a tidyverse like package collection for health economic packages?\n\nlibrary(tidyverse)\n\n\n2.1.1 Parameters\nYears, simulations, options as a tibble.\n\n# basic parameters\nn_years &lt;- 20\nn_sim &lt;- 1000\n\n# decisions/choices\ndecisions &lt;- tibble(\n  decision = c(\"Private\", \"Public\", \"Startup\"),\n  startup_prob_success = c(NA, NA, 0.30),\n  # all moneys in 1000 DKK/year\n  startup_income_success = c(NA, NA, 6000),\n  base_income = c(600, 400, 50),\n  growth_rate_income = c(0.02, 0.02, 0.03),\n  std_dev_income = c(10, 5, 300),\n  base_content = c(3.2, 3.5, 3.0),\n  content_sd = c(0.4, 0.3, 0.9),\n  burnout_prob = c(0.10, 0.05, 0.25),\n  health_shock_prob = c(0.05, 0.05, 0.15)\n)\n\n\n\n2.1.2 Simulation function\nFirst the simulation function for one simulation, for one decision.\n\nsimulate_one_life &lt;- function(decision_row, sim_id) {\n  # if dec=startup, calculate prob of success, if not, NA\n  has_success &lt;- ifelse(decision_row$decision == \"startup\",\n                        rbinom(1, 1, decision_row$startup_prob_success), NA)\n\n  # Income, for startup it depends on success\n  years &lt;- 1:n_years\n  income_base &lt;- decision_row$base_income * (1 + decision_row$growth_rate_income)^(years - 1)\n\n  # for the gamma distribution thats necesary for startups (no 0)\n\n  income &lt;- case_when(\n    decision_row$decision == \"Startup\" & !is.na(has_success) & has_success == 1 ~\n      rep(decision_row$startup_prob_success, n_years),\n    decision_row$decision == \"Startup\" ~ {\n      # I assume gamma because income even for startup is probably from 0 and above\n      shape &lt;- (500 / 100)^2 #mean/sd\n      rate &lt;- 500 / 100^2 #mean/sd\n      rgamma(n_years, shape = shape, rate = rate)\n    },\n    TRUE ~\n  # For the rest, it follows their respective distribution parameters\n      rnorm(n_years, mean = income_base, sd = decision_row$std_dev_income)\n  )\n\n  # Yearly chance events\n  health_shock &lt;- rbinom(n_years, 1, decision_row$health_shock_prob)\n  burnout &lt;- rbinom(n_years, 1, decision_row$burnout_prob)\n\n  # Parenting - which affects the contentness\n    # First child: already present (baseline) - dont consider\n    # Second child: guaranteed to arrive within first 5 years\n    kid2_year &lt;- sample(2:6, 1)\n\n    # Parenting flag for the second\n    parenting2 &lt;- ifelse((1:n_years) &gt;= kid2_year, 1, 0)\n\n  # Contentness\n  content &lt;- rnorm(n_years, decision_row$base_content, decision_row$content_sd) -\n    0.4 * burnout - 0.6 * health_shock + 0.3 * parenting2\n\n  # Scale must be between 1 (very not content), to 5 (very content)\n  content &lt;- pmin(5, pmax(1, content))\n\n  # Savings of 10% each year, minus healthshock prob * its cost\n  savings &lt;- cumsum(income * 0.1 - 5 * health_shock) # health schock cost in 1000's too\n\n  # Standardising a total hapiness score. each one contributes 20-40% to it\n  # happiness &lt;- 0.4 * scale(income)[, 1] +\n  #              0.4 * scale(content)[, 1] +\n  #              0.2 * scale(savings)[, 1]\n\n  # make tibble for output\n  tibble(\n    year = 1:n_years,\n    sim = sim_id,\n    decision = decision_row$decision,\n    income,\n    content,\n    burnout,\n    health_shock,\n    parenting2,\n    savings\n  )\n}\n\n\n\n\n2.2 Output\n\n2.2.1 Run and summarize\nFor each decision, we simulate 1000 iterations of what would happen over 20 years.\n\nresults &lt;- map(1:nrow(decisions), function(i) {\n  decision_row &lt;- decisions[i, ]\n\n  map(1:n_sim, ~simulate_one_life(decision_row, .x)) |&gt;\n    list_rbind()\n}) |&gt; list_rbind()\n\nIt literally only takes seconds to simulate three diferent life paths (20 years), 1000 times. Holy moley. But, I know from experience that including more variables or more simulations increases the time non-linearly.\nNow, to get some point estimates for each decision, we can summarise the results by grouping the simulations results by simulation number, and decision and calculating some averages, and the last of the savings.\nThen, we group by each decision, and then just take the mean of each value, to get a point estimate for each decision.\nThe mean of all simulations for each decision. Mean of mean of the possible values, if you will.\n\nsummary &lt;- results |&gt;\n  group_by(sim, decision) |&gt;\n  summarise(\n    avg_income = mean(income),\n    avg_content = mean(content),\n    burnout_rate = mean(burnout),\n    health_event_rate = mean(health_shock),\n    end_savings = last(savings),\n    .groups = \"drop\"\n  ) |&gt;\n  select(-sim) |&gt;  # &lt;- remove sim so it's not re-summarized\n  group_by(decision) |&gt;\n  summarise(across(everything(), list(mean = mean, sd = sd)), .groups = \"drop\")\n\nprint(summary)\n\n# A tibble: 3 × 11\n  decision avg_income_mean avg_income_sd avg_content_mean avg_content_sd\n  &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Private             729.          2.29             3.38         0.101 \n2 Public              486.          1.12             3.70         0.0761\n3 Startup             501.         21.6              3.07         0.208 \n# ℹ 6 more variables: burnout_rate_mean &lt;dbl&gt;, burnout_rate_sd &lt;dbl&gt;,\n#   health_event_rate_mean &lt;dbl&gt;, health_event_rate_sd &lt;dbl&gt;,\n#   end_savings_mean &lt;dbl&gt;, end_savings_sd &lt;dbl&gt;\n\n\n\n\n2.2.2 Visualize\nNow for perhaps the most helpful part of this: the raw visualisation of the possible future lifepath outcomes.\n\nresults |&gt;\n  ggplot(aes(x = income, fill = decision)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Life Income Distribution by Decision Path\",\n    x = \"Total Income in 1000 DKK over 30 Years\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\nresults |&gt;\n  ggplot(aes(x = content, fill = decision)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Life Income Distribution by Decision Path\",\n    x = \"Total Income in 1000 DKK over 30 Years\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nresults |&gt;\n  ggplot(aes(x = income, y = content, color = decision)) +\n  geom_point(alpha = 0.2) +\n  geom_point(\n    data = summary,\n    aes(x = avg_income_mean, y = avg_content_mean, group = decision),\n    color = \"black\",\n    shape = 18) +\n  geom_text(\n    data = summary,\n    aes(x = avg_income_mean, y = avg_content_mean, label = decision),\n    vjust = -1,\n    inherit.aes = FALSE\n  ) +\n  labs(\n    title = \"Life Income Distribution by Decision Path - Point Estimates Added\",\n    x = \"Total Income in 1000 DKK over 30 Years\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saved as thumbnail\n\nAs the pure scatter plots are a bit wild on the eyes, I added the point estimates from the previous summary, for each decision.\nSo there. A truckload of simulations of arbitrary lifepaths with relatively arbitrary data feeding it. The following can be seen at a glance:\n\nThe public sector job decision sits with relatively high contentness at lower income\nThe private sector job has much higher income, at slightly more dispersed contentness\nThe startup path is mired in such great uncertainty, that its cloud takes up a lot of space. It has both the highest and lowest income and contentness, with many simulations at the highest and lowest contentment levels.\n\nI will skip visualising the savings for now.\nWhat is more important to visualise is the effects besides the income and contentness, that is:\n\nHow much burnout and health schock are associated with each decision?\n\n\nresults |&gt;\n  group_by(decision, year) |&gt;\n  summarise(burnout_rate = mean(burnout), .groups = \"drop\") |&gt;\n  ggplot(aes(x = year, y = burnout_rate, color = decision)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Burnout Rate by Decision Over Time\",\n    y = \"Proportion Burned Out\",\n    x = \"Year\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nresults |&gt;\n  group_by(decision, year) |&gt;\n  summarise(shock_rate = mean(health_shock), .groups = \"drop\") |&gt;\n  ggplot(aes(x = year, y = shock_rate, color = decision)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Annual Health Shock Rate by Decision\",\n    y = \"Proportion with Health Shock\",\n    x = \"Year\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAround twice the burnout rate and three times the health shock rate for the Startup decision relative to the others.\nSo the simulations accurately represents (because the inputs accurately represents), the high risk high reward style of starting a company."
  },
  {
    "objectID": "posts/2025-07-10-montecarlo/index.html#summary-cleaned-up-refactored",
    "href": "posts/2025-07-10-montecarlo/index.html#summary-cleaned-up-refactored",
    "title": "Monte Carlo, Life paths, and AI",
    "section": "3 Summary / cleaned up / refactored",
    "text": "3 Summary / cleaned up / refactored\nSo, to summarise.\nI created three imaginary life paths that I could take and assumed that I would stick to the path chosen for the simulation period.\nThen I defined some different base values and uncertanties in the form of standard deviations and probabilities, for the life paths, in a little data frame.\nThese were then put into a simulation function that defined what would occur for each simulation, with differences for each path.\nThen the simulation function was applied to the life path dataframe, for each decision, for 20 years, 1000 times.\nAnd the results were visualised with density plots, scatter plots for content. With these arbitrary inputs, not much can be gleaned from such a simulation, other than the fact that uncertainty plays a huge role and one can have a lot of different outcomes. Perhaps based on this one could decide whether one felt\n\n3.1 A Note on AI\nAI (ChatGPT) played a role in this blog in the following ways:\n\nCarrying my idea into practice (writing the main code)\nChanging the code\n\nThe amount of time I saved from thinking my way through this is enormous. Which leads to the critical question asked often these days:\n\nWhat have I lost in terms of learning, with the short term gain in efficiency from using AI?\n\nThis applies to both point 1 and 2 above.\nFor both points, it is obvious that I have lost something. It has been established by many researchers that slow and deliberate practice fosters better brain development and learning than just absorbing summaries or cramming a bunch of information in your head in a short amount of time.\nFrom my memory, the following has been shown to be very beneficial for learning:\n\nPen and paper (in most scenarios)\nSlow, deliberate learning with clear goals for which progress can be tracked\nSpaced repitition (learn about a thing.. let it sit a bit.. bring it to your memory again.. wait longer.. bring it to your memory, rinse, repeat)\nExplaining it in your own terms, and to others (hitching on to the famous “Feynman Technique”)\nDoing it in a location and environment that specifically fits you, but probably mostly with distractions removed\nShort intense bursts, and then breaks with NOTHING to do but relaxing (not on phone or SOME)\n\nHowever, is there perhaps also an obvious benefit to using AI? For example, how different is AI from , at least parts of, the mentor-mentee relationship?\nWhile a real mentor provides the following:\n\nPersonalised feedback and guidance on:\n\nProgress\nStrengths and weaknesses\nCareer advice\n\nField specific expert level knowledge transfer\nTacit knowledge transfer\nMotivation\nPublic approval and recommendations to others\nTrust and support\n\nAn AI mentor can in large part provide the same, where, I believe, the following is significantly different, in terms of what the AI can do:\n\nMuch quicker access to knowledge\nPoor deep brainstorming and not precise progress tracking\n\nA real expert will know exactly how much a person knows, and when to prompt different levels or stages of knowledge\n\nTracking of progress is fine until AI hallucinations occur\nMuch weaker transfer of tacit knowledge\nWeaker motivation\n\nThe text version of “You’ve got it!” does not hit nearly the same way as the real life, honest, and enthusiastic version\n\nNo recommendation and trust\nAI can’t abuse you (bad mentor)\n\nOr, as ChatGPT puts it:\n\nWhat it can’t provide are the richer social-emotional cues, contextual wisdom and professional sponsorship that come from a flesh-and-blood mentor embedded in your world. Treat the model as a tireless practice partner and sounding board, not a full substitute, and you’ll capture the best of both worlds.\n\nIn summary, I believe AI will raise the floor of learning, making it resemble plot B more than plot A. See below.\n\ndf1 &lt;- data.frame(\n  a = as.numeric(rnorm(1000, 100, 15)),\n  b = as.numeric(rnorm(1000, 100, 25))\n)\n\nggplot(data = df1) +\n  geom_density(aes(x=a), color = \"red\") +\n  geom_density(aes(x=b), color = \"blue\") +\n  labs(x = NULL,y = NULL) + theme_minimal()\n\n\n\n\n\n\n\n\nI set the mean around 100 to make it resemble IQ.\nThat is, I believe:\n\nIt will be a good tool to gain access to knowledge for people who were either too lazy to access it or were prevented in some way before\nThere are some who will overly rely on it and suffer the consequences of losing some of their ability to think for themselves\nThere are some who will recognise its potential and downsides, and use it when it is appropriate, soring about others\n\nThe last group is likely the ones who are least likely, at least in the white-collar jobs, to be replaced by AI.\nUp until AI is more “Agentic”, ofcourse. By then I have no idea what will happen. Some smart people are thinking about this, though.\n\n\n3.2 Refactored code\nBelow you can find the entire code, refactored a bit.\nI used the following prompt, which worked fine although it actually made some errors that made me have to remake some of it.\nFirst, please take out all chunks, and refactor them in the following way:\n\n- make code more concise, with functions for repeating lines or plots\n- remove comments\n- remove unecessary code\n\nlibrary(tidyverse)\n\nn_years &lt;- 20\nn_sim   &lt;- 1000\n\ndecisions &lt;- tibble(\n    decision = c(\"Private\", \"Public\", \"Startup\"),\n    startup_prob_success = c(NA, NA, 0.30),\n    # all moneys in 1000 DKK/year\n    startup_income_success = c(NA, NA, 6000),\n    base_income = c(600, 400, 50),\n    growth_rate_income = c(0.02, 0.02, 0.03),\n    std_dev_income = c(10, 5, 300),\n  base_content = c(3.2, 3.5, 3.0),\n  content_sd = c(0.4, 0.3, 0.9),\n    burnout_prob = c(0.10, 0.05, 0.25),\n    health_shock_prob = c(0.05, 0.05, 0.15)\n)\n\nsimulate_one_life &lt;- function(decision_row, sim_id) {\n  has_success &lt;- ifelse(decision_row$decision == \"startup\",\n                        rbinom(1, 1, decision_row$startup_prob_success), NA)\n  years &lt;- 1:n_years\n  income_base &lt;- decision_row$base_income * (1 + decision_row$growth_rate_income)^(years - 1)\n\n  income &lt;- case_when(\n    decision_row$decision == \"Startup\" & !is.na(has_success) & has_success == 1 ~\n      rep(decision_row$startup_prob_success, n_years),\n    decision_row$decision == \"Startup\" ~ {\n      shape &lt;- (500 / 100)^2 #mean/sd\n      rate &lt;- 500 / 100^2 #mean/sd\n      rgamma(n_years, shape = shape, rate = rate)\n    },\n    TRUE ~ rnorm(n_years, mean = income_base, sd = decision_row$std_dev_income)\n  )\n\n  health_shock &lt;- rbinom(n_years, 1, decision_row$health_shock_prob)\n  burnout &lt;- rbinom(n_years, 1, decision_row$burnout_prob)\n  kid2_year &lt;- sample(2:6, 1)\n  parenting2 &lt;- ifelse((1:n_years) &gt;= kid2_year, 1, 0)\n\n  content &lt;- rnorm(n_years, decision_row$base_content, decision_row$content_sd) -\n    0.4 * burnout - 0.6 * health_shock + 0.3 * parenting2\n\n  content &lt;- pmin(5, pmax(1, content))\n  savings &lt;- cumsum(income * 0.1 - 5 * health_shock)\n\n  tibble(\n    year = 1:n_years,\n    sim = sim_id,\n    decision = decision_row$decision,\n    income,\n    content,\n    burnout,\n    health_shock,\n    parenting2,\n    savings\n  )\n}\n\nrun_decision &lt;- function(row, reps) {\n  map(seq_len(reps), ~simulate_one_life(row, .x)) |&gt; list_rbind()\n}\n\nresults &lt;- map(seq_len(nrow(decisions)),\n               ~run_decision(decisions[.x, ], n_sim)) |&gt; list_rbind()\n\nsummary &lt;- results |&gt;\n    group_by(sim, decision) |&gt;\n    summarise(\n      avg_income   = mean(income),\n      avg_content  = mean(content),\n      burnout_rate = mean(burnout),\n      health_rate  = mean(health_shock),\n      end_savings  = last(savings),\n      .groups = \"drop\"\n    ) |&gt;\n    select(-sim) |&gt;\n    group_by(decision) |&gt;\n    summarise(across(everything(), list(mean = mean, sd = sd)), .groups = \"drop\")\n\nplot_density &lt;- function(dat, var){\n  ggplot(dat, aes({{ var }}, fill = decision)) +\n    geom_density(alpha = 0.4) +\n    theme_minimal()\n  }\n\nplot_scatter &lt;- function(dat, sumdat){\n  ggplot(dat, aes(income, content, colour = decision)) +\n    geom_point(alpha = 0.2) +\n    geom_point(data = sumdat,\n               aes(avg_income_mean, avg_content_mean),\n               colour = \"black\", shape = 18, inherit.aes = FALSE) +\n    geom_text(data = sumdat,\n              aes(avg_income_mean, avg_content_mean, label = decision),\n              vjust = -1, inherit.aes = FALSE) +\n    theme_minimal()\n}\n\nplot_rate &lt;- function(dat, var, ttl){\n  ggplot(dat %&gt;% group_by(decision, year) %&gt;%\n           summarise(rate = mean({{ var }}), .groups = \"drop\"),\n         aes(year, rate, colour = decision)) +\n    geom_line(linewidth = 1) +\n    labs(title = ttl) +\n    theme_minimal()\n}\n\nplot_density(results, income)\nplot_density(results, content)\nplot_scatter(results, summary)\nplot_rate(results, burnout,       \"Burnout Rate\")\nplot_rate(results, health_shock,  \"Health-Shock Rate\")"
  },
  {
    "objectID": "posts/2024-09-30-tot_ddd_use/index.html",
    "href": "posts/2024-09-30-tot_ddd_use/index.html",
    "title": "Evolution in DDD use of Diabetes Drugs over time",
    "section": "",
    "text": "My two previous posts examined data available from medstat and were focused on those datasets and specifically the turnover of specific drug classes.\nHere, I wish to demonstrate a different type of publicly available data, still related to prescription data.\nThis is data regarding drug prices, and can be found on The Danish Health Data Authority’s website here.\nDanish drug prices are by and large renegotiated every 14 days. Therefore, the dataset is updated accordingly. I will use the data from the most recent update as of the start of making this post.\nLooking at it post hoc, I used the following packages:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\nlibrary(stringr)\nlibrary(extrafont)\n# install.packages(\"ggpubr\")\n# library(gridExtra)\n# library(ggpubr)\n\n\n\nI had some initial trouble figuring out how to download the file. What kept me from figuring it out was not realising that the file from the link was a zip file containing an excel file.\nApparently, “The extension .ashx doesn’t specify the file format but rather that a server-side handler is being used to serve the content.”, as chatGPT puts it. I have no gained the understanding that .ashx is just some sort of framework that serves up the data, not a file type.\nThis leads to the following code necessary to download the data. Code is just for show.\n\n# URL of the .ashx file\nurl &lt;- \"https://www.esundhed.dk/-/media/Files/Publikationer/Emner/Laegemidler/Medicinpriser/2024/lmpriser_eSundhed_240916.ashx\"\n\n# Define a path in my local datafolder, used as described in the first post about medstat data\ndatapath &lt;- \"C:/Users/henri/Documents/data-publicdataprojects\"\n\n# Define the path where the ZIP file will be saved\nzip_destfile &lt;- paste0(datapath,\"lmpriser_eSundhed_240916.zip\")\n\n# Download the ZIP file\ndownload.file(url, zip_destfile, mode = \"wb\")\n\n# Unzip the file\nunzip(zip_destfile, exdir = paste0(datapath,\"unzipped_files\"))\n\n# Check the unzipped files\nlist.files(paste0(datapath,\"unzipped_files\"))\n\n# check which sheet to import from the excel file\nsheet_names &lt;- excel_sheets(paste0(datapath,\"unzipped_files/lmpriser_eSundhed_240916.xlsx\"))\n\nAnd then I import the file:\n\n# Read the excel file\ndatapath &lt;- \"C:/Users/henri/Documents/data-publicdataprojects\"\ndf_ddd &lt;- read_excel(paste0(datapath,\"unzipped_files/lmpriser_eSundhed_240916.xlsx\"), sheet = \"lmpriser_eSundhed_240916\")\n\nhead(df_ddd)\n\n# A tibble: 6 × 144\n  ATC    Lægemiddel Indholdsstof Varenummer Pakning Styrke Form  Firma Indikator\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    \n1 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… AIP      \n2 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… AUP      \n3 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… DDD      \n4 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… AUP_pr_D…\n5 A01AA… Duraphat   Natriumfluo… 066422     3 x 51… 5 mg/g tand… Colg… AIP      \n6 A01AA… Duraphat   Natriumfluo… 066422     3 x 51… 5 mg/g tand… Colg… AUP      \n# ℹ 135 more variables: `20190729` &lt;dbl&gt;, `20190812` &lt;dbl&gt;, `20190826` &lt;dbl&gt;,\n#   `20190909` &lt;dbl&gt;, `20190923` &lt;dbl&gt;, `20191007` &lt;dbl&gt;, `20191021` &lt;dbl&gt;,\n#   `20191104` &lt;dbl&gt;, `20191118` &lt;dbl&gt;, `20191202` &lt;dbl&gt;, `20191216` &lt;dbl&gt;,\n#   `20191230` &lt;dbl&gt;, `20200113` &lt;dbl&gt;, `20200127` &lt;dbl&gt;, `20200210` &lt;dbl&gt;,\n#   `20200224` &lt;dbl&gt;, `20200309` &lt;dbl&gt;, `20200323` &lt;dbl&gt;, `20200406` &lt;dbl&gt;,\n#   `20200420` &lt;dbl&gt;, `20200504` &lt;dbl&gt;, `20200518` &lt;dbl&gt;, `20200601` &lt;dbl&gt;,\n#   `20200615` &lt;dbl&gt;, `20200629` &lt;dbl&gt;, `20200713` &lt;dbl&gt;, `20200727` &lt;dbl&gt;, …\n\n\n\n\n\nJust as the post, I want to focus on drugs used for diabetes. So I keep only the observations with A10A and A10B:\n\n# Cleaning it in one set of operations\ndf_ddd_A10 &lt;- df_ddd |&gt;\n# Filter for just A10A and A10B, and keep just prp per ddd\n  filter(\n    str_detect(ATC,(\"^A10B\")) | str_detect(ATC,(\"^A10A\")),\n    Indikator == \"AUP_pr_DDD\") |&gt;\n  # Select variables\n  select(ATC, Indholdsstof, Lægemiddel, starts_with(\"20\")) |&gt;\n  # Pivot the data so that the variable columns that contain time are contained in one variable column\n  pivot_longer(\n    cols = starts_with(\"20\"),\n    names_to = \"Tid\",\n    values_to = \"prpddd\") |&gt;\n  mutate(\n    Tid = ymd(Tid)) |&gt;\n  # group_by(tid) |&gt;  DELETE??\n  # mutate(\n  #   hip_ddd = max(prpddd), # hip_ddd = maxpris per ddd\n  #   lop_ddd = min(prpddd) # lop_ddd = minpris per ddd\n  # ) |&gt;\n  # ungroup() |&gt;\n  filter(\n    !is.na(prpddd)\n  )\n\n# And translate the colnames to english for good measure\nhead(df_ddd_A10)\n\n# A tibble: 6 × 5\n  ATC     Indholdsstof    Lægemiddel Tid        prpddd\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;      &lt;date&gt;      &lt;dbl&gt;\n1 A10AB01 Insulin (human) Actrapid   2019-07-29   6.48\n2 A10AB01 Insulin (human) Actrapid   2019-08-12   6.48\n3 A10AB01 Insulin (human) Actrapid   2019-08-26   6.48\n4 A10AB01 Insulin (human) Actrapid   2019-09-09   6.48\n5 A10AB01 Insulin (human) Actrapid   2019-09-23   6.48\n6 A10AB01 Insulin (human) Actrapid   2019-10-07   6.48\n\ncolnames(df_ddd_A10) &lt;- c(\n  \"atc\",\n  \"compound\",\n  \"product\",\n  \"time\",\n  \"prpddd\" # Pharmacy Retail Price DDD\n)\nhead(df_ddd_A10)\n\n# A tibble: 6 × 5\n  atc     compound        product  time       prpddd\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;    &lt;date&gt;      &lt;dbl&gt;\n1 A10AB01 Insulin (human) Actrapid 2019-07-29   6.48\n2 A10AB01 Insulin (human) Actrapid 2019-08-12   6.48\n3 A10AB01 Insulin (human) Actrapid 2019-08-26   6.48\n4 A10AB01 Insulin (human) Actrapid 2019-09-09   6.48\n5 A10AB01 Insulin (human) Actrapid 2019-09-23   6.48\n6 A10AB01 Insulin (human) Actrapid 2019-10-07   6.48\n\n\n\n\n\nThe resulting dataset is one with all registered Pharmacy Retail Price per Defined Daily Dose (PRP per DDD) of Drugs in the A10 ATC category, from between 2019-07-29 and 2024-09-16. In other words: from the past 5.1362081 years.\nThis is basically a measure of how much it costs to treat a person with the medication with a standard dose, each day, and can be used to compare how much each drug costs to use for treatment. This is used because it can be difficult to compare drugs based on redeemed prescriptions or prices, as some drugs are prescribed differently than others. Also, some drugs have absurdly high prices per pill, as some specialised drugs in oncology and opthalmology, compared to very cheap generic pain killers.\nThis is exemplified in the code below.\n\ndf_ddd_test &lt;- df_ddd |&gt;\n  filter(Indikator == \"AUP\") |&gt; # AUP = Pharmacy Retail Price\n  pivot_longer(\n    cols = starts_with(\"20\"),\n    names_to = \"Tid\",\n    values_to = \"prp\"\n    ) |&gt;\n    summarise(\n    highest_prp = max(prp, na.rm = TRUE),\n    lowest_prp = min(prp, na.rm = TRUE)\n  )\ndf_ddd_test\n\n# A tibble: 1 × 2\n  highest_prp lowest_prp\n        &lt;dbl&gt;      &lt;dbl&gt;\n1    9999990.       7.25\n\n\nI want to present the numbers with inline code, and this can be done easily with the basic R function format() and cat() which can format the numeric value and concatenate the text.\n\n# Format the numbers in base R\nformatted_highest_prp &lt;- format(df_ddd_test$highest_prp, big.mark = \",\", scientific = FALSE)\nformatted_lowest_prp &lt;- format(df_ddd_test$lowest_prp, big.mark = \",\", scientific = FALSE)\n\n# Print the formatted numbers\ncat(\"Highest AUP:\", formatted_highest_prp, \"DKK\\n\")\n\nHighest AUP: 9,999,990 DKK\n\ncat(\"Lowest AUP:\", formatted_lowest_prp, \"DKK\\n\")\n\nLowest AUP: 7.25 DKK\n\n\nBelow, the numbers are generated with inline code:\nThe highest Pharmacy Retail Price is NULL, and the lowest is NULL, which is QUITE a difference.\n\n\n\nNow, back to the retail price per DDD of diabetes drugs. Just as a visual aid, lets compare the most expensive to the cheapest, of the diabetes drugs.\nBut before that, a useful thing I picked up from Meghan Hall’s blog, is putting a consistent theme on your plots. This can be done as below.\n\nlibrary(gghighlight)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(ggtext)\nlibrary(ggrepel)\n\nhen_theme &lt;- function () {\n  theme_linedraw(base_size=11) %+replace%\n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.background = element_rect(fill = \"white\", color = NA),\n      legend.key = element_rect(fill = \"white\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.title = element_text(size = 16, hjust = 0, vjust = 0.5,\n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 10, hjust = 0, vjust = 0.5,\n                                   margin = margin(b = 0.4, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\",\n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\n\nThe “hen_theme” is then added to future plots, at the end.\n\n# find most expensive\ndf_ddd_A10 |&gt;\n  group_by(atc) |&gt;\n  summarise(\n    highest_prpddd = max(prpddd, na.rm = TRUE),\n    lowest_prpddd = min(prpddd, na.rm = TRUE)\n  ) |&gt;\n  summarise(\n    most_expensive_atc = atc[which.max(highest_prpddd)],\n    most_expensive_value = max(highest_prpddd),\n    least_expensive_atc = atc[which.min(lowest_prpddd)],\n    least_expensive_value = min(lowest_prpddd)\n  )\n\n# A tibble: 1 × 4\n  most_expensive_atc most_expensive_value least_expensive_atc\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;              \n1 A10BX16                            522. A10BB12            \n# ℹ 1 more variable: least_expensive_value &lt;dbl&gt;\n\n# plot\nhilo_ddd &lt;- df_ddd_A10 |&gt;\n  filter(atc == \"A10BX16\" | atc == \"A10BB12\") |&gt;\n  select(compound,time,prpddd)\n\nggplot(hilo_ddd, mapping = aes(x=time,y=prpddd, colour = compound)) +\n  geom_point() +\n  labs(\n    title = \"Cheapest and most expensive A10 drugs\",\n    subtitle = \"by Pharmacy Retail Price per DDD\",\n    y = \"Pharmacy Retail Price per DDD\",\n    x = \"\",\n  ) +\n  hen_theme() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = c(0.8,0.9),\n    legend.background = element_rect(fill= \"white\")\n    )\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nThis plot tells us that the two drug compounds with the highest and lowest PRP per DDD are Glimepiride, of the sulfonyurea class, and Tirzepatide, a GIP and GLP1 combination.\nNow, I wish to visualise how PRP per DDD changes over time. Below, I create the variables for the minimum and maximum values for each compound.\n\n# Make a new var thats floored to months and find the min for each month\ndf_ddd_A10_minmax &lt;- df_ddd_A10 |&gt;\n  mutate(month = floor_date(time,\"month\")) |&gt;\n  group_by(month, atc, compound) |&gt;\n  summarise(\n    a_min_prpddd = min(prpddd, na.rm = TRUE), # \"a_\" is to make it first in the facetwrap later\n    b_max_prpddd = max(prpddd, na.rm = TRUE)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'month', 'atc'. You can override using the\n`.groups` argument.\n\nhead(df_ddd_A10_minmax)\n\n# A tibble: 6 × 5\n  month      atc     compound         a_min_prpddd b_max_prpddd\n  &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;                   &lt;dbl&gt;        &lt;dbl&gt;\n1 2019-07-01 A10AB01 Insulin (human)          6.07         9.90\n2 2019-07-01 A10AB04 Insulin lispro           9.8         12.2 \n3 2019-07-01 A10AB05 Insulin aspart           9.66        13.1 \n4 2019-07-01 A10AB06 Insulin glulisin         9.35         9.55\n5 2019-07-01 A10AC01 Insulin (human)          6.07        10.9 \n6 2019-07-01 A10AD01 Insulin (human)          6.34        10.9 \n\n\nAnd now for the grand plot I have planned for this post, which will be an overview of the PRP per DDD for the main drug classes in diabetes type 2 treatment.\nFirst, I limit the dataset to only encompass the classes I want to look at.\n\ndfforplot &lt;- df_ddd_A10_minmax |&gt;\n  filter(\n    str_detect(atc,\"^A10BA\") |   # met\n      str_detect(atc,\"^A10BB\") | # sul\n      str_detect(atc,\"^A10BK\") | # sglt2\n      str_detect(atc,\"^A10BJ\") | # glp1\n      str_detect(atc,\"^A10BH\") | # dpp4\n      str_detect(atc,\"^A10BX16\") # tirzepatid, honoured guest\n    )\n\nThen, I try to make the plots which contain a surmountable amount of information.\n\n# Minimum values plot\nplotmin &lt;- ggplot(dfforplot) +\n  geom_line(aes(x = month, y = a_min_prpddd, colour = compound), linewidth = 0.8) +\n  labs(title = \"Minimum PRP per DDD Over Time\",\n       x = \"\",\n       y = \"PRP per DDD\",\n       colour = \"Drug\") +\n  hen_theme()\n# Max\nplotmax &lt;- ggplot(dfforplot) +\n  geom_line(aes(x = month, y = b_max_prpddd, colour = compound), linewidth = 0.8) +\n  labs(title = \"Maximum PRP per DDD Over Time\",\n       x = \"\",\n       y = \"PRP per DDD\",\n       colour = \"Drug\") +\n  hen_theme()\n\nplotmin\n\n\n\n\n\n\n\nplotmax\n\n\n\n\n\n\n\n\nNow that is quite the jumble of lines.\nI want to do the following:\n\nCreate a more meaningful colour representation\nLook at the outliers separately from the ones staying below 50 and 100, respectively.\n\nFirst, meaningful colour representation.\nThe exact amount of different compounds within each class can be found via the code below.\n\ndfforplot |&gt; group_by(atc) |&gt; distinct(compound)\n\n# A tibble: 20 × 2\n# Groups:   atc [20]\n   atc     compound     \n   &lt;chr&gt;   &lt;chr&gt;        \n 1 A10BA02 Metformin    \n 2 A10BB01 Glibenclamid \n 3 A10BB07 Glipizid     \n 4 A10BB09 Gliclazid    \n 5 A10BB12 Glimepirid   \n 6 A10BH01 Sitagliptin  \n 7 A10BH02 Vildagliptin \n 8 A10BH03 Saxagliptin  \n 9 A10BH04 Alogliptin   \n10 A10BH05 Linagliptin  \n11 A10BJ01 Exenatid     \n12 A10BJ02 Liraglutid   \n13 A10BJ03 Lixisenatid  \n14 A10BJ05 Dulaglutid   \n15 A10BJ06 Semaglutid   \n16 A10BK01 Dapagliflozin\n17 A10BK02 Canagliflozin\n18 A10BK03 Empagliflozin\n19 A10BK04 Ertugliflozin\n20 A10BX16 Tirzepatid   \n\n\nNow we can setup a vector with colour mapping using the output from above. Copy and paste it into a text editor to convert it to something useful. There are probably smarter ways to use that output.\n\n# prep colour scheme for drugs\n\ncolour_mapping &lt;- c(\n  # Unique drug\n  \"Metformin\"       = \"#000000\",  # Deep Blue (for uniqueness)\n\n  # Sulfonylureas (reddish colors)\n  \"Glibenclamid\"    = \"#d62728\",  # Red\n  \"Glipizid\"        = \"#e37777\",  # Light Red\n  \"Gliclazid\"       = \"#c13515\",  # Darker Red\n  \"Glimepirid\"      = \"#ff6347\",  # Tomato Red\n\n  # DPP-4 inhibitors (greenish colors)\n  \"Sitagliptin\"     = \"#2ca02c\",  # Green\n  \"Vildagliptin\"    = \"#98df8a\",  # Light Green\n  \"Saxagliptin\"     = \"#34a56f\",  # Teal Green\n  \"Alogliptin\"      = \"#57a774\",  # Medium Green\n  \"Linagliptin\"     = \"#1e7f5f\",  # Dark Green\n\n  # GLP-1 receptor agonists (bluish colors)\n  \"Exenatid\"        = \"#1f77b4\",  # Deep Blue\n  \"Liraglutid\"      = \"#5b9bd5\",  # Light Blue\n  \"Lixisenatid\"     = \"#6495ed\",  # Cornflower Blue\n  \"Dulaglutid\"      = \"#4682b4\",  # Steel Blue\n  \"Semaglutid\"      = \"#4169e1\",  # Royal Blue\n\n  # SGLT2 inhibitors (yellowish and brownish colors)\n  \"Dapagliflozin\"   = \"#ffd700\",  # Gold\n  \"Canagliflozin\"   = \"#e5b33f\",  # Dark Yellow\n  \"Empagliflozin\"   = \"#d9a120\",  # Mustard\n  \"Ertugliflozin\"   = \"#b8860b\",  # Dark Goldenrod\n\n  # Tirzepatide (distinct color)\n  \"Tirzepatid\"      = \"#8b008b\"   # Dark Magenta\n)\n\n\nNow to look at the ones who are not outliers separately to see if the graph is visually meaningful.\n\ntestplot1 &lt;- ggplot(dfforplot |&gt; filter(a_min_prpddd&lt;100), aes(x = month, y = a_min_prpddd, colour = compound)) +\n  geom_line() +\n  scale_colour_manual(values = colour_mapping) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\ntestplot1\n\n\n\n\n\n\n\n\nFirst of all, this is a mess. In addition to looking at the outliers separately, I want to do the following:\n\nSort the legend according to drug class (atc codes)\nReduce the amount of compounds included\n\nFor (1), y’all need to hold on for dear life because this stuff is an amalgamation of what I have done in my PhD combined with some new things. I will have to explain each part of the code in great detail to even remember how this works.\nThe explanations are maybe mostly for me.\nTo achieve that, I will firstly make a variable with a label for each drug class. Then, I will order the compound variable by the class variable. Then I apply cus tom colours to the compound variable.\nThe first part is relatively simply done using case_when() and knowledge of what classes the ATC codes represent.\n\ndfforplot &lt;- dfforplot |&gt;\n  mutate(class = case_when(\n    substr(atc, 1, 5) == \"A10BA\" ~ \"Metformin\",\n    substr(atc, 1, 5) == \"A10BB\" ~ \"Sulfonylureas\",\n    substr(atc, 1, 5) == \"A10BH\" ~ \"DPP4\",\n    substr(atc, 1, 5) == \"A10BJ\" ~ \"GLP1\",\n    substr(atc, 1, 5) == \"A10BK\" ~ \"SGLT2\",\n    substr(atc, 1, 5) == \"A10BX\" ~ \"Tirzepatide\",\n    TRUE ~ \"Other\"  # For any other unclassified codes\n  ))\n\nFor the second part, I set the order I want, turn the class variable into a factor, and then arrange() the compounds within each class according to this order. Finally, I turn the compound variable into a factor that is ordered by its unique levels, which correspond to the classes, and check that compound has the correct order.\n\nclass_order &lt;- c(\"Metformin\", \"Sulfonylureas\", \"DPP4\", \"GLP1\", \"SGLT2\", \"Tirzepatide\")\ndfforplot &lt;- dfforplot |&gt;\n  mutate(class = factor(class, levels = class_order))  |&gt;\n  arrange(class, compound) |&gt;\n  mutate(compound = factor(compound, levels = unique(compound)))\nprint(levels(dfforplot$compound))\n\n [1] \"Metformin\"     \"Glibenclamid\"  \"Gliclazid\"     \"Glimepirid\"   \n [5] \"Glipizid\"      \"Alogliptin\"    \"Linagliptin\"   \"Saxagliptin\"  \n [9] \"Sitagliptin\"   \"Vildagliptin\"  \"Dulaglutid\"    \"Exenatid\"     \n[13] \"Liraglutid\"    \"Lixisenatid\"   \"Semaglutid\"    \"Canagliflozin\"\n[17] \"Dapagliflozin\" \"Empagliflozin\" \"Ertugliflozin\" \"Tirzepatid\"   \n\n\nThe third part is simply remembering the order of classes, and how many compounds are within each class, and then creating a selection of colours via the colorRampPallette() function, and then store that in the object colours_compunds, to use in the plot.\n\nmet_colours  &lt;- colorRampPalette(c(\"black\"))(1)\nsu_colours  &lt;- colorRampPalette(c(\"pink\",\"darkred\"))(4)\ndpp4_colours   &lt;- colorRampPalette(c(\"green\",\"darkgreen\"))(5) # NB THIS WORKS BECAUSE THE FIRST 5 ARE DPP4, 6 are GLP1, etc etc.\nglp1_colours   &lt;- colorRampPalette(c(\"lightblue\",\"darkblue\"))(5)\nsglt2_colours  &lt;- colorRampPalette(c(\"yellow\",\"#b8860b\"))(4)\ntir_colours  &lt;- colorRampPalette(c(\"#8b008b\"))(1)\ncolours_compounds &lt;- c(met_colours,\n                su_colours,\n                dpp4_colours,\n                glp1_colours,\n                sglt2_colours,\n                tir_colours) # combine\nbarplot(rep(1,20), col=colours_compounds, border = \"white\", axes = FALSE)\n\n\n\n\n\n\n\n\nFor (2), I will focus on the three most sold compounds in the latest year and remove the rest to avoid cluttering the plot with information.\nI use Medstat as a source, and what I look for is DDD sold in total across the entire healthcare sector, which is incredible easy to extract from the site. The amounts are from when I viewed the site in October 2024. Below, I store a vector of the most three most used compounds (less if class has less compounds).\n\nkept_compounds &lt;- c(\n  \"Metformin\",\n  \"Glipizid\",\n  \"Gliclazid\",\n  \"Glimepirid\",\n  \"Sitagliptin\",\n  \"Vildagliptin\",\n  \"Linagliptin\",\n  \"Liraglutid\",\n  \"Dulaglutid\",\n  \"Semaglutid\",\n  \"Dapagliflozin\",\n  \"Canagliflozin\",\n  \"Empagliflozin\",\n  \"Tirzepatid\"\n)\n\nNow, lets make plots for the ones that are not outliers. In practice I will do that by just limiting the y axis.\n\ndfforplot |&gt;\n  filter(a_min_prpddd&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = a_min_prpddd, colour = compound)) +\n  geom_line() +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nOops, I messed up. The colours do not fit anymore. Will fit the colours to the new more limited amount of compounds.\n\nmet_colours  &lt;- colorRampPalette(c(\"black\"))(1)\nsu_colours  &lt;- colorRampPalette(c(\"pink\",\"darkred\"))(3)\ndpp4_colours   &lt;- colorRampPalette(c(\"green\",\"darkgreen\"))(3) # NB THIS WORKS BECAUSE THE FIRST 5 ARE DPP4, 6 are GLP1, etc etc.\nglp1_colours   &lt;- colorRampPalette(c(\"lightblue\",\"darkblue\"))(3)\nsglt2_colours  &lt;- colorRampPalette(c(\"yellow\",\"#b8860b\"))(3)\ntir_colours  &lt;- colorRampPalette(c(\"#8b008b\"))(1)\ncolours_compounds &lt;- c(met_colours,\n                su_colours,\n                dpp4_colours,\n                glp1_colours,\n                sglt2_colours,\n                tir_colours) # combine\nbarplot(rep(1,15), col=colours_compounds, border = \"white\", axes = FALSE)\n\n\n\n\n\n\n\n\nNow try again.\n\ndfforplot |&gt;\n  filter(a_min_prpddd&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = a_min_prpddd, colour = compound)) +\n  geom_line() +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nNow that looks much better. Now let’s make the final graph, using facet_wrap() to distinguish between the minimum and maximum PRP per DDD.\nTo use facet_wrap(), we pivot the dataset before plotting.\n\ndfforplot_pivot &lt;- dfforplot |&gt;\n  pivot_longer(\n    cols = ends_with(\"prpddd\"),\n    names_to = \"ddd\",\n    values_to = \"minmax\"\n  ) |&gt; arrange(minmax)\n\nhead(dfforplot_pivot)\n\n# A tibble: 6 × 6\n  month      atc     compound   class         ddd          minmax\n  &lt;date&gt;     &lt;chr&gt;   &lt;fct&gt;      &lt;fct&gt;         &lt;chr&gt;         &lt;dbl&gt;\n1 2019-09-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.135\n2 2019-08-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.146\n3 2020-01-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.155\n4 2020-02-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.155\n5 2019-11-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.156\n6 2019-12-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.156\n\n\nAnd then make the plot.\n\ndfforplot_pivot |&gt;\n  filter(compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = minmax, colour = compound)) +\n  geom_line() +\n  facet_wrap(\n    ~ddd,\n    scales = \"free\",\n    labeller = labeller(ddd =\n                          c(\"a_min_prpddd\" = \"Minimum\",\n                            \"b_max_prpddd\" = \"Maximum\")\n                        )) +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nAnd the final, more presentable plot.\n\ndfforplot_pivot |&gt;\n  filter(minmax&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = minmax, colour = compound)) +\n  geom_line() +\n  facet_wrap(\n    ~ddd,\n    scales = \"free\",\n    labeller = labeller(ddd =\n                          c(\"a_min_prpddd\" = \"Minimum\",\n                            \"b_max_prpddd\" = \"Maximum\")\n                        )) +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(\n    title = \"Pharmacy Retail Price per DDD, in DKK\",\n    caption = \"Outliers: Minimum Tirzepatide prices go from\",\n    x = \"\",\n    y = \"\",\n    colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nI also want to add information about the outliers on the plot.\nJEG HAR GJORT DET MESTE FÆRDIGT OG VIL BARE GERNE SKRIVE DET RENT SENERE\n\n# Filter the dataset for Semaglutide and Tirzepatide\nsubset_df &lt;- dfforplot_pivot %&gt;%\n  filter(compound %in% c(\"Semaglutid\", \"Tirzepatid\"))\n\n# Summarise the earliest, latest, highest, and lowest values for each compound\nsummary_df &lt;- subset_df %&gt;%\n  group_by(compound) %&gt;%\n  summarise(\n    earliest_date = min(month),\n    latest_date = max(month),\n    highest_ddd = max(minmax),\n    lowest_ddd = min(minmax),\n    .groups = 'drop'  # Ungroup after summarising\n  )\n\n# View the resulting dataset\nprint(summary_df)\n\n# A tibble: 2 × 5\n  compound   earliest_date latest_date highest_ddd lowest_ddd\n  &lt;fct&gt;      &lt;date&gt;        &lt;date&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Semaglutid 2019-07-01    2024-09-01         308.       22.6\n2 Tirzepatid 2024-04-01    2024-09-01         522.      103. \n\n# Create a caption string\ncaption_text &lt;- paste(\n  \"Outliers were Semaglutide and Tirzepatide. Lowest PRP per DDD for Semaglutide was\",\n  \"22.6, which increased to 308 in 2024.\",\n  \"For Tirzepatide, the lowest value was 103, and increased to 522 in September of 2024.\"\n  )\n\n# Print caption for verification\nprint(caption_text)\n\n[1] \"Outliers were Semaglutide and Tirzepatide. Lowest PRP per DDD for Semaglutide was 22.6, which increased to 308 in 2024. For Tirzepatide, the lowest value was 103, and increased to 522 in September of 2024.\"\n\n\nDoublechecking with my older data.\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Tirzepatid\")) |&gt; summarise(haps = min(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  103.\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Tirzepatid\")) |&gt; summarise(haps = max(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  522.\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Semaglutid\")) |&gt; summarise(haps = min(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  22.6\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Semaglutid\")) |&gt; summarise(haps = max(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  308.\n\n\nSo that makes the final final plot:\n\ndfforplot_pivot |&gt;\n  filter(minmax&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = minmax, colour = compound)) +\n  geom_line() +\n  facet_wrap(\n    ~ddd,\n    scales = \"free\",\n    labeller = labeller(ddd =\n                          c(\"a_min_prpddd\" = \"Minimum\",\n                            \"b_max_prpddd\" = \"Maximum\")\n                        )) +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(\n    title = \"Pharmacy Retail Price per DDD, in DKK\",\n    caption = caption_text,\n    x = \"\",\n    y = \"\",\n    colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saved as thumbnail"
  },
  {
    "objectID": "posts/2024-09-30-tot_ddd_use/index.html#drug-utilisation-over-time",
    "href": "posts/2024-09-30-tot_ddd_use/index.html#drug-utilisation-over-time",
    "title": "Evolution in DDD use of Diabetes Drugs over time",
    "section": "",
    "text": "My two previous posts examined data available from medstat and were focused on those datasets and specifically the turnover of specific drug classes.\nHere, I wish to demonstrate a different type of publicly available data, still related to prescription data.\nThis is data regarding drug prices, and can be found on The Danish Health Data Authority’s website here.\nDanish drug prices are by and large renegotiated every 14 days. Therefore, the dataset is updated accordingly. I will use the data from the most recent update as of the start of making this post.\nLooking at it post hoc, I used the following packages:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)\nlibrary(stringr)\nlibrary(extrafont)\n# install.packages(\"ggpubr\")\n# library(gridExtra)\n# library(ggpubr)\n\n\n\nI had some initial trouble figuring out how to download the file. What kept me from figuring it out was not realising that the file from the link was a zip file containing an excel file.\nApparently, “The extension .ashx doesn’t specify the file format but rather that a server-side handler is being used to serve the content.”, as chatGPT puts it. I have no gained the understanding that .ashx is just some sort of framework that serves up the data, not a file type.\nThis leads to the following code necessary to download the data. Code is just for show.\n\n# URL of the .ashx file\nurl &lt;- \"https://www.esundhed.dk/-/media/Files/Publikationer/Emner/Laegemidler/Medicinpriser/2024/lmpriser_eSundhed_240916.ashx\"\n\n# Define a path in my local datafolder, used as described in the first post about medstat data\ndatapath &lt;- \"C:/Users/henri/Documents/data-publicdataprojects\"\n\n# Define the path where the ZIP file will be saved\nzip_destfile &lt;- paste0(datapath,\"lmpriser_eSundhed_240916.zip\")\n\n# Download the ZIP file\ndownload.file(url, zip_destfile, mode = \"wb\")\n\n# Unzip the file\nunzip(zip_destfile, exdir = paste0(datapath,\"unzipped_files\"))\n\n# Check the unzipped files\nlist.files(paste0(datapath,\"unzipped_files\"))\n\n# check which sheet to import from the excel file\nsheet_names &lt;- excel_sheets(paste0(datapath,\"unzipped_files/lmpriser_eSundhed_240916.xlsx\"))\n\nAnd then I import the file:\n\n# Read the excel file\ndatapath &lt;- \"C:/Users/henri/Documents/data-publicdataprojects\"\ndf_ddd &lt;- read_excel(paste0(datapath,\"unzipped_files/lmpriser_eSundhed_240916.xlsx\"), sheet = \"lmpriser_eSundhed_240916\")\n\nhead(df_ddd)\n\n# A tibble: 6 × 144\n  ATC    Lægemiddel Indholdsstof Varenummer Pakning Styrke Form  Firma Indikator\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    \n1 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… AIP      \n2 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… AUP      \n3 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… DDD      \n4 A01AA… Duraphat   Natriumfluo… 066411     51 g    5 mg/g tand… Colg… AUP_pr_D…\n5 A01AA… Duraphat   Natriumfluo… 066422     3 x 51… 5 mg/g tand… Colg… AIP      \n6 A01AA… Duraphat   Natriumfluo… 066422     3 x 51… 5 mg/g tand… Colg… AUP      \n# ℹ 135 more variables: `20190729` &lt;dbl&gt;, `20190812` &lt;dbl&gt;, `20190826` &lt;dbl&gt;,\n#   `20190909` &lt;dbl&gt;, `20190923` &lt;dbl&gt;, `20191007` &lt;dbl&gt;, `20191021` &lt;dbl&gt;,\n#   `20191104` &lt;dbl&gt;, `20191118` &lt;dbl&gt;, `20191202` &lt;dbl&gt;, `20191216` &lt;dbl&gt;,\n#   `20191230` &lt;dbl&gt;, `20200113` &lt;dbl&gt;, `20200127` &lt;dbl&gt;, `20200210` &lt;dbl&gt;,\n#   `20200224` &lt;dbl&gt;, `20200309` &lt;dbl&gt;, `20200323` &lt;dbl&gt;, `20200406` &lt;dbl&gt;,\n#   `20200420` &lt;dbl&gt;, `20200504` &lt;dbl&gt;, `20200518` &lt;dbl&gt;, `20200601` &lt;dbl&gt;,\n#   `20200615` &lt;dbl&gt;, `20200629` &lt;dbl&gt;, `20200713` &lt;dbl&gt;, `20200727` &lt;dbl&gt;, …\n\n\n\n\n\nJust as the post, I want to focus on drugs used for diabetes. So I keep only the observations with A10A and A10B:\n\n# Cleaning it in one set of operations\ndf_ddd_A10 &lt;- df_ddd |&gt;\n# Filter for just A10A and A10B, and keep just prp per ddd\n  filter(\n    str_detect(ATC,(\"^A10B\")) | str_detect(ATC,(\"^A10A\")),\n    Indikator == \"AUP_pr_DDD\") |&gt;\n  # Select variables\n  select(ATC, Indholdsstof, Lægemiddel, starts_with(\"20\")) |&gt;\n  # Pivot the data so that the variable columns that contain time are contained in one variable column\n  pivot_longer(\n    cols = starts_with(\"20\"),\n    names_to = \"Tid\",\n    values_to = \"prpddd\") |&gt;\n  mutate(\n    Tid = ymd(Tid)) |&gt;\n  # group_by(tid) |&gt;  DELETE??\n  # mutate(\n  #   hip_ddd = max(prpddd), # hip_ddd = maxpris per ddd\n  #   lop_ddd = min(prpddd) # lop_ddd = minpris per ddd\n  # ) |&gt;\n  # ungroup() |&gt;\n  filter(\n    !is.na(prpddd)\n  )\n\n# And translate the colnames to english for good measure\nhead(df_ddd_A10)\n\n# A tibble: 6 × 5\n  ATC     Indholdsstof    Lægemiddel Tid        prpddd\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;      &lt;date&gt;      &lt;dbl&gt;\n1 A10AB01 Insulin (human) Actrapid   2019-07-29   6.48\n2 A10AB01 Insulin (human) Actrapid   2019-08-12   6.48\n3 A10AB01 Insulin (human) Actrapid   2019-08-26   6.48\n4 A10AB01 Insulin (human) Actrapid   2019-09-09   6.48\n5 A10AB01 Insulin (human) Actrapid   2019-09-23   6.48\n6 A10AB01 Insulin (human) Actrapid   2019-10-07   6.48\n\ncolnames(df_ddd_A10) &lt;- c(\n  \"atc\",\n  \"compound\",\n  \"product\",\n  \"time\",\n  \"prpddd\" # Pharmacy Retail Price DDD\n)\nhead(df_ddd_A10)\n\n# A tibble: 6 × 5\n  atc     compound        product  time       prpddd\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;    &lt;date&gt;      &lt;dbl&gt;\n1 A10AB01 Insulin (human) Actrapid 2019-07-29   6.48\n2 A10AB01 Insulin (human) Actrapid 2019-08-12   6.48\n3 A10AB01 Insulin (human) Actrapid 2019-08-26   6.48\n4 A10AB01 Insulin (human) Actrapid 2019-09-09   6.48\n5 A10AB01 Insulin (human) Actrapid 2019-09-23   6.48\n6 A10AB01 Insulin (human) Actrapid 2019-10-07   6.48\n\n\n\n\n\nThe resulting dataset is one with all registered Pharmacy Retail Price per Defined Daily Dose (PRP per DDD) of Drugs in the A10 ATC category, from between 2019-07-29 and 2024-09-16. In other words: from the past 5.1362081 years.\nThis is basically a measure of how much it costs to treat a person with the medication with a standard dose, each day, and can be used to compare how much each drug costs to use for treatment. This is used because it can be difficult to compare drugs based on redeemed prescriptions or prices, as some drugs are prescribed differently than others. Also, some drugs have absurdly high prices per pill, as some specialised drugs in oncology and opthalmology, compared to very cheap generic pain killers.\nThis is exemplified in the code below.\n\ndf_ddd_test &lt;- df_ddd |&gt;\n  filter(Indikator == \"AUP\") |&gt; # AUP = Pharmacy Retail Price\n  pivot_longer(\n    cols = starts_with(\"20\"),\n    names_to = \"Tid\",\n    values_to = \"prp\"\n    ) |&gt;\n    summarise(\n    highest_prp = max(prp, na.rm = TRUE),\n    lowest_prp = min(prp, na.rm = TRUE)\n  )\ndf_ddd_test\n\n# A tibble: 1 × 2\n  highest_prp lowest_prp\n        &lt;dbl&gt;      &lt;dbl&gt;\n1    9999990.       7.25\n\n\nI want to present the numbers with inline code, and this can be done easily with the basic R function format() and cat() which can format the numeric value and concatenate the text.\n\n# Format the numbers in base R\nformatted_highest_prp &lt;- format(df_ddd_test$highest_prp, big.mark = \",\", scientific = FALSE)\nformatted_lowest_prp &lt;- format(df_ddd_test$lowest_prp, big.mark = \",\", scientific = FALSE)\n\n# Print the formatted numbers\ncat(\"Highest AUP:\", formatted_highest_prp, \"DKK\\n\")\n\nHighest AUP: 9,999,990 DKK\n\ncat(\"Lowest AUP:\", formatted_lowest_prp, \"DKK\\n\")\n\nLowest AUP: 7.25 DKK\n\n\nBelow, the numbers are generated with inline code:\nThe highest Pharmacy Retail Price is NULL, and the lowest is NULL, which is QUITE a difference.\n\n\n\nNow, back to the retail price per DDD of diabetes drugs. Just as a visual aid, lets compare the most expensive to the cheapest, of the diabetes drugs.\nBut before that, a useful thing I picked up from Meghan Hall’s blog, is putting a consistent theme on your plots. This can be done as below.\n\nlibrary(gghighlight)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(ggtext)\nlibrary(ggrepel)\n\nhen_theme &lt;- function () {\n  theme_linedraw(base_size=11) %+replace%\n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.background = element_rect(fill = \"white\", color = NA),\n      legend.key = element_rect(fill = \"white\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.title = element_text(size = 16, hjust = 0, vjust = 0.5,\n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 10, hjust = 0, vjust = 0.5,\n                                   margin = margin(b = 0.4, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\",\n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\n\nThe “hen_theme” is then added to future plots, at the end.\n\n# find most expensive\ndf_ddd_A10 |&gt;\n  group_by(atc) |&gt;\n  summarise(\n    highest_prpddd = max(prpddd, na.rm = TRUE),\n    lowest_prpddd = min(prpddd, na.rm = TRUE)\n  ) |&gt;\n  summarise(\n    most_expensive_atc = atc[which.max(highest_prpddd)],\n    most_expensive_value = max(highest_prpddd),\n    least_expensive_atc = atc[which.min(lowest_prpddd)],\n    least_expensive_value = min(lowest_prpddd)\n  )\n\n# A tibble: 1 × 4\n  most_expensive_atc most_expensive_value least_expensive_atc\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;              \n1 A10BX16                            522. A10BB12            \n# ℹ 1 more variable: least_expensive_value &lt;dbl&gt;\n\n# plot\nhilo_ddd &lt;- df_ddd_A10 |&gt;\n  filter(atc == \"A10BX16\" | atc == \"A10BB12\") |&gt;\n  select(compound,time,prpddd)\n\nggplot(hilo_ddd, mapping = aes(x=time,y=prpddd, colour = compound)) +\n  geom_point() +\n  labs(\n    title = \"Cheapest and most expensive A10 drugs\",\n    subtitle = \"by Pharmacy Retail Price per DDD\",\n    y = \"Pharmacy Retail Price per DDD\",\n    x = \"\",\n  ) +\n  hen_theme() +\n  theme(\n    legend.title = element_blank(),\n    legend.position = c(0.8,0.9),\n    legend.background = element_rect(fill= \"white\")\n    )\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nThis plot tells us that the two drug compounds with the highest and lowest PRP per DDD are Glimepiride, of the sulfonyurea class, and Tirzepatide, a GIP and GLP1 combination.\nNow, I wish to visualise how PRP per DDD changes over time. Below, I create the variables for the minimum and maximum values for each compound.\n\n# Make a new var thats floored to months and find the min for each month\ndf_ddd_A10_minmax &lt;- df_ddd_A10 |&gt;\n  mutate(month = floor_date(time,\"month\")) |&gt;\n  group_by(month, atc, compound) |&gt;\n  summarise(\n    a_min_prpddd = min(prpddd, na.rm = TRUE), # \"a_\" is to make it first in the facetwrap later\n    b_max_prpddd = max(prpddd, na.rm = TRUE)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'month', 'atc'. You can override using the\n`.groups` argument.\n\nhead(df_ddd_A10_minmax)\n\n# A tibble: 6 × 5\n  month      atc     compound         a_min_prpddd b_max_prpddd\n  &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;                   &lt;dbl&gt;        &lt;dbl&gt;\n1 2019-07-01 A10AB01 Insulin (human)          6.07         9.90\n2 2019-07-01 A10AB04 Insulin lispro           9.8         12.2 \n3 2019-07-01 A10AB05 Insulin aspart           9.66        13.1 \n4 2019-07-01 A10AB06 Insulin glulisin         9.35         9.55\n5 2019-07-01 A10AC01 Insulin (human)          6.07        10.9 \n6 2019-07-01 A10AD01 Insulin (human)          6.34        10.9 \n\n\nAnd now for the grand plot I have planned for this post, which will be an overview of the PRP per DDD for the main drug classes in diabetes type 2 treatment.\nFirst, I limit the dataset to only encompass the classes I want to look at.\n\ndfforplot &lt;- df_ddd_A10_minmax |&gt;\n  filter(\n    str_detect(atc,\"^A10BA\") |   # met\n      str_detect(atc,\"^A10BB\") | # sul\n      str_detect(atc,\"^A10BK\") | # sglt2\n      str_detect(atc,\"^A10BJ\") | # glp1\n      str_detect(atc,\"^A10BH\") | # dpp4\n      str_detect(atc,\"^A10BX16\") # tirzepatid, honoured guest\n    )\n\nThen, I try to make the plots which contain a surmountable amount of information.\n\n# Minimum values plot\nplotmin &lt;- ggplot(dfforplot) +\n  geom_line(aes(x = month, y = a_min_prpddd, colour = compound), linewidth = 0.8) +\n  labs(title = \"Minimum PRP per DDD Over Time\",\n       x = \"\",\n       y = \"PRP per DDD\",\n       colour = \"Drug\") +\n  hen_theme()\n# Max\nplotmax &lt;- ggplot(dfforplot) +\n  geom_line(aes(x = month, y = b_max_prpddd, colour = compound), linewidth = 0.8) +\n  labs(title = \"Maximum PRP per DDD Over Time\",\n       x = \"\",\n       y = \"PRP per DDD\",\n       colour = \"Drug\") +\n  hen_theme()\n\nplotmin\n\n\n\n\n\n\n\nplotmax\n\n\n\n\n\n\n\n\nNow that is quite the jumble of lines.\nI want to do the following:\n\nCreate a more meaningful colour representation\nLook at the outliers separately from the ones staying below 50 and 100, respectively.\n\nFirst, meaningful colour representation.\nThe exact amount of different compounds within each class can be found via the code below.\n\ndfforplot |&gt; group_by(atc) |&gt; distinct(compound)\n\n# A tibble: 20 × 2\n# Groups:   atc [20]\n   atc     compound     \n   &lt;chr&gt;   &lt;chr&gt;        \n 1 A10BA02 Metformin    \n 2 A10BB01 Glibenclamid \n 3 A10BB07 Glipizid     \n 4 A10BB09 Gliclazid    \n 5 A10BB12 Glimepirid   \n 6 A10BH01 Sitagliptin  \n 7 A10BH02 Vildagliptin \n 8 A10BH03 Saxagliptin  \n 9 A10BH04 Alogliptin   \n10 A10BH05 Linagliptin  \n11 A10BJ01 Exenatid     \n12 A10BJ02 Liraglutid   \n13 A10BJ03 Lixisenatid  \n14 A10BJ05 Dulaglutid   \n15 A10BJ06 Semaglutid   \n16 A10BK01 Dapagliflozin\n17 A10BK02 Canagliflozin\n18 A10BK03 Empagliflozin\n19 A10BK04 Ertugliflozin\n20 A10BX16 Tirzepatid   \n\n\nNow we can setup a vector with colour mapping using the output from above. Copy and paste it into a text editor to convert it to something useful. There are probably smarter ways to use that output.\n\n# prep colour scheme for drugs\n\ncolour_mapping &lt;- c(\n  # Unique drug\n  \"Metformin\"       = \"#000000\",  # Deep Blue (for uniqueness)\n\n  # Sulfonylureas (reddish colors)\n  \"Glibenclamid\"    = \"#d62728\",  # Red\n  \"Glipizid\"        = \"#e37777\",  # Light Red\n  \"Gliclazid\"       = \"#c13515\",  # Darker Red\n  \"Glimepirid\"      = \"#ff6347\",  # Tomato Red\n\n  # DPP-4 inhibitors (greenish colors)\n  \"Sitagliptin\"     = \"#2ca02c\",  # Green\n  \"Vildagliptin\"    = \"#98df8a\",  # Light Green\n  \"Saxagliptin\"     = \"#34a56f\",  # Teal Green\n  \"Alogliptin\"      = \"#57a774\",  # Medium Green\n  \"Linagliptin\"     = \"#1e7f5f\",  # Dark Green\n\n  # GLP-1 receptor agonists (bluish colors)\n  \"Exenatid\"        = \"#1f77b4\",  # Deep Blue\n  \"Liraglutid\"      = \"#5b9bd5\",  # Light Blue\n  \"Lixisenatid\"     = \"#6495ed\",  # Cornflower Blue\n  \"Dulaglutid\"      = \"#4682b4\",  # Steel Blue\n  \"Semaglutid\"      = \"#4169e1\",  # Royal Blue\n\n  # SGLT2 inhibitors (yellowish and brownish colors)\n  \"Dapagliflozin\"   = \"#ffd700\",  # Gold\n  \"Canagliflozin\"   = \"#e5b33f\",  # Dark Yellow\n  \"Empagliflozin\"   = \"#d9a120\",  # Mustard\n  \"Ertugliflozin\"   = \"#b8860b\",  # Dark Goldenrod\n\n  # Tirzepatide (distinct color)\n  \"Tirzepatid\"      = \"#8b008b\"   # Dark Magenta\n)\n\n\nNow to look at the ones who are not outliers separately to see if the graph is visually meaningful.\n\ntestplot1 &lt;- ggplot(dfforplot |&gt; filter(a_min_prpddd&lt;100), aes(x = month, y = a_min_prpddd, colour = compound)) +\n  geom_line() +\n  scale_colour_manual(values = colour_mapping) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\ntestplot1\n\n\n\n\n\n\n\n\nFirst of all, this is a mess. In addition to looking at the outliers separately, I want to do the following:\n\nSort the legend according to drug class (atc codes)\nReduce the amount of compounds included\n\nFor (1), y’all need to hold on for dear life because this stuff is an amalgamation of what I have done in my PhD combined with some new things. I will have to explain each part of the code in great detail to even remember how this works.\nThe explanations are maybe mostly for me.\nTo achieve that, I will firstly make a variable with a label for each drug class. Then, I will order the compound variable by the class variable. Then I apply cus tom colours to the compound variable.\nThe first part is relatively simply done using case_when() and knowledge of what classes the ATC codes represent.\n\ndfforplot &lt;- dfforplot |&gt;\n  mutate(class = case_when(\n    substr(atc, 1, 5) == \"A10BA\" ~ \"Metformin\",\n    substr(atc, 1, 5) == \"A10BB\" ~ \"Sulfonylureas\",\n    substr(atc, 1, 5) == \"A10BH\" ~ \"DPP4\",\n    substr(atc, 1, 5) == \"A10BJ\" ~ \"GLP1\",\n    substr(atc, 1, 5) == \"A10BK\" ~ \"SGLT2\",\n    substr(atc, 1, 5) == \"A10BX\" ~ \"Tirzepatide\",\n    TRUE ~ \"Other\"  # For any other unclassified codes\n  ))\n\nFor the second part, I set the order I want, turn the class variable into a factor, and then arrange() the compounds within each class according to this order. Finally, I turn the compound variable into a factor that is ordered by its unique levels, which correspond to the classes, and check that compound has the correct order.\n\nclass_order &lt;- c(\"Metformin\", \"Sulfonylureas\", \"DPP4\", \"GLP1\", \"SGLT2\", \"Tirzepatide\")\ndfforplot &lt;- dfforplot |&gt;\n  mutate(class = factor(class, levels = class_order))  |&gt;\n  arrange(class, compound) |&gt;\n  mutate(compound = factor(compound, levels = unique(compound)))\nprint(levels(dfforplot$compound))\n\n [1] \"Metformin\"     \"Glibenclamid\"  \"Gliclazid\"     \"Glimepirid\"   \n [5] \"Glipizid\"      \"Alogliptin\"    \"Linagliptin\"   \"Saxagliptin\"  \n [9] \"Sitagliptin\"   \"Vildagliptin\"  \"Dulaglutid\"    \"Exenatid\"     \n[13] \"Liraglutid\"    \"Lixisenatid\"   \"Semaglutid\"    \"Canagliflozin\"\n[17] \"Dapagliflozin\" \"Empagliflozin\" \"Ertugliflozin\" \"Tirzepatid\"   \n\n\nThe third part is simply remembering the order of classes, and how many compounds are within each class, and then creating a selection of colours via the colorRampPallette() function, and then store that in the object colours_compunds, to use in the plot.\n\nmet_colours  &lt;- colorRampPalette(c(\"black\"))(1)\nsu_colours  &lt;- colorRampPalette(c(\"pink\",\"darkred\"))(4)\ndpp4_colours   &lt;- colorRampPalette(c(\"green\",\"darkgreen\"))(5) # NB THIS WORKS BECAUSE THE FIRST 5 ARE DPP4, 6 are GLP1, etc etc.\nglp1_colours   &lt;- colorRampPalette(c(\"lightblue\",\"darkblue\"))(5)\nsglt2_colours  &lt;- colorRampPalette(c(\"yellow\",\"#b8860b\"))(4)\ntir_colours  &lt;- colorRampPalette(c(\"#8b008b\"))(1)\ncolours_compounds &lt;- c(met_colours,\n                su_colours,\n                dpp4_colours,\n                glp1_colours,\n                sglt2_colours,\n                tir_colours) # combine\nbarplot(rep(1,20), col=colours_compounds, border = \"white\", axes = FALSE)\n\n\n\n\n\n\n\n\nFor (2), I will focus on the three most sold compounds in the latest year and remove the rest to avoid cluttering the plot with information.\nI use Medstat as a source, and what I look for is DDD sold in total across the entire healthcare sector, which is incredible easy to extract from the site. The amounts are from when I viewed the site in October 2024. Below, I store a vector of the most three most used compounds (less if class has less compounds).\n\nkept_compounds &lt;- c(\n  \"Metformin\",\n  \"Glipizid\",\n  \"Gliclazid\",\n  \"Glimepirid\",\n  \"Sitagliptin\",\n  \"Vildagliptin\",\n  \"Linagliptin\",\n  \"Liraglutid\",\n  \"Dulaglutid\",\n  \"Semaglutid\",\n  \"Dapagliflozin\",\n  \"Canagliflozin\",\n  \"Empagliflozin\",\n  \"Tirzepatid\"\n)\n\nNow, lets make plots for the ones that are not outliers. In practice I will do that by just limiting the y axis.\n\ndfforplot |&gt;\n  filter(a_min_prpddd&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = a_min_prpddd, colour = compound)) +\n  geom_line() +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nOops, I messed up. The colours do not fit anymore. Will fit the colours to the new more limited amount of compounds.\n\nmet_colours  &lt;- colorRampPalette(c(\"black\"))(1)\nsu_colours  &lt;- colorRampPalette(c(\"pink\",\"darkred\"))(3)\ndpp4_colours   &lt;- colorRampPalette(c(\"green\",\"darkgreen\"))(3) # NB THIS WORKS BECAUSE THE FIRST 5 ARE DPP4, 6 are GLP1, etc etc.\nglp1_colours   &lt;- colorRampPalette(c(\"lightblue\",\"darkblue\"))(3)\nsglt2_colours  &lt;- colorRampPalette(c(\"yellow\",\"#b8860b\"))(3)\ntir_colours  &lt;- colorRampPalette(c(\"#8b008b\"))(1)\ncolours_compounds &lt;- c(met_colours,\n                su_colours,\n                dpp4_colours,\n                glp1_colours,\n                sglt2_colours,\n                tir_colours) # combine\nbarplot(rep(1,15), col=colours_compounds, border = \"white\", axes = FALSE)\n\n\n\n\n\n\n\n\nNow try again.\n\ndfforplot |&gt;\n  filter(a_min_prpddd&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = a_min_prpddd, colour = compound)) +\n  geom_line() +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nNow that looks much better. Now let’s make the final graph, using facet_wrap() to distinguish between the minimum and maximum PRP per DDD.\nTo use facet_wrap(), we pivot the dataset before plotting.\n\ndfforplot_pivot &lt;- dfforplot |&gt;\n  pivot_longer(\n    cols = ends_with(\"prpddd\"),\n    names_to = \"ddd\",\n    values_to = \"minmax\"\n  ) |&gt; arrange(minmax)\n\nhead(dfforplot_pivot)\n\n# A tibble: 6 × 6\n  month      atc     compound   class         ddd          minmax\n  &lt;date&gt;     &lt;chr&gt;   &lt;fct&gt;      &lt;fct&gt;         &lt;chr&gt;         &lt;dbl&gt;\n1 2019-09-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.135\n2 2019-08-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.146\n3 2020-01-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.155\n4 2020-02-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.155\n5 2019-11-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.156\n6 2019-12-01 A10BB12 Glimepirid Sulfonylureas a_min_prpddd  0.156\n\n\nAnd then make the plot.\n\ndfforplot_pivot |&gt;\n  filter(compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = minmax, colour = compound)) +\n  geom_line() +\n  facet_wrap(\n    ~ddd,\n    scales = \"free\",\n    labeller = labeller(ddd =\n                          c(\"a_min_prpddd\" = \"Minimum\",\n                            \"b_max_prpddd\" = \"Maximum\")\n                        )) +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(title = \"Minimum Pharmacy Retail Price per DDD, in DKK\",\n       x = \"\",\n       y = \"\",\n       colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nAnd the final, more presentable plot.\n\ndfforplot_pivot |&gt;\n  filter(minmax&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = minmax, colour = compound)) +\n  geom_line() +\n  facet_wrap(\n    ~ddd,\n    scales = \"free\",\n    labeller = labeller(ddd =\n                          c(\"a_min_prpddd\" = \"Minimum\",\n                            \"b_max_prpddd\" = \"Maximum\")\n                        )) +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(\n    title = \"Pharmacy Retail Price per DDD, in DKK\",\n    caption = \"Outliers: Minimum Tirzepatide prices go from\",\n    x = \"\",\n    y = \"\",\n    colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\n\nI also want to add information about the outliers on the plot.\nJEG HAR GJORT DET MESTE FÆRDIGT OG VIL BARE GERNE SKRIVE DET RENT SENERE\n\n# Filter the dataset for Semaglutide and Tirzepatide\nsubset_df &lt;- dfforplot_pivot %&gt;%\n  filter(compound %in% c(\"Semaglutid\", \"Tirzepatid\"))\n\n# Summarise the earliest, latest, highest, and lowest values for each compound\nsummary_df &lt;- subset_df %&gt;%\n  group_by(compound) %&gt;%\n  summarise(\n    earliest_date = min(month),\n    latest_date = max(month),\n    highest_ddd = max(minmax),\n    lowest_ddd = min(minmax),\n    .groups = 'drop'  # Ungroup after summarising\n  )\n\n# View the resulting dataset\nprint(summary_df)\n\n# A tibble: 2 × 5\n  compound   earliest_date latest_date highest_ddd lowest_ddd\n  &lt;fct&gt;      &lt;date&gt;        &lt;date&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Semaglutid 2019-07-01    2024-09-01         308.       22.6\n2 Tirzepatid 2024-04-01    2024-09-01         522.      103. \n\n# Create a caption string\ncaption_text &lt;- paste(\n  \"Outliers were Semaglutide and Tirzepatide. Lowest PRP per DDD for Semaglutide was\",\n  \"22.6, which increased to 308 in 2024.\",\n  \"For Tirzepatide, the lowest value was 103, and increased to 522 in September of 2024.\"\n  )\n\n# Print caption for verification\nprint(caption_text)\n\n[1] \"Outliers were Semaglutide and Tirzepatide. Lowest PRP per DDD for Semaglutide was 22.6, which increased to 308 in 2024. For Tirzepatide, the lowest value was 103, and increased to 522 in September of 2024.\"\n\n\nDoublechecking with my older data.\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Tirzepatid\")) |&gt; summarise(haps = min(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  103.\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Tirzepatid\")) |&gt; summarise(haps = max(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  522.\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Semaglutid\")) |&gt; summarise(haps = min(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  22.6\n\ndf_ddd_A10 |&gt; filter(str_detect(compound, \"Semaglutid\")) |&gt; summarise(haps = max(prpddd))\n\n# A tibble: 1 × 1\n   haps\n  &lt;dbl&gt;\n1  308.\n\n\nSo that makes the final final plot:\n\ndfforplot_pivot |&gt;\n  filter(minmax&lt;100\n         & compound %in% kept_compounds) |&gt;\n  ggplot(aes(x = month, y = minmax, colour = compound)) +\n  geom_line() +\n  facet_wrap(\n    ~ddd,\n    scales = \"free\",\n    labeller = labeller(ddd =\n                          c(\"a_min_prpddd\" = \"Minimum\",\n                            \"b_max_prpddd\" = \"Maximum\")\n                        )) +\n  scale_colour_manual(values = colours_compounds) +\n  scale_x_continuous(breaks = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"),\n                     labels = format(seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2025-12-01\"), by = \"year\"), \"%Y\")) +\n  labs(\n    title = \"Pharmacy Retail Price per DDD, in DKK\",\n    caption = caption_text,\n    x = \"\",\n    y = \"\",\n    colour = \"Drug\") +\n  hen_theme()\n\n\n\n\n\n\n\nggsave(\"thumbnail.jpg\", plot = last_plot(), width = 6, height = 4) # saved as thumbnail"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is dedicated to showcasing my skills as a data scientist with an understanding of health economics, epidemiology and medicine.\n\n\n\n\n\n\nCurrently in testing phase!\n\ntesting deployment: rerendering all posts to get thumbnails right\ntesting deployment: test to see if i can do dark theme first, and only change about.qmd and _quarto.yml and then push changes - it should work\n\n\n\n\nQualifications:\n\nPh.D. in diabetes epidemiology and health economics\n\nStudied the use of antidiabetic drugs and their utilisation in Denmark, and economic model assumptions used in their comparisons in cost-effectiveness analyses using decision analytical models\nTwo observational studies\nPublished review of economic model assumptions when comparing newer medications for treating type 2 diabetes\nData sources: registry data from The Register of Pharmaceutical Sales and The National Patient Register\n\nWorked as a scientific assistant\n\nPublished review of PDSA frameworks as used in the real world\nother work\nAnother one\ntake links from CV"
  }
]